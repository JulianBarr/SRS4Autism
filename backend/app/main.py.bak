from fastapi import FastAPI, HTTPException, Form, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional, Dict, Any, Set
import json
import os
from datetime import datetime
import requests
from urllib.parse import urlencode
from collections import defaultdict
from html import unescape
import csv
import io
import re
import unicodedata
from functools import lru_cache
from pathlib import Path
import math
import tempfile
import zipfile
import shutil
import re

# Database imports
import sys
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from database.db import get_db, init_db, get_db_session
from database.services import ProfileService, CardService, ChatService
from sqlalchemy.orm import Session
from fastapi import Depends

from agentic import AgenticPlanner, AgentMemory, PrincipleStore, AgentTools
import google.generativeai as genai
try:
    from dotenv import load_dotenv
    load_dotenv()
    load_dotenv(os.path.join(os.path.dirname(__file__), "../gemini.env"))
except Exception:
    pass

app = FastAPI(title="Curious Mario API", version="1.0.0")

# ============================================================================
# KG_Map Helper Functions (Following Strict Schema from Knowledge Tracking Spec)
# ============================================================================

def build_kg_map_strict(card_mappings: Dict[str, List[Dict[str, Any]]]) -> str:
    """
    Build _KG_Map JSON following the strict schema from Knowledge Tracking Specification.
    
    Schema:
    {
      "0": [
        { "kp": "word-en-apple", "skill": "sound_to_concept", "weight": 1.0 }
      ],
      "1": [
        { "kp": "word-en-apple", "skill": "concept_to_sound", "weight": 1.0 }
      ]
    }
    
    Args:
        card_mappings: Dict mapping card index (as string "0", "1", etc.) to list of KnowledgeTrace dicts.
                      Each KnowledgeTrace must have: kp (str), skill (str), weight (float, optional), context (str, optional)
    
    Returns:
        JSON string for _KG_Map field
    
    Valid Skill IDs (from Section 3):
    - Cognitive Layer: concept_to_sound, sound_to_concept
    - Literacy Layer: form_to_sound, sound_to_form, phonics_decoding, pinyin_assembly
    - Comprehension Layer: form_to_concept, concept_to_form
    """
    # Validate skill IDs
    VALID_SKILLS = {
        # Cognitive Layer
        "concept_to_sound", "sound_to_concept",
        # Literacy Layer
        "form_to_sound", "sound_to_form", "phonics_decoding", "pinyin_assembly",
        # Comprehension Layer
        "form_to_concept", "concept_to_form"
    }
    
    validated_map = {}
    for card_index, traces in card_mappings.items():
        validated_traces = []
        for trace in traces:
            if "kp" not in trace or "skill" not in trace:
                raise ValueError(f"KnowledgeTrace must have 'kp' and 'skill' fields: {trace}")
            
            skill = trace["skill"]
            if skill not in VALID_SKILLS:
                raise ValueError(f"Invalid skill ID '{skill}'. Must be one of: {sorted(VALID_SKILLS)}")
            
            validated_trace = {
                "kp": trace["kp"],
                "skill": skill,
                "weight": trace.get("weight", 1.0),
            }
            if "context" in trace:
                validated_trace["context"] = trace["context"]
            
            validated_traces.append(validated_trace)
        
        validated_map[str(card_index)] = validated_traces
    
    return json.dumps(validated_map, ensure_ascii=False)

# Initialize database on startup
@app.on_event("startup")
async def startup_event():
    print("üöÄ Starting Curious Mario API...")
    print("üìä Initializing database...")
    init_db()
    print("‚úÖ Database ready!")

# CORS middleware for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Data models
class ChildProfile(BaseModel):
    id: Optional[str] = None  # Add ID field for unique identification
    name: str
    dob: str
    gender: str
    address: str
    school: str
    neighborhood: str
    interests: List[str]
    character_roster: Optional[List[str]] = []
    verbal_fluency: Optional[str] = None
    passive_language_level: Optional[str] = None
    mental_age: Optional[float] = None  # Mental/developmental age in years (for AoA filtering)
    raw_input: Optional[str] = None
    mastered_words: Optional[str] = None  # Comma-separated list of mastered words
    mastered_english_words: Optional[str] = None  # Comma-separated list of mastered English words
    mastered_grammar: Optional[str] = None  # Comma-separated list of mastered grammar points
    extracted_data: Optional[Dict[str, Any]] = None

class Card(BaseModel):
    id: str
    front: str
    back: str
    card_type: str  # "basic", "basic_reverse", "cloze", "interactive_cloze"
    cloze_text: Optional[str] = None
    text_field: Optional[str] = None  # For interactive cloze
    extra_field: Optional[str] = None  # Additional context
    note_type: Optional[str] = None  # Anki note type name
    tags: List[str] = []
    created_at: datetime
    status: str = "pending"  # "pending", "approved", "synced"
    image_description: Optional[str] = None  # AI-generated image description
    image_prompt: Optional[str] = None  # Prompt used for image generation
    image_url: Optional[str] = None  # URL of generated image
    image_data: Optional[str] = None  # Base64 encoded image data
    image_generated: Optional[bool] = None  # Whether image was successfully generated
    image_error: Optional[str] = None  # Error message if image generation failed
    is_placeholder: Optional[bool] = None  # Whether the image is a placeholder

class CardImageRequest(BaseModel):
    position: Optional[str] = "front"
    location: Optional[str] = "after"
    user_request: Optional[str] = None

class ChatMessage(BaseModel):
    id: str
    content: str
    role: str  # "user" or "assistant"
    timestamp: datetime
    mentions: List[str] = []

class AnkiProfile(BaseModel):
    name: str
    deck_name: str
    is_active: bool = True

class PromptTemplate(BaseModel):
    id: str
    name: str
    description: str
    template_text: str  # Free-form text with examples
    created_at: datetime
    updated_at: Optional[datetime] = None

class RecommendationRequest(BaseModel):
    mastered_words: List[str]
    profile_id: str
    concreteness_weight: Optional[float] = 0.5  # Weight for concreteness (0.0-1.0), default 0.5
    # 0.0 = only HSK level matters, 1.0 = only concreteness matters, 0.5 = balanced
    mental_age: Optional[float] = None  # Mental age for AoA filtering (e.g., 7.0 for a 7-year-old)

class EnglishRecommendationRequest(BaseModel):
    mastered_words: List[str]
    profile_id: str
    # Slider: 0.0 = Max Frequency (Utility), 1.0 = Max Concreteness (Ease)
    concreteness_weight: Optional[float] = 0.5  # Slider position (0.0-1.0), default 0.5 = balanced
    # Note: CEFR is now a hard filter (not a weight), only showing current level and +1
    mental_age: Optional[float] = None  # Mental age for AoA filtering (e.g., 7.0 for a 7-year-old)

class PPRRecommendationRequest(BaseModel):
    """Request for PPR-based English word recommendations."""
    profile_id: str
    mastered_words: Optional[List[str]] = None  # If None, loaded from database
    exclude_words: Optional[List[str]] = None
    # PPR configuration
    alpha: Optional[float] = 0.5  # PPR teleport probability
    beta_ppr: Optional[float] = 1.0  # Weight for log-transformed PPR
    beta_concreteness: Optional[float] = 0.8  # Weight for z-scored concreteness
    beta_frequency: Optional[float] = 0.3  # Weight for log-transformed frequency
    beta_aoa_penalty: Optional[float] = 2.0  # Weight for AoA penalty
    beta_intercept: Optional[float] = 0.0  # Intercept term
    mental_age: Optional[float] = 8.0  # Mental age for AoA filtering
    aoa_buffer: Optional[float] = 2.0  # Buffer years beyond mental age
    exclude_multiword: Optional[bool] = True  # Filter out multi-word phrases
    top_n: Optional[int] = 50  # Number of recommendations

class ChinesePPRRecommendationRequest(BaseModel):
    """Request for PPR-based Chinese word recommendations."""
    profile_id: str
    mastered_words: Optional[List[str]] = None  # If None, loaded from database
    exclude_words: Optional[List[str]] = None
    # PPR configuration (same as English)
    alpha: Optional[float] = 0.5
    beta_ppr: Optional[float] = 1.0
    beta_concreteness: Optional[float] = 0.8
    beta_frequency: Optional[float] = 0.3
    beta_aoa_penalty: Optional[float] = 2.0
    beta_intercept: Optional[float] = 0.0
    mental_age: Optional[float] = 8.0
    aoa_buffer: Optional[float] = 2.0
    exclude_multiword: Optional[bool] = True
    top_n: Optional[int] = 50
    max_hsk_level: Optional[int] = 6

class GrammarRecommendationRequest(BaseModel):
    mastered_grammar: List[str]
    profile_id: str
    language: Optional[str] = "zh"  # "zh" for Chinese, "en" for English

class IntegratedRecommendationRequest(BaseModel):
    """Request for integrated recommendations (PPR + ZPD + Campaign Manager)."""
    profile_id: str
    language: Optional[str] = "zh"  # "zh" for Chinese, "en" for English
    mastered_words: Optional[List[str]] = None  # If None, loaded from database
    # PPR configuration overrides (optional)
    alpha: Optional[float] = None
    beta_ppr: Optional[float] = None
    beta_concreteness: Optional[float] = None
    beta_frequency: Optional[float] = None
    beta_aoa_penalty: Optional[float] = None
    beta_intercept: Optional[float] = None
    mental_age: Optional[float] = None
    aoa_buffer: Optional[float] = None
    exclude_multiword: Optional[bool] = None
    top_n: Optional[int] = None
    max_hsk_level: Optional[int] = None  # For Chinese only

class WordRecommendation(BaseModel):
    word: str
    pinyin: str
    hsk: int
    score: float  # Changed to float for more precise scoring
    known_chars: int
    total_chars: int
    concreteness: Optional[float] = None  # Concreteness rating (1-5 scale)
    age_of_acquisition: Optional[float] = None  # Age of Acquisition (AoA) in years

class GrammarRecommendation(BaseModel):
    grammar_point: str
    grammar_point_zh: Optional[str]
    structure: Optional[str]
    explanation: Optional[str]
    cefr_level: Optional[str]
    example_chinese: Optional[str]
    score: int


class AgenticPlanRequest(BaseModel):
    user_id: str
    topic: Optional[str] = None  # Optional - agent can determine what to learn
    learner_level: Optional[str] = None
    topic_complexity: Optional[str] = None  # Optional - agent can infer from mastery

# File paths for data storage (relative to project root)
# Get project root: backend/app/main.py -> project root is 2 levels up
PROJECT_ROOT = Path(__file__).parent.parent.parent.resolve()
PROFILES_FILE = str(PROJECT_ROOT / "data" / "profiles" / "child_profiles.json")
CARDS_FILE = str(PROJECT_ROOT / "data" / "content_db" / "approved_cards.json")
ANKI_PROFILES_FILE = str(PROJECT_ROOT / "data" / "profiles" / "anki_profiles.json")
CHAT_HISTORY_FILE = str(PROJECT_ROOT / "data" / "content_db" / "chat_history.json")
PROMPT_TEMPLATES_FILE = str(PROJECT_ROOT / "data" / "profiles" / "prompt_templates.json")
WORD_KP_CACHE_FILE = str(PROJECT_ROOT / "data" / "content_db" / "word_kp_cache.json")
ENGLISH_SIMILARITY_FILE = PROJECT_ROOT / "data" / "content_db" / "english_word_similarity.json"

# Ensure data directories exist
os.makedirs(PROJECT_ROOT / "data" / "profiles", exist_ok=True)
os.makedirs(PROJECT_ROOT / "data" / "content_db", exist_ok=True)

def load_json_file(file_path: str, default: Any = None):
    """Load JSON data from file, return default if file doesn't exist"""
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return json.load(f)
    return default if default is not None else []

def save_json_file(file_path: str, data: Any):
    """Save data to JSON file"""
    with open(file_path, 'w') as f:
        json.dump(data, f, indent=2, default=str)

# Cached semantic similarity map for English words
_english_similarity_cache: Optional[Dict[str, List[Dict[str, Any]]]] = None


def get_english_semantic_similarity() -> Dict[str, List[Dict[str, Any]]]:
    """Load precomputed semantic similarity map for English words."""
    global _english_similarity_cache
    if _english_similarity_cache is not None:
        return _english_similarity_cache

    if not ENGLISH_SIMILARITY_FILE.exists():
        print(f"‚ö†Ô∏è  English similarity file not found at {ENGLISH_SIMILARITY_FILE}")
        _english_similarity_cache = {}
        return _english_similarity_cache

    try:
        with ENGLISH_SIMILARITY_FILE.open("r", encoding="utf-8") as f:
            payload = json.load(f)
            _english_similarity_cache = payload.get("similarities", {})
            meta = payload.get("metadata", {})
            print(f"üß† Loaded English semantic similarity graph "
                  f"(words={len(_english_similarity_cache)}, "
                  f"model={meta.get('model')}, "
                  f"threshold={meta.get('threshold')})")
    except Exception as exc:
        print(f"‚ö†Ô∏è  Failed to load English similarity file: {exc}")
        _english_similarity_cache = {}

    return _english_similarity_cache

# Gemini configuration for fallback knowledge lookups
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL", "models/gemini-2.5-flash")
_genai_model = None
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        _genai_model = genai.GenerativeModel(GEMINI_MODEL_NAME)
    except Exception as exc:
        print(f"‚ö†Ô∏è Unable to configure Gemini model: {exc}")
else:
    print("‚ö†Ô∏è GEMINI_API_KEY not set; falling back to cache-only knowledge lookup.")

# Cache for word knowledge derived from LLM or manual sources
_word_kp_cache: Dict[str, Dict[str, Any]] = {}
try:
    _word_kp_cache = load_json_file(WORD_KP_CACHE_FILE, {})
    if not isinstance(_word_kp_cache, dict):
        _word_kp_cache = {}
except Exception as exc:
    print(f"‚ö†Ô∏è Could not load word knowledge cache: {exc}")
    _word_kp_cache = {}

def _save_word_kp_cache():
    try:
        save_json_file(WORD_KP_CACHE_FILE, _word_kp_cache)
    except Exception as exc:
        print(f"‚ö†Ô∏è Failed to save word knowledge cache: {exc}")


# Initialize Agentic components (lazy instantiation ensures compatibility with existing code)
_agentic_planner: Optional[AgenticPlanner] = None


def get_agentic_planner() -> AgenticPlanner:
    global _agentic_planner
    if _agentic_planner is None:
        memory = AgentMemory()
        principles = PrincipleStore()
        tools = AgentTools()
        _agentic_planner = AgenticPlanner(memory=memory, principles=principles, tools=tools)
    return _agentic_planner

def normalize_to_slug(value: str) -> str:
    """Normalize a string to a slug-friendly format without enforcing uniqueness."""
    if not value:
        return ""
    import re
    slug = value.lower()
    slug = re.sub(r'[\s_]+', '-', slug)
    slug = re.sub(r'[^\w\u4e00-\u9fff-]', '', slug)
    slug = slug.strip('-')
    slug = re.sub(r'-+', '-', slug)
    return slug

def extract_plain_text(value: str) -> str:
    """Strip HTML tags and normalize whitespace for prompt generation."""
    if not value:
        return ""
    import re
    text = unescape(value)
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def normalize_for_kp_id(value: str) -> str:
    """Normalize text for inclusion in a knowledge point identifier."""
    if value is None:
        return ""
    value = str(value).strip()
    if not value:
        return ""
    value = unicodedata.normalize("NFKD", value)
    value = ''.join(ch for ch in value if not unicodedata.combining(ch))
    for sep in [' ', '/', '\\', ':', '@', ',', 'Ôºå', ';', 'Ôºõ', '|']:
        value = value.replace(sep, '-')
    value = re.sub(r'-+', '-', value)
    slug = normalize_to_slug(value)
    return slug or "value"

def generate_kp_id(subject: str, predicate: str, obj: str) -> str:
    """Create a composite knowledge point identifier."""
    subject_part = normalize_for_kp_id(subject) or "subject"
    predicate_part = normalize_for_kp_id(predicate) or "predicate"
    object_part = normalize_for_kp_id(obj) or "value"
    return f"kp:{subject_part}--{predicate_part}--{object_part}"

# Note type normalization (CUMA-prefixed models)
NOTE_TYPE_SLUG_MAP = {
    "cuma-interactive-cloze": "CUMA - Interactive Cloze",
    "interactive-cloze": "CUMA - Interactive Cloze",
    "interactive_cloze": "CUMA - Interactive Cloze",
    "cuma-basic": "CUMA - Basic",
    "basic": "CUMA - Basic",
    "cuma-basic-and-reversed-card": "CUMA - Basic (and reversed card)",
    "basic-and-reversed-card": "CUMA - Basic (and reversed card)",
    "basicand-reversed-card": "CUMA - Basic (and reversed card)",
    "cuma-cloze": "CUMA - Cloze",
    "cloze": "CUMA - Cloze",
}


def resolve_note_type_name(value: str) -> str:
    """Resolve note type slugs/aliases to canonical CUMA-prefixed names."""
    if not value:
        return value
    candidate = value.strip()
    if not candidate:
        return candidate
    slug = normalize_to_slug(candidate.replace('_', '-'))
    canonical = NOTE_TYPE_SLUG_MAP.get(slug)
    if canonical:
        return canonical
    if candidate.lower().startswith("cuma -"):
        return candidate
    return candidate

CHINESE_CHAR_PATTERN = re.compile(r'[\u4e00-\u9fff]')

TAG_ANNOTATION_PREFIXES = (
    "pronunciation",
    "meaning",
    "hsk",
    "knowledge",
    "note",
    "remark",
    "example",
)


def contains_chinese_chars(value: str) -> bool:
    """Check if the string contains at least one CJK unified ideograph."""
    if not value:
        return False
    return bool(CHINESE_CHAR_PATTERN.search(value))


def _sanitize_for_sparql_literal(value: str) -> str:
    """Escape characters for safe SPARQL literal usage."""
    if value is None:
        return ""
    return value.replace("\\", "\\\\").replace('"', '\\"')

def split_tag_annotations(tags: List[Any]) -> (List[str], List[str]):
    """Separate descriptive annotations from machine-friendly tags."""
    clean_tags: List[str] = []
    annotations: List[str] = []
    
    # Handle case where tags is a string (comma-separated or single tag)
    if isinstance(tags, str):
        # Split comma-separated string into list
        tags = [t.strip() for t in tags.split(',') if t.strip()]
    elif not isinstance(tags, (list, tuple)):
        # If it's not a string or list, convert to list
        tags = [tags] if tags else []
    
    for tag in tags or []:
        if tag is None:
            continue
        tag_str = str(tag).strip()
        if not tag_str:
            continue
        lowered = tag_str.lower()
        if ":" in tag_str or any(lowered.startswith(prefix) for prefix in TAG_ANNOTATION_PREFIXES):
            annotations.append(tag_str)
        else:
            clean_tags.append(tag_str)
    return clean_tags, annotations


@lru_cache(maxsize=256)
def fetch_word_knowledge_points(word: str) -> Dict[str, Any]:
    """Fetch pronunciation/meaning knowledge points for a given Chinese word."""
    if not word:
        return {}
    sanitized_word = word.strip()
    if not sanitized_word:
        return {}
    sparql_word = _sanitize_for_sparql_literal(sanitized_word)
    sparql = f"""
    PREFIX srs-kg: <http://srs4autism.com/schema/>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    
    SELECT ?pinyin ?definition ?conceptLabel ?hsk WHERE {{
        ?word a srs-kg:Word ;
              srs-kg:text "{sparql_word}" .
        OPTIONAL {{ ?word srs-kg:pinyin ?pinyin . }}
        OPTIONAL {{ ?word srs-kg:definition ?definition .
                    FILTER(LANG(?definition) = "en" || LANG(?definition) = "") }}
        OPTIONAL {{
            ?word srs-kg:means ?concept .
            ?concept rdfs:label ?conceptLabel .
        }}
        OPTIONAL {{ ?word srs-kg:hskLevel ?hsk . }}
    }}
    """
    try:
        results = query_sparql(sparql, output_format="application/sparql-results+json")
    except HTTPException:
        return {}
    except Exception:
        return {}
    
    bindings = []
    if isinstance(results, dict):
        bindings = results.get("results", {}).get("bindings", [])
    
    pronunciations: List[str] = []
    meanings: List[str] = []
    hsk_level = None
    
    for row in bindings or []:
        pinyin_value = row.get("pinyin", {}).get("value")
        if pinyin_value and pinyin_value not in pronunciations:
            pronunciations.append(pinyin_value)
        definition_value = row.get("definition", {}).get("value")
        if definition_value and definition_value not in meanings:
            meanings.append(definition_value)
        concept_value = row.get("conceptLabel", {}).get("value")
        if concept_value and concept_value not in meanings:
            meanings.append(concept_value)
        hsk_value = row.get("hsk", {}).get("value")
        if hsk_value and not hsk_level:
            hsk_level = hsk_value
    
    if not pronunciations and not meanings and not hsk_level:
        return {}
    
    return {
        "pronunciations": pronunciations,
        "meanings": meanings,
        "hsk_level": hsk_level
    }


def _clean_llm_json(text: str) -> Dict[str, Any]:
    try:
        text = text.strip()
        if text.startswith("```"):
            text = text.split("```", 2)[1]
            if text.startswith("json"):
                text = text[4:]
        return json.loads(text)
    except Exception:
        return {}


def _fetch_word_info_via_llm(word: str) -> Dict[str, Any]:
    if not _genai_model:
        return {}
    prompt = (
        "You are a bilingual lexicon assistant.\n"
        f"Provide the Hanyu Pinyin (with tone marks) and a concise English meaning for the Chinese word \"{word}\".\n"
        "Respond ONLY with JSON like: {\"pinyin\": \"d√∫ zhƒõ\", \"meaning\": \"reader\"}."
    )
    try:
        response = _genai_model.generate_content(prompt)
        text = getattr(response, "text", "") or ""
        data = _clean_llm_json(text)
        result: Dict[str, Any] = {}
        pinyin = data.get("pinyin")
        meaning = data.get("meaning") or data.get("meaning_en") or data.get("english")
        if pinyin:
            result["pinyin"] = pinyin.strip()
        if meaning:
            result["meaning"] = meaning.strip()
        return result
    except Exception as exc:
        print(f"‚ö†Ô∏è Gemini lookup failed for '{word}': {exc}")
        return {}


def fix_iu_ui_tone_placement(pinyin: str) -> str:
    """
    Fix pinyin tone marks to follow 'i u Âπ∂ÂàóÊ†áÂú®Âêé' rule.
    When i and u appear together:
    - 'iu' -> tone should be on 'u' (e.g., 'li√∫' not 'l√≠u')
    - 'ui' -> tone should be on 'i' (e.g., 'gu√¨' not 'g√∫i')
    
    Returns corrected pinyin.
    """
    if not pinyin:
        return pinyin
    
    import re
    from scripts.knowledge_graph.pinyin_parser import TONE_MARKS, extract_tone, add_tone_to_final
    
    # Split into syllables (space-separated)
    syllables = pinyin.split()
    fixed_syllables = []
    
    for syllable in syllables:
        # Extract tone from syllable
        syllable_no_tone, tone = extract_tone(syllable)
        
        if not tone:
            fixed_syllables.append(syllable)
            continue
        
        # Check for 'iu' pattern - tone should be on 'u' (the latter)
        if 'iu' in syllable_no_tone.lower():
            # Check if tone is currently on 'i' (wrong)
            i_tones = ['ƒ´', '√≠', '«ê', '√¨', 'ƒ™', '√ç', '«è', '√å']
            has_tone_on_i = any(i_tone in syllable for i_tone in i_tones)
            
            if has_tone_on_i:
                # Wrong: tone is on i, should be on u
                # Use add_tone_to_final which follows the i u Âπ∂ÂàóÊ†áÂú®Âêé rule
                fixed = add_tone_to_final(syllable_no_tone, tone)
                fixed_syllables.append(fixed)
            else:
                # Tone is already on u or correct, keep as is
                fixed_syllables.append(syllable)
        
        # Check for 'ui' pattern - tone should be on 'i' (the latter)
        elif 'ui' in syllable_no_tone.lower():
            # Check if tone is currently on 'u' (wrong)
            u_tones = ['≈´', '√∫', '«î', '√π', '≈™', '√ö', '«ì', '√ô']
            has_tone_on_u = any(u_tone in syllable for u_tone in u_tones)
            
            if has_tone_on_u:
                # Wrong: tone is on u, should be on i
                # Use add_tone_to_final which follows the i u Âπ∂ÂàóÊ†áÂú®Âêé rule
                fixed = add_tone_to_final(syllable_no_tone, tone)
                fixed_syllables.append(fixed)
            else:
                # Tone is already on i or correct, keep as is
                fixed_syllables.append(syllable)
        else:
            # No iu/ui pattern, keep as is
            fixed_syllables.append(syllable)
    
    return ' '.join(fixed_syllables)


def get_word_knowledge(word: str) -> Dict[str, Any]:
    """Combine knowledge graph data, cache, and LLM fallback for a Chinese word."""
    info = fetch_word_knowledge_points(word) or {}
    pronunciations: List[str] = list(dict.fromkeys(info.get("pronunciations") or []))
    meanings: List[str] = list(dict.fromkeys(info.get("meanings") or []))
    hsk_level = info.get("hsk_level")

    cached = _word_kp_cache.get(word) or {}
    cache_pinyin = cached.get("pinyin")
    cache_meaning = cached.get("meaning") or cached.get("meaning_en")
    cache_pron_list = cached.get("pronunciations") or []
    cache_meaning_list = cached.get("meanings") or []
    cache_hsk = cached.get("hsk_level")

    for val in [cache_pinyin]:
        if val and val not in pronunciations:
            pronunciations.append(val)
    for val in cache_pron_list:
        if val and val not in pronunciations:
            pronunciations.append(val)
    for val in [cache_meaning]:
        if val and val not in meanings:
            meanings.append(val)
    for val in cache_meaning_list:
        if val and val not in meanings:
            meanings.append(val)
    if not hsk_level and cache_hsk:
        hsk_level = cache_hsk

    needs_llm = (not pronunciations or not meanings) and _genai_model is not None
    if needs_llm:
        llm_info = _fetch_word_info_via_llm(word)
        updated = False
        llm_pinyin = llm_info.get("pinyin")
        llm_meaning = llm_info.get("meaning")
        if llm_pinyin and llm_pinyin not in pronunciations:
            pronunciations.append(llm_pinyin)
            updated = True
        if llm_meaning and llm_meaning not in meanings:
            meanings.append(llm_meaning)
            updated = True
        if updated:
            cache_entry = _word_kp_cache.get(word, {})
            cache_entry.setdefault("pronunciations", [])
            cache_entry.setdefault("meanings", [])
            if llm_pinyin and llm_pinyin not in cache_entry["pronunciations"]:
                cache_entry["pronunciations"].append(llm_pinyin)
            if llm_meaning and llm_meaning not in cache_entry["meanings"]:
                cache_entry["meanings"].append(llm_meaning)
            _word_kp_cache[word] = cache_entry
            _save_word_kp_cache()

    return {
        "pronunciations": pronunciations,
        "meanings": meanings,
        "hsk_level": hsk_level
    }


def expand_kp_template_variables(kp_pattern: str, context_tags: List[Dict[str, Any]]) -> str:
    """
    Expand template variables in knowledge point patterns.
    
    Supports variables:
    - {{word}} - the actual word (from @word: mention)
    - {{concept}} - the meaning/concept of the word (looked up from knowledge graph)
    - {{pronunciation}} - the pinyin pronunciation
    - {{hsk_level}} - the HSK level
    
    Example:
    - Input: "@{{word}}--means--{{concept}}"
    - If word is "ËØªËÄÖ" and concept is "reader", output: "@ËØªËÄÖ--means--reader"
    """
    if not kp_pattern or "{{" not in kp_pattern:
        return kp_pattern
    
    # Extract word from context_tags
    word_value = None
    for tag in context_tags:
        if tag.get("type") == "word":
            word_value = tag.get("value")
            break
    
    if not word_value:
        # No word found, return pattern as-is (variables won't be replaced)
        return kp_pattern
    
    # Look up word knowledge if we need concept, pronunciation, or hsk_level
    word_knowledge = {}
    if "{{concept}}" in kp_pattern or "{{pronunciation}}" in kp_pattern or "{{hsk_level}}" in kp_pattern:
        word_knowledge = get_word_knowledge(word_value)
    
    # Replace variables
    result = kp_pattern
    result = result.replace("{{word}}", word_value)
    
    if "{{concept}}" in result:
        # Use first meaning as concept
        meanings = word_knowledge.get("meanings", [])
        concept = meanings[0] if meanings else word_value  # Fallback to word itself
        result = result.replace("{{concept}}", concept)
    
    if "{{pronunciation}}" in result:
        pronunciations = word_knowledge.get("pronunciations", [])
        pronunciation = pronunciations[0] if pronunciations else ""
        result = result.replace("{{pronunciation}}", pronunciation)
    
    if "{{hsk_level}}" in result:
        hsk_level = word_knowledge.get("hsk_level", "")
        result = result.replace("{{hsk_level}}", str(hsk_level) if hsk_level else "")
    
    return result


def build_cuma_remarks(card: Dict[str, Any], context_tags: List[Dict[str, Any]]) -> str:
    """Construct the _Remarks field combining tags and knowledge point info."""
    lines: List[str] = []
    original_tags = card.get("tags", []) or []
    clean_tags, extracted_annotations = split_tag_annotations(original_tags)
    card["tags"] = clean_tags
    annotations = (card.get("field__Remarks_annotations") or []) + extracted_annotations
    kp_ids_set: Set[str] = set(card.get("knowledge_points") or [])
    knowledge_entries: List[str] = []
    knowledge_entries_seen: Set[str] = set()

    def add_entry(text: str):
        if not text:
            return
        formatted = text.strip()
        if not formatted:
            return
        if formatted not in knowledge_entries_seen:
            knowledge_entries.append(formatted)
            knowledge_entries_seen.add(formatted)

    def add_kp_entry(raw_kp: str):
        kp_value = (raw_kp or "").strip()
        if not kp_value:
            return
        # Ensure it has kp: prefix for storage
        if not kp_value.startswith("kp:"):
            stored_kp = f"kp:{kp_value}"
        else:
            stored_kp = kp_value
        kp_ids_set.add(stored_kp)
        
        # Parse the KP for readable display
        display_parts = kp_value.split("--", 2) if not kp_value.startswith("kp:") else kp_value[3:].split("--", 2)
        if len(display_parts) == 3:
            subj, pred, obj = display_parts
            # Create readable display format based on predicate
            pred_lower = pred.lower().replace('-', ' ')
            if pred_lower in ['means', 'has meaning', 'meaning']:
                # "ËØªËÄÖ means reader (concept)"
                display_text = f"{subj} means {obj} (concept)"
            elif pred_lower in ['has pronunciation', 'pronunciation', 'pronounced']:
                # "ËØªËÄÖ pronounced d√∫ zhƒõ"
                display_text = f"{subj} pronounced {obj}"
            elif pred_lower in ['has hsk level', 'hsk level', 'hsk']:
                # "ËØªËÄÖ HSK level 3"
                display_text = f"{subj} HSK level {obj}"
            elif pred_lower in ['has grammar rule', 'grammar rule', 'grammar']:
                # "Êää grammar rule: causative construction"
                display_text = f"{subj} grammar rule: {obj}"
            elif pred_lower in ['has part of speech', 'part of speech', 'pos']:
                # "ËØªËÄÖ part of speech: noun"
                display_text = f"{subj} part of speech: {obj}"
            else:
                # Generic format: "subject ‚Üí object (predicate)"
                display_text = f"{subj} ‚Üí {obj} ({pred.replace('-', ' ')})"
        else:
            # Fallback to raw format if parsing fails
            display_text = kp_value.replace("kp:", "").replace("--", " ‚Üí ")
        add_entry(display_text)

    # Seed knowledge points from card metadata
    for kp in sorted(kp_ids_set):
        add_kp_entry(kp)

    # Allow explicit @kp:... mentions to append
    for tag in context_tags or []:
        if tag.get("type") == "kp":
            value = (tag.get("value") or "").strip()
            if value:
                if not value.startswith("kp:"):
                    value = f"kp:{value}"
                add_kp_entry(value)
    
    for annotation in annotations:
        annotation_text = str(annotation).strip()
        if not annotation_text:
            continue
        if annotation_text.startswith("kp:"):
            add_kp_entry(annotation_text)
        else:
            add_entry(annotation_text)
    
    if knowledge_entries:
        lines.append("Knowledge Points:")
        for entry in knowledge_entries:
            lines.append(f"- {entry}")
    
    if clean_tags:
        lines.append("CUMA Tags: " + ", ".join(clean_tags))

    if kp_ids_set:
        card["knowledge_points"] = sorted(kp_ids_set)
    else:
        card.pop("knowledge_points", None)
    
    return "\n".join(lines).strip()
# Knowledge Graph Configuration
FUSEKI_ENDPOINT = "http://localhost:3030/srs4autism/query"

def query_sparql(sparql_query: str, output_format: str = "text/csv"):
    """Execute a SPARQL query against Jena Fuseki."""
    try:
        params = urlencode({"query": sparql_query})
        url = f"{FUSEKI_ENDPOINT}?{params}"
        response = requests.get(url, headers={"Accept": output_format}, timeout=10)
        response.raise_for_status()
        if output_format == "application/sparql-results+json":
            return response.json()
        return response.text
    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=503, detail=f"Knowledge graph server unavailable: {str(e)}")

def find_learning_frontier(mastered_words: List[str], target_level: int = 1, top_n: int = 20, concreteness_weight: float = 0.5, mental_age: Optional[float] = None):
    """
    Find words to learn next using the "Learning Frontier" algorithm with concreteness scoring and AoA filtering.
    
    Algorithm:
    1. Get all words with HSK levels, pinyin, concreteness ratings, and AoA
    2. Find words in the next level (Learning Frontier)
    3. Filter words by AoA if mental_age is provided
    4. Score words based on:
       - HSK level (learning frontier): weighted by (1 - concreteness_weight)
       - Concreteness (higher = more concrete = easier): weighted by concreteness_weight
       - AoA (lower = easier): bonus/penalty
       - Known characters (prerequisites): bonus points
       - Being too hard: penalty
    
    Args:
        mastered_words: List of mastered words
        target_level: Target HSK level to focus on
        top_n: Number of recommendations to return
        concreteness_weight: Weight for concreteness (0.0-1.0)
            - 0.0 = only HSK level matters
            - 1.0 = only concreteness matters
            - 0.5 = balanced (default)
        mental_age: Mental age for AoA filtering (e.g., 7.0 for a 7-year-old)
    """
    # Step 1: Get all words with HSK levels, pinyin, concreteness, and AoA
    # Only include words that have HSK levels (HSK 1-7 vocabulary)
    # Make pinyin optional since not all words have it
    sparql = f"""
    PREFIX srs-kg: <http://srs4autism.com/schema/>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    
    SELECT ?word ?word_text ?pinyin ?hsk ?concreteness ?aoa WHERE {{
        ?word a srs-kg:Word ;
              srs-kg:text ?word_text ;
              srs-kg:hskLevel ?hsk .
        OPTIONAL {{ ?word srs-kg:pinyin ?pinyin }}
        OPTIONAL {{ ?word srs-kg:concreteness ?concreteness }}
        OPTIONAL {{ ?word srs-kg:ageOfAcquisition ?aoa }}
    }}
    """
    
    csv_result = query_sparql(sparql, "text/csv")
    
    # Parse results using proper CSV parser
    words_data = defaultdict(lambda: {'pinyin': '', 'hsk': None, 'concreteness': None, 'aoa': None, 'chars': set()})
    reader = csv.reader(io.StringIO(csv_result))
    header = next(reader)  # Skip header
    print(f"   üìä CSV Header: {header}")
    
    mastered_set = set(mastered_words)
    
    words_with_concreteness = 0
    words_with_aoa = 0
    total_words = 0
    
    for row in reader:
        if len(row) >= 4:
            total_words += 1
            word_text = row[1]  # word_text is the second column
            pinyin = row[2] if len(row) > 2 else ''
            try:
                hsk = int(row[3]) if len(row) > 3 and row[3] else None
            except ValueError:
                hsk = None
            
            # Parse concreteness (optional, may be empty)
            # CSV format: word, word_text, pinyin, hsk, concreteness, aoa
            concreteness = None
            if len(row) > 4 and row[4] and row[4].strip():
                try:
                    concreteness = float(row[4].strip())
                    words_with_concreteness += 1
                except (ValueError, TypeError) as e:
                    concreteness = None
            
            # Parse AoA (optional, may be empty)
            aoa = None
            if len(row) > 5 and row[5] and row[5].strip():
                try:
                    aoa = float(row[5].strip())
                    words_with_aoa += 1
                except (ValueError, TypeError) as e:
                    aoa = None
            
            words_data[word_text]['pinyin'] = pinyin
            words_data[word_text]['hsk'] = hsk
            words_data[word_text]['concreteness'] = concreteness
            words_data[word_text]['aoa'] = aoa
    
    if total_words > 0:
        print(f"   üìà Loaded {total_words} words, {words_with_concreteness} with concreteness ({words_with_concreteness/total_words*100:.1f}%), {words_with_aoa} with AoA ({words_with_aoa/total_words*100:.1f}%)")
    else:
        print(f"   ‚ö†Ô∏è  Warning: No words loaded from SPARQL query!")
    
    # AoA filtering: exclude words with AoA > mental_age + buffer (if mental_age is set)
    aoa_buffer = 2.0
    if mental_age is not None:
        aoa_ceiling = mental_age + aoa_buffer
        filtered_count = 0
        words_to_remove = []
        for word_text, data in words_data.items():
            if data['aoa'] is not None and data['aoa'] > aoa_ceiling:
                words_to_remove.append(word_text)
                filtered_count += 1
        for word_text in words_to_remove:
            del words_data[word_text]
        if filtered_count > 0:
            print(f"   üéØ Filtered out {filtered_count} words with AoA > {aoa_ceiling:.1f} (mental_age={mental_age:.1f} + buffer={aoa_buffer})")
    
    # Step 2: Get all character composition data in one query
    sparql_all_chars = """
    PREFIX srs-kg: <http://srs4autism.com/schema/>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    
    SELECT ?word_label ?char_label WHERE {
        ?word a srs-kg:Word ;
              srs-kg:composedOf ?char ;
              rdfs:label ?word_label .
        ?char rdfs:label ?char_label .
    }
    """
    
    try:
        char_result = query_sparql(sparql_all_chars, "text/csv")
        char_reader = csv.reader(io.StringIO(char_result))
        next(char_reader)  # Skip header
        
        for row in char_reader:
            if len(row) >= 2:
                word_text = row[0]
                char_text = row[1]
                if word_text in words_data:
                    words_data[word_text]['chars'].add(char_text)
    except Exception as e:
        print(f"Warning: Could not load character data: {e}")
    
    # Step 3: Score words with concreteness and HSK level balance
    scored_words = []
    
    # Normalize concreteness_weight to 0-1 range
    concreteness_weight = max(0.0, min(1.0, concreteness_weight))
    hsk_weight = 1.0 - concreteness_weight
    
    print(f"   ‚öñÔ∏è  Scoring with concreteness_weight={concreteness_weight:.2f}, hsk_weight={hsk_weight:.2f}")
    
    for word, data in words_data.items():
        if word in mastered_set:
            continue  # Skip already mastered words
        
        hsk_score_raw = 0.0
        concreteness_score_raw = 0.0
        
        # HSK level scoring (raw values, will be normalized)
        if data['hsk'] is not None:
            if data['hsk'] == target_level:
                hsk_score_raw = 100.0  # Target level gets highest priority
            elif data['hsk'] == target_level + 1:
                hsk_score_raw = 50.0   # Next level gets medium priority
            elif data['hsk'] < target_level:
                hsk_score_raw = 25.0  # Lower levels get small bonus (review)
            elif data['hsk'] > target_level + 1:
                hsk_score_raw = -500.0  # Too hard gets penalized
        else:
            # No HSK level, give small baseline
            hsk_score_raw = 10.0
        
        # Concreteness scoring (raw values, will be normalized)
        # Higher concreteness = easier to learn = higher score
        # Concreteness is on 1-5 scale
        if data['concreteness'] is not None:
            # Raw concreteness value (1.0 to 5.0)
            concreteness_score_raw = data['concreteness']
        else:
            # No concreteness data, use neutral value (middle of range: 3.0)
            # BUT: if concreteness_weight is high, words without concreteness should be penalized
            # So we use a lower neutral value when weight is high
            concreteness_score_raw = 3.0 * (1.0 - concreteness_weight * 0.5)  # Scale down neutral when weight is high
        
        # Normalize both scores to 0-100 scale for fair comparison
        # BUT: We want to preserve the relative importance of HSK level differences
        # The issue: if we normalize -500 to 100, we lose the distinction between target level and too hard
        
        # HSK score normalization: 
        # - Target level (100) should stay at 100 (highest priority)
        # - Next level (50) should stay at 50 (medium priority)  
        # - Lower levels (25) should stay at 25 (review)
        # - Too hard (-500) should be 0 (excluded)
        # - No level (10) should stay at 10 (baseline)
        # So we keep positive scores as-is, and map negative to 0
        if hsk_score_raw <= 0:
            hsk_score = 0.0  # Too hard or negative scores get 0
        else:
            # Keep positive scores as-is (they're already in 0-100 range)
            hsk_score = hsk_score_raw
        
        # Concreteness score normalization: 1.0 (min) to 5.0 (max) -> 0 to 100
        # Formula: (raw - 1) / (5 - 1) * 100
        concreteness_score = max(0.0, min(100.0, ((concreteness_score_raw - 1.0) / 4.0) * 100.0))
        
        # AoA bonus/penalty (if AoA data available)
        # Lower AoA = easier = bonus, Higher AoA = harder = penalty
        aoa_bonus = 0.0
        if data['aoa'] is not None:
            # Normalize AoA to 0-1 scale (lower AoA = higher score)
            # Assuming typical range: 2-15 years
            aoa_score = max(0.0, 1.0 - (data['aoa'] / 15.0))
            # Add AoA as a bonus/penalty (scale: -20 to +20 points)
            # Words learned earlier (lower AoA) get bonus, words learned later get penalty
            aoa_bonus = (aoa_score - 0.5) * 40.0  # Maps 0-1 to -20 to +20
        
        # Combine scores with weights
        combined_score = (hsk_score * hsk_weight) + (concreteness_score * concreteness_weight)
        
        # Count known characters (prerequisites) - bonus points
        known_chars = sum(1 for char in data['chars'] if char in mastered_set)
        total_chars = len(data['chars']) if data['chars'] else 1
        char_bonus = 0.0
        if total_chars > 0:
            char_ratio = known_chars / total_chars
            char_bonus = 50.0 * char_ratio  # Bonus based on ratio of known chars
        
        final_score = combined_score + char_bonus + aoa_bonus
        
        if final_score > 0:  # Only include words with positive scores
            scored_words.append({
                'word': word,
                'pinyin': data['pinyin'],
                'hsk': data['hsk'],
                'score': final_score,
                'known_chars': known_chars,
                'total_chars': len(data['chars']),
                'concreteness': data['concreteness'],
                'age_of_acquisition': data['aoa']  # Add AoA to response
            })
    
    # Sort by score and return top N
    scored_words.sort(key=lambda x: x['score'], reverse=True)
    
    # Debug: Show top 5 scores with breakdown
    if scored_words:
        print(f"   üîù Top 5 scores (target_level={target_level}):")
        for i, item in enumerate(scored_words[:5]):
            word_data = words_data.get(item['word'], {})
            hsk = item['hsk']
            conc = word_data.get('concreteness', None)
            score = item['score']
            # Calculate what the scores would have been
            hsk_raw = 0.0
            if hsk == target_level:
                hsk_raw = 100.0
            elif hsk == target_level + 1:
                hsk_raw = 50.0
            elif hsk and hsk < target_level:
                hsk_raw = 25.0
            elif hsk and hsk > target_level + 1:
                hsk_raw = -500.0
            hsk_norm = min(100.0, hsk_raw) if hsk_raw > 0 else 0.0
            conc_norm = ((conc - 1.0) / 4.0 * 100.0) if conc else 50.0
            hsk_contrib = hsk_norm * hsk_weight
            conc_contrib = conc_norm * concreteness_weight
            char_bonus = item.get('known_chars', 0) / max(item.get('total_chars', 1), 1) * 50.0
            print(f"      {i+1}. {item['word']}: final={score:.1f} (HSK={hsk}‚Üí{hsk_norm:.0f}*{hsk_weight:.2f}={hsk_contrib:.1f}, conc={conc if conc else 'N/A'}‚Üí{conc_norm:.0f}*{concreteness_weight:.2f}={conc_contrib:.1f}, chars={char_bonus:.1f})")
    
    return scored_words[:top_n]

def parse_context_tags(content: str, mentions: List[str]) -> List[Dict[str, Any]]:
    """
    Parse @mentions from message content into structured context tags.
    
    Supports formats:
    - @profile:Alex -> {"type": "profile", "value": "Alex"}
    - @interest:trains -> {"type": "interest", "value": "trains"}
    - @word:Á∫¢Ëâ≤ -> {"type": "word", "value": "Á∫¢Ëâ≤"}
    - @skill:grammar-001 -> {"type": "skill", "value": "grammar-001"}
    - @character:Pinocchio -> {"type": "character", "value": "Pinocchio"}
    - @notetype:cuma-interactive-cloze -> {"type": "notetype", "value": "CUMA - Interactive Cloze"}
    - @template:my_template -> {"type": "template", "value": "my_template"}
    - @Alex (plain mention) -> {"type": "profile", "value": "Alex"}
    """
    import re
    
    context_tags = []
    
    # Find special standalone @roster mention (no colon)
    # Match @roster as a whole word (not part of another word)
    if re.search(r'(?:^|[\s,])@roster(?:[\s,]|$)', content):
        context_tags.append({
            "type": "roster",
            "value": "roster"
        })
        print("‚úÖ Detected @roster mention")
    
    # Special handling for knowledge point mentions
    # Supports two formats:
    # 1. @kp:subject--predicate--object (explicit format)
    # 2. @subject--predicate--object (simplified format, e.g., @ËØªËÄÖ--means--reader)
    # Match everything after @ until whitespace+@ (next mention) or end of line/string
    # Pattern: @[^@\s]+--[^@\s]+--[^@\n]+?(?=\s+@|[\s,]*$|[\s,]*\n)
    
    # First, try @kp: format
    kp_explicit_pattern = r'@kp:([^@\n]+?)(?=\s+@|[\s,]*$|[\s,]*\n)'
    kp_explicit_matches = re.findall(kp_explicit_pattern, content)
    for kp_value in kp_explicit_matches:
        kp_value = kp_value.strip().rstrip(',')
        if kp_value:
            context_tags.append({
                "type": "kp",
                "value": kp_value
            })
            print(f"üìå Parsed knowledge point (explicit): {kp_value}")
    
    # Then, try simplified format: @subject--predicate--object
    # This matches patterns like @ËØªËÄÖ--means--reader
    # But avoid matching if it's already a known mention type (like @profile:, @word:, etc.)
    kp_simplified_pattern = r'@([^@\s:]+)--([^@\s]+)--([^@\n]+?)(?=\s+@|[\s,]*$|[\s,]*\n)'
    kp_simplified_matches = re.findall(kp_simplified_pattern, content)
    for subj, pred, obj in kp_simplified_matches:
        # Skip if this looks like a regular mention (e.g., @profile:value)
        # Only accept if it's in the format subject--predicate--object
        # Common predicates: means, has-meaning, has-pronunciation, has-hsk-level, etc.
        if pred and '--' not in subj and '--' not in pred:
            kp_value = f"{subj}--{pred}--{obj.strip()}"
            # Check if this KP was already added (from @kp: format)
            already_exists = any(
                t.get("type") == "kp" and t.get("value") == kp_value
                for t in context_tags
            )
            if not already_exists:
                context_tags.append({
                    "type": "kp",
                    "value": kp_value
                })
                print(f"üìå Parsed knowledge point (simplified): @{kp_value}")
    
    # Find all other @type:value patterns (excluding @kp: which we already handled)
    # Pattern requires word boundary before @ to avoid matching @ inside values
    # Example: @template:English_Word_Cloze should NOT match @Word as a separate mention
    # Exclude @kp: from this pattern since we already handled it
    pattern = r'(?:^|[\s,])@(?!kp:)(\w+):([^\s,@]+)'
    matches = re.findall(pattern, content)
    
    for tag_type, tag_value in matches:
        # Skip if it's roster:roster (old format)
        if tag_type == 'roster' and tag_value == 'roster':
            continue
        if tag_type == 'profile':
            value_normalized = normalize_to_slug(tag_value)
        elif tag_type == 'notetype':
            value_normalized = resolve_note_type_name(tag_value)
        else:
            value_normalized = tag_value
        context_tags.append({
            "type": tag_type,
            "value": value_normalized
        })
    
    # Add plain mentions as profile references
    for mention in mentions:
        # Check if this mention is already in context_tags
        already_added = any(t['value'] == mention for t in context_tags)
        if not already_added and mention != 'roster':
            context_tags.append({
                "type": "profile",
                "value": mention
            })
    
    return context_tags

# API Routes

@app.get("/")
async def root():
    return {"message": "Curious Mario API is running"}

# Child Profile endpoints
@app.get("/profiles", response_model=List[ChildProfile])
async def get_profiles(db: Session = Depends(get_db)):
    """Get all profiles from database"""
    profiles = ProfileService.get_all(db)
    return [ProfileService.profile_to_dict(db, p) for p in profiles]

@app.post("/profiles", response_model=ChildProfile)
async def create_profile(profile: ChildProfile, db: Session = Depends(get_db)):
    """Create new profile in database"""
    import sys
    import os
    sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
    from utils import generate_slug
    
    # Generate slug-based ID from name if not provided
    if not profile.id:
        profile.id = generate_slug(profile.name)
    
    # Ensure uniqueness
    existing = ProfileService.get_by_id(db, profile.id)
    if existing:
        counter = 2
        base_id = profile.id
        while ProfileService.get_by_id(db, f"{base_id}-{counter}"):
            counter += 1
        profile.id = f"{base_id}-{counter}"
    
    # Prepare data for database
    profile_data = profile.dict()
    
    # Split mastered words into lists
    mastered_words_str = profile_data.pop('mastered_words', '') or ''
    mastered_english_str = profile_data.pop('mastered_english_words', '') or ''
    mastered_grammar_str = profile_data.pop('mastered_grammar', '') or ''
    
    profile_data['mastered_words_list'] = [w.strip() for w in mastered_words_str.split(', ') if w.strip()]
    profile_data['mastered_english_words_list'] = [w.strip() for w in mastered_english_str.split(', ') if w.strip()]
    profile_data['mastered_grammar_list'] = [g.strip() for g in mastered_grammar_str.split(',') if g.strip()]
    
    created_profile = ProfileService.create(db, profile_data)
    return ProfileService.profile_to_dict(db, created_profile)

@app.get("/profiles/{profile_id}", response_model=ChildProfile)
async def get_profile(profile_id: str, db: Session = Depends(get_db)):
    """Get specific profile from database"""
    profile = ProfileService.get_by_id(db, profile_id)
    if not profile:
        # Try to find by name for backward compatibility
        all_profiles = ProfileService.get_all(db)
        for p in all_profiles:
            if p.name == profile_id:
                return ProfileService.profile_to_dict(db, p)
        raise HTTPException(status_code=404, detail="Profile not found")
    return ProfileService.profile_to_dict(db, profile)

@app.put("/profiles/{profile_id}", response_model=ChildProfile)
async def update_profile(profile_id: str, updated_profile: ChildProfile, db: Session = Depends(get_db)):
    """Update profile in database"""
    print(f"\nüìù Updating profile: {profile_id}")
    print(f"   Received data: name={updated_profile.name}, mental_age={updated_profile.mental_age}")
    
    # Find profile by ID or name
    profile = ProfileService.get_by_id(db, profile_id)
    if not profile:
        all_profiles = ProfileService.get_all(db)
        for p in all_profiles:
            if p.name == profile_id:
                profile = p
                profile_id = p.id
                break
    
    if not profile:
        print(f"   ‚ùå Profile not found: {profile_id}")
        raise HTTPException(status_code=404, detail="Profile not found")
    
    print(f"   ‚úÖ Found profile: {profile.name} (id: {profile_id})")
    
    # Prepare data for database
    profile_data = updated_profile.dict()
    
    # Remove id field - we don't update the ID, and it might be None which causes constraint errors
    profile_data.pop('id', None)
    
    # Split mastered words into lists
    mastered_words_str = profile_data.pop('mastered_words', '') or ''
    mastered_english_str = profile_data.pop('mastered_english_words', '') or ''
    mastered_grammar_str = profile_data.pop('mastered_grammar', '') or ''
    
    profile_data['mastered_words_list'] = [w.strip() for w in mastered_words_str.split(', ') if w.strip()]
    profile_data['mastered_english_words_list'] = [w.strip() for w in mastered_english_str.split(', ') if w.strip()]
    profile_data['mastered_grammar_list'] = [g.strip() for g in mastered_grammar_str.split(',') if g.strip()]
    
    updated = ProfileService.update(db, profile_id, profile_data)
    print(f"   ‚úÖ Profile updated successfully")
    return ProfileService.profile_to_dict(db, updated)

@app.delete("/profiles/{profile_id}")
async def delete_profile(profile_id: str, db: Session = Depends(get_db)):
    """Delete profile from database"""
    # Find profile by ID or name
    profile = ProfileService.get_by_id(db, profile_id)
    if not profile:
        all_profiles = ProfileService.get_all(db)
        for p in all_profiles:
            if p.name == profile_id:
                profile_id = p.id
                break
    
    success = ProfileService.delete(db, profile_id)
    if not success:
        raise HTTPException(status_code=404, detail="Profile not found")
    
    return {"message": "Profile deleted successfully"}

# Card management endpoints
@app.get("/cards")
async def get_cards():
    cards = load_json_file(CARDS_FILE, [])
    # Normalize tags field - convert string to list if needed
    for card in cards:
        if isinstance(card.get('tags'), str):
            # Split comma-separated string into list
            card['tags'] = [t.strip() for t in card['tags'].split(',') if t.strip()]
        clean_tags, extracted_annotations = split_tag_annotations(card.get("tags", []))
        card['tags'] = clean_tags
        if extracted_annotations:
            existing_annotations = card.get("field__Remarks_annotations") or []
            card["field__Remarks_annotations"] = existing_annotations + extracted_annotations
        # Remove large binary payloads to keep response lightweight
        has_image = bool(card.get("image_data"))
        card["has_image_data"] = has_image
        if has_image:
            card.pop('image_data', None)
        if 'generated_image' in card and isinstance(card['generated_image'], dict):
            card['generated_image'].pop('data', None)
    return cards

@app.get("/cards/{card_id}/image-data")
async def get_card_image_data(card_id: str):
    cards = load_json_file(CARDS_FILE, [])
    for card in cards:
        if card.get("id") == card_id:
            image_data = card.get("image_data")
            if not image_data:
                raise HTTPException(status_code=404, detail="No image data found for this card")
            return {"image_data": image_data}
    raise HTTPException(status_code=404, detail="Card not found")

@app.post("/cards", response_model=Card)
async def create_card(card: Card):
    cards = load_json_file(CARDS_FILE, [])
    cards.append(card.dict())
    save_json_file(CARDS_FILE, cards)
    return card

@app.put("/cards/{card_id}/approve")
async def approve_card(card_id: str):
    cards = load_json_file(CARDS_FILE, [])
    for card in cards:
        if card["id"] == card_id:
            card["status"] = "approved"
            save_json_file(CARDS_FILE, cards)
            return {"message": "Card approved"}
    raise HTTPException(status_code=404, detail="Card not found")

@app.put("/cards/{card_id}")
async def update_card(card_id: str, updated_card: Card):
    cards = load_json_file(CARDS_FILE, [])
    card_found = False
    
    for i, card in enumerate(cards):
        if card["id"] == card_id:
            # Preserve the original ID
            card_data = updated_card.dict()
            card_data["id"] = card_id
            cards[i] = card_data
            card_found = True
            break
    
    if not card_found:
        raise HTTPException(status_code=404, detail="Card not found")
    
    save_json_file(CARDS_FILE, cards)
    return updated_card

@app.delete("/cards/{card_id}")
async def delete_card(card_id: str):
    cards = load_json_file(CARDS_FILE, [])
    initial_count = len(cards)
    cards = [card for card in cards if card["id"] != card_id]
    
    if len(cards) == initial_count:
        raise HTTPException(status_code=404, detail="Card not found")
    
    save_json_file(CARDS_FILE, cards)
    return {"message": "Card deleted successfully"}

@app.post("/cards/{card_id}/generate-image")
async def generate_card_image(card_id: str, request: CardImageRequest):
    cards = load_json_file(CARDS_FILE, [])
    card_index = next((idx for idx, card in enumerate(cards) if card.get("id") == card_id), None)
    
    if card_index is None:
        raise HTTPException(status_code=404, detail="Card not found")
    
    card = cards[card_index]
    sanitized_card = {
        **card,
        "front": extract_plain_text(card.get("front", "")),
        "back": extract_plain_text(card.get("back", "")),
        "text_field": extract_plain_text(card.get("text_field", "")),
        "extra_field": extract_plain_text(card.get("extra_field", "")),
        "cloze_text": extract_plain_text(card.get("cloze_text", ""))
    }
    
    try:
        from agent.conversation_handler import ConversationHandler
        conversation_handler = ConversationHandler()
        
        primary_text = (
            sanitized_card.get("text_field")
            or sanitized_card.get("cloze_text")
            or sanitized_card.get("front")
            or sanitized_card.get("back")
            or ""
        )
        user_request = request.user_request or f"Generate an illustration for this flashcard content: {primary_text}"
        
        image_description = conversation_handler._generate_image_description(
            card_content=sanitized_card,
            user_request=user_request,
            child_profile=None
        )
        
        image_result = conversation_handler.generate_actual_image(
            image_description=image_description,
            user_request=user_request
        )
        
        card["image_description"] = image_description
        card["image_generated"] = image_result.get("success", False)
        card["image_error"] = image_result.get("error")
        card["is_placeholder"] = image_result.get("is_placeholder", False)
        card["image_prompt"] = image_result.get("prompt_used")
        card["image_url"] = image_result.get("image_url")
        card["image_data"] = image_result.get("image_data")
        
        image_html = None
        if image_result.get("success") and image_result.get("image_data"):
            image_html = (
                f'<img src="{image_result["image_data"]}" alt="Generated image" '
                'style="max-width: 100%; height: auto; margin: 0 0 10px 0; border-radius: 8px; border: 1px solid #dee2e6;" />'
            )
            
            position = (request.position or "front").lower()
            location = (request.location or "before").lower()
            
            card_type = card.get("card_type", "")
            if card_type == "interactive_cloze":
                target_field = "text_field"
            elif card_type == "cloze":
                target_field = "cloze_text"
            else:
                target_field = "front" if position != "back" else "back"
            
            current_content = card.get(target_field) or ""
            if location == "before":
                new_content = f"{image_html}\n{current_content}".strip()
            else:
                new_content = f"{current_content}\n{image_html}".strip()
            card[target_field] = new_content
        
        cards[card_index] = card
        save_json_file(CARDS_FILE, cards)
        
        message = "Generated image description."
        if image_result.get("success"):
            message = "Image generated and added to card."
        elif image_result.get("error"):
            message = image_result.get("error")
        
        return {
            "message": message,
            "card": card,
            "image": {
                "success": image_result.get("success", False),
                "is_placeholder": image_result.get("is_placeholder", False),
                "error": image_result.get("error"),
                "description": image_description
            }
        }
        
    except Exception as e:
        print(f"Image generation endpoint error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to generate image: {str(e)}")

# Chat endpoints
@app.get("/chat/history", response_model=List[ChatMessage])
async def get_chat_history():
    """Get chat history."""
    history = load_json_file(CHAT_HISTORY_FILE, [])
    return history

@app.delete("/chat/history")
async def clear_chat_history():
    """Clear chat history."""
    save_json_file(CHAT_HISTORY_FILE, [])
    return {"message": "Chat history cleared"}

@app.post("/chat", response_model=ChatMessage)
async def send_message(message: ChatMessage):
    try:
        # Save user message to history
        history = load_json_file(CHAT_HISTORY_FILE, [])
        history.append(message.dict())
        save_json_file(CHAT_HISTORY_FILE, history)
        
        # Import intent detection and conversation handler
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        
        try:
            from agent.intent_detector import IntentDetector, IntentType
            from agent.conversation_handler import ConversationHandler
            from agent.content_generator import ContentGenerator
            
            # Initialize handlers
            intent_detector = IntentDetector()
            conversation_handler = ConversationHandler()
            generator = ContentGenerator()
            
            # Parse @mentions from the message
            context_tags = parse_context_tags(message.content, message.mentions)
            
            # Detect user intent
            intent_result = intent_detector.detect_intent(message.content, context_tags)
            intent_type = intent_result["intent"]
            confidence = intent_result["confidence"]
            reason = intent_result["reason"]
            
            print(f"\nüéØ INTENT DETECTION:")
            print(f"   Intent: {intent_type.value}")
            print(f"   Confidence: {confidence}")
            print(f"   Reason: {reason}")
            print(f"   Entities: {intent_result.get('entities', {})}")
            
            # Get child profile from mentions if specified
            child_profile = None
            profiles = load_json_file(PROFILES_FILE, [])
            for mention in message.mentions:
                for profile in profiles:
                    # Match by ID (including slugs), then by name
                    # Handle both old UUIDs and new slugs
                    profile_id_raw = (profile.get("id") or "").lower()
                    profile_name_slug = normalize_to_slug(profile.get("name", ""))
                    mention_lower = mention.lower()
                    mention_slug = normalize_to_slug(mention)
                    
                    if (profile_id_raw and profile_id_raw == mention_lower or
                        profile.get("name") == mention or 
                        profile_name_slug and profile_name_slug == mention_slug or
                        (profile_id_raw and profile_id_raw.endswith(mention_lower)) or  # For partial UUID matching
                        (profile_id_raw and mention_lower.endswith(profile_id_raw)) or
                        (profile_name_slug and mention_slug.endswith(profile_name_slug))):
                        child_profile = profile
                        print(f"üìã Found profile: {profile.get('name')} (ID: {profile.get('id')})")
                        break
                if child_profile:
                    break
            
            # Handle different intents
            if intent_type == IntentType.CONVERSATION:
                # Handle conversational messages
                response_content = conversation_handler.handle_conversation(
                    message=message.content,
                    context_tags=context_tags,
                    child_profile=child_profile,
                    chat_history=history[-5:]  # Last 5 messages for context
                )
                
            elif intent_type == IntentType.CARD_GENERATION:
                # Handle card generation requests
                response_content = await _handle_card_generation(
                    message, context_tags, child_profile, generator, profiles
                )
                
            elif intent_type == IntentType.IMAGE_GENERATION:
                # Handle image generation requests
                response_content = await _handle_image_generation(
                    message, context_tags, child_profile, generator, profiles
                )
                
            elif intent_type == IntentType.IMAGE_INSERTION:
                # Handle image insertion requests
                response_content = await _handle_image_insertion(
                    message, context_tags, child_profile
                )
                
            elif intent_type == IntentType.CARD_UPDATE:
                # Handle card update requests (placeholder for now)
                response_content = "‚úèÔ∏è Card update feature is coming soon! For now, you can edit cards in the Card Curation tab."
                
            else:
                # Fallback to conversation
                response_content = conversation_handler.handle_conversation(
                    message=message.content,
                    context_tags=context_tags,
                    child_profile=child_profile,
                    chat_history=history[-5:]
                )
            
        except ImportError as e:
            print(f"Agent import error: {e}")
            # Fallback response when agent is not available
            response_content = f"I received your message: '{message.content}'. "
            response_content += "The AI agent is currently not available, but I can help you create flashcards manually. "
            response_content += "Please use the Card Curation tab to add cards."
            
    except Exception as e:
        print(f"Chat error: {e}")
        response_content = f"I encountered an error processing your message: '{message.content}'. Please try again."
    
    response = ChatMessage(
        id=f"resp_{datetime.now().timestamp()}",
        content=response_content,
        role="assistant",
        timestamp=datetime.now(),
        mentions=message.mentions
    )
    
    # Save assistant response to history
    history = load_json_file(CHAT_HISTORY_FILE, [])
    history.append(response.dict())
    save_json_file(CHAT_HISTORY_FILE, history)
    
    return response

async def _handle_card_generation(message: ChatMessage, context_tags: List[Dict[str, Any]], 
                                child_profile: Dict[str, Any], generator,
                                profiles: List[Dict[str, Any]]) -> str:
    """Handle card generation requests."""
    try:
        # Check for @roster mention - use entire character roster from profile
        has_roster_mention = any(tag.get("type") == "roster" for tag in context_tags)
        
        if has_roster_mention:
            # If no specific profile mentioned, use the first available profile
            if not child_profile and profiles:
                child_profile = profiles[0]
                print(f"üìã No profile specified with @roster, using first profile: {child_profile.get('name')}")
            
            if child_profile and child_profile.get("character_roster"):
                characters_str = ", ".join(child_profile["character_roster"])
                context_tags.append({
                    "type": "character_list",
                    "value": characters_str
                })
                print(f"üé≠ Using character roster: {characters_str}")
        
        # Get prompt template if specified
        prompt_template = None
        for tag in context_tags:
            if tag.get("type") == "template":
                template_value = tag.get("value")
                templates = load_json_file(PROMPT_TEMPLATES_FILE, [])
                print(f"Looking for template: {template_value}")
                print(f"Available templates: {[t.get('name') for t in templates]}")
                
                for tmpl in templates:
                    # Match by ID, name, or name with underscores/hyphens
                    # Normalize spaces, underscores, and hyphens for matching
                    template_name_normalized = tmpl.get("name", "").replace(' ', '_').lower()
                    template_value_normalized = template_value.replace('_', '_').replace('-', '_').lower()
                    
                    if (tmpl.get("id") == template_value or 
                        tmpl.get("name") == template_value or
                        template_name_normalized == template_value_normalized or
                        template_name_normalized.replace('_', '') == template_value_normalized.replace('_', '') or
                        tmpl.get("name", "").lower().replace(' ', '-') == template_value.lower().replace('_', '-')):
                        prompt_template = tmpl.get("template_text")
                        print(f"‚úÖ Found template: {tmpl.get('name')}")
                        
                        # Parse knowledge point mentions from template text and add to context_tags
                        import re
                        # First, try @kp: format (may contain template variables)
                        template_kp_explicit_pattern = r'@kp:([^@\n]+?)(?=\s+@|[\s,]*$|[\s,]*\n)'
                        template_kp_explicit_matches = re.findall(template_kp_explicit_pattern, prompt_template)
                        for kp_pattern in template_kp_explicit_matches:
                            kp_pattern = kp_pattern.strip().rstrip(',')
                            if not kp_pattern:
                                continue
                            # Expand template variables (e.g., {{word}}, {{concept}})
                            kp_value = expand_kp_template_variables(kp_pattern, context_tags)
                            if kp_value == kp_pattern and "{{" in kp_pattern:
                                # Variables couldn't be expanded, skip this KP
                                print(f"‚ö†Ô∏è Could not expand knowledge point template variables: {kp_pattern}")
                                continue
                            # Check if this KP is already in context_tags (from user message)
                            kp_already_exists = any(
                                t.get("type") == "kp" and t.get("value") == kp_value
                                for t in context_tags
                            )
                            if not kp_already_exists:
                                context_tags.append({
                                    "type": "kp",
                                    "value": kp_value
                                })
                                print(f"üìå Added knowledge point from template (explicit, expanded): {kp_value}")
                        
                        # Then, try simplified format: @subject--predicate--object (may contain template variables)
                        # Use a more flexible pattern that captures the whole KP pattern including template variables
                        template_kp_simplified_pattern = r'@([^@\n]+?--[^@\n]+?--[^@\n]+?)(?=\s+@|[\s,]*$|[\s,]*\n)'
                        template_kp_simplified_matches = re.findall(template_kp_simplified_pattern, prompt_template)
                        for kp_pattern_raw in template_kp_simplified_matches:
                            kp_pattern_raw = kp_pattern_raw.strip().rstrip(',')
                            if not kp_pattern_raw:
                                continue
                            # Check if this looks like a knowledge point (has -- separators)
                            if kp_pattern_raw.count('--') != 2:
                                continue
                            # Expand template variables (e.g., {{word}}, {{concept}})
                            kp_value = expand_kp_template_variables(f"@{kp_pattern_raw}", context_tags)
                            # Remove leading @ if it was added
                            if kp_value.startswith("@"):
                                kp_value = kp_value[1:]
                            if kp_value == kp_pattern_raw and "{{" in kp_pattern_raw:
                                # Variables couldn't be expanded, skip this KP
                                print(f"‚ö†Ô∏è Could not expand knowledge point template variables: @{kp_pattern_raw}")
                                continue
                            # Check if this KP is already in context_tags
                            kp_already_exists = any(
                                t.get("type") == "kp" and t.get("value") == kp_value
                                for t in context_tags
                            )
                            if not kp_already_exists:
                                context_tags.append({
                                    "type": "kp",
                                    "value": kp_value
                                })
                                print(f"üìå Added knowledge point from template (simplified, expanded): {kp_value}")
                        break
                
                if not prompt_template:
                    print(f"‚ùå Template not found: {template_value}")
        
        # Use the flexible agent method
        cards = generator.generate_from_prompt(
            user_prompt=message.content,
            context_tags=context_tags,
            child_profile=child_profile,
            prompt_template=prompt_template
        )
        
        # Save generated cards
        existing_cards = load_json_file(CARDS_FILE, [])
        for card in cards:
            remarks = build_cuma_remarks(card, context_tags)
            card["field__Remarks"] = remarks or ""
            card.pop("field__Remarks_annotations", None)
            existing_cards.append(card)
        save_json_file(CARDS_FILE, existing_cards)
        
        # Create response message
        response_content = f"‚ú® Generated {len(cards)} flashcard(s) from your request!\n\n"
        response_content += f"üìù Created {len([c for c in cards if c['card_type'] == 'basic'])} basic, "
        response_content += f"{len([c for c in cards if c['card_type'] == 'basic_reverse'])} reverse, "
        response_content += f"and {len([c for c in cards if c['card_type'] == 'cloze'])} cloze cards.\n\n"
        
        if context_tags:
            # For display, show profile name instead of ID
            tag_strings = []
            for t in context_tags:
                if t['type'] == 'profile' and child_profile:
                    tag_strings.append(f"profile={child_profile.get('name')}")
                else:
                    tag_strings.append(f"{t['type']}={t['value']}")
            response_content += f"üéØ Applied context: {', '.join(tag_strings)}\n\n"
        
        response_content += "üëâ Review and approve them in the Card Curation tab!"
        
        return response_content
        
    except Exception as e:
        print(f"Card generation error: {e}")
        return f"I encountered an error generating cards: {str(e)}. Please try again with a different request."

async def _handle_image_generation(message: ChatMessage, context_tags: List[Dict[str, Any]], 
                                 child_profile: Dict[str, Any], generator,
                                 profiles: List[Dict[str, Any]]) -> str:
    """Handle image generation requests."""
    try:
        # Get recent cards to find the target card
        all_cards = load_json_file(CARDS_FILE, [])
        
        # Find the most recent card (last generated)
        if not all_cards:
            return "‚ùå No cards found to add images to. Please generate some cards first."
        
        # Get the last card (most recent)
        target_card = all_cards[-1]
        card_id = target_card["id"]
        
        print(f"üé® Generating image for card: {card_id}")
        print(f"Card content: {target_card.get('front', '')} / {target_card.get('back', '')}")
        
        # Generate image using the LLM
        image_prompt = f"Create a simple, child-friendly illustration for this flashcard content: '{target_card.get('front', '')}' - '{target_card.get('back', '')}'. The image should be colorful, simple, and appropriate for a child with autism."
        
        # Use the conversation handler to generate image description
        from agent.conversation_handler import ConversationHandler
        conversation_handler = ConversationHandler()
        
        # Generate image description first
        image_description = conversation_handler._generate_image_description(
            card_content=target_card,
            user_request=message.content,
            child_profile=child_profile
        )
        
        # Generate actual image using DALL-E
        image_result = conversation_handler.generate_actual_image(
            image_description=image_description,
            user_request=message.content
        )
        
        # Update the card with image data
        target_card["image_description"] = image_description
        target_card["image_prompt"] = image_prompt
        
        if image_result["success"]:
            # Don't automatically add to card - show in chat first
            if image_result.get("is_placeholder", False):
                return f"üñºÔ∏è **Generated Image Description:**\n\n{image_description}\n\n‚ö†Ô∏è **Note:** This is a placeholder image. To generate actual images, integrate with an image generation service like DALL-E 3, Midjourney, or Stable Diffusion.\n\nüí° **Instructions:** {image_result.get('instructions', '')}\n\n**To add this image to a card, please specify:**\n- Which card (by ID or 'last card')\n- Front or back\n- Before or after the text"
            else:
                # Show the image in chat with options
                return f"üñºÔ∏è **Generated Image:**\n\n![Generated Image]({image_result['image_data']})\n\n**Image Description:**\n{image_description}\n\n**To add this image to a card, please specify:**\n- Which card (by ID or 'last card')\n- Front or back\n- Before or after the text\n\n**Example commands:**\n- 'Add this image to the last card, front, before text'\n- 'Insert image to card #123, back, after text'"
        else:
            # Fallback to description only
            target_card["image_generated"] = False
            target_card["image_error"] = image_result["error"]
            
            # Save updated card
            for i, card in enumerate(all_cards):
                if card["id"] == card_id:
                    all_cards[i] = target_card
                    break
            
            save_json_file(CARDS_FILE, all_cards)
            
            return f"üñºÔ∏è Generated image description for card '{target_card.get('front', 'Card')}':\n\n{image_description}\n\n‚ùå **Image generation failed:** {image_result['error']}\n\nüí° The description above can be used by an artist or image generation service."
        
    except Exception as e:
        print(f"Image generation error: {e}")
        return f"I encountered an error generating an image: {str(e)}. Please try again."

async def _handle_image_insertion(message: ChatMessage, context_tags: List[Dict[str, Any]], 
                                child_profile: Optional[Dict[str, Any]]) -> str:
    """Handle image insertion requests."""
    try:
        # Parse the insertion command to extract:
        # - Card reference (last card, card ID, etc.)
        # - Position (front/back)
        # - Location (before/after text)
        
        message_lower = message.content.lower()
        
        # Extract card reference
        card_ref = None
        if "last card" in message_lower:
            # Get the most recent card
            all_cards = load_json_file(CARDS_FILE)
            if all_cards:
                card_ref = all_cards[-1]
            else:
                return "‚ùå No cards found. Please create a card first."
        elif "card #" in message_lower or "card " in message_lower:
            # Extract card ID from message
            import re
            card_id_match = re.search(r'card\s*#?(\w+)', message_lower)
            if card_id_match:
                card_id = card_id_match.group(1)
                print(f"üîç Looking for card ending with: {card_id}")
                all_cards = load_json_file(CARDS_FILE)
                print(f"üîç Total cards: {len(all_cards)}")
                card_ref = next((card for card in all_cards if card["id"].endswith(card_id)), None)
                if not card_ref:
                    print(f"‚ùå Card #{card_id} not found in {len(all_cards)} cards")
                    return f"‚ùå Card #{card_id} not found."
                else:
                    print(f"‚úÖ Found card: {card_ref['id']} (type: {card_ref.get('card_type', 'unknown')})")
        else:
            return "‚ùå Please specify which card to add the image to (e.g., 'last card', 'card #123')."
        
        # Extract position (front/back)
        position = "front"  # default
        if "back" in message_lower:
            position = "back"
        elif "front" in message_lower:
            position = "front"
        
        # Extract location (before/after text)
        location = "after"  # default
        if "before" in message_lower:
            location = "before"
        elif "after" in message_lower:
            location = "after"
        
        # Get the last generated image from chat history
        # For now, we'll need to store the last generated image somewhere
        # This is a simplified approach - in a real system, you'd store this in session state
        
        # Check if there's a recent image in the chat history
        chat_history = load_json_file(CHAT_HISTORY_FILE)
        last_image = None
        
        print(f"üîç Searching for image in last {len(chat_history[-10:])} messages...")
        
        # Look for the most recent image generation in chat history
        for msg in reversed(chat_history[-10:]):  # Check last 10 messages
            if msg.get("role") == "assistant":
                content = msg.get("content", "")
                print(f"  Checking message: {content[:100]}...")
                
                if "Generated Image:" in content:
                    print(f"  ‚úÖ Found 'Generated Image:' in message")
                    # Extract image data from the message
                    import re
                    img_match = re.search(r'!\[Generated Image\]\(([^)]+)\)', content)
                    if img_match:
                        last_image = img_match.group(1)
                        print(f"  ‚úÖ Extracted image: {last_image[:50]}...")
                        break
                    else:
                        print(f"  ‚ùå No image pattern found in message")
        
        if not last_image:
            return "‚ùå No recent image found. Please generate an image first using commands like 'generate an image of [something]'."
        
        # Insert the image into the card
        # For CUMA - Interactive Cloze cards, use text_field instead of front
        card_type = card_ref.get("card_type", "")
        is_interactive_cloze = card_type == "interactive_cloze"
        
        if position == "front":
            target_field = "text_field" if is_interactive_cloze else "front"
            current_content = card_ref.get(target_field, "") or card_ref.get("front", "")
            
            if location == "before":
                card_ref[target_field] = f"<img src=\"{last_image}\" alt=\"Generated image\" style=\"max-width: 100%; height: auto; margin-bottom: 10px;\">\n{current_content}"
            else:  # after
                card_ref[target_field] = f"{current_content}\n<img src=\"{last_image}\" alt=\"Generated image\" style=\"max-width: 100%; height: auto; margin-top: 10px;\">"
        else:  # back
            target_field = "extra_field" if is_interactive_cloze else "back"
            current_content = card_ref.get(target_field, "") or card_ref.get("back", "")
            
            if location == "before":
                card_ref[target_field] = f"<img src=\"{last_image}\" alt=\"Generated image\" style=\"max-width: 100%; height: auto; margin-bottom: 10px;\">\n{current_content}"
            else:  # after
                card_ref[target_field] = f"{current_content}\n<img src=\"{last_image}\" alt=\"Generated image\" style=\"max-width: 100%; height: auto; margin-top: 10px;\">"
        
        print(f"‚úÖ Updated {target_field} for card {card_ref['id'][-6:]}")
        
        # Save the updated card
        all_cards = load_json_file(CARDS_FILE)
        card_updated = False
        for i, card in enumerate(all_cards):
            if card["id"] == card_ref["id"]:
                all_cards[i] = card_ref
                card_updated = True
                print(f"‚úÖ Card updated in memory at index {i}")
                break
        
        if not card_updated:
            print(f"‚ùå Warning: Card {card_ref['id']} not found in cards list when trying to save!")
        else:
            save_json_file(CARDS_FILE, all_cards)
            print(f"‚úÖ Card saved to file")
        
        return f"‚úÖ Image successfully added to card #{card_ref['id'][-6:]}!\n\n**Position:** {position}\n**Location:** {location} text\n\nYou can view the updated card in the Card Curation tab."
        
    except Exception as e:
        print(f"Image insertion error: {e}")
        return f"I encountered an error inserting the image: {str(e)}. Please try again."

# Anki profile endpoints
@app.get("/anki-profiles", response_model=List[AnkiProfile])
async def get_anki_profiles():
    profiles = load_json_file(ANKI_PROFILES_FILE, [])
    return profiles

@app.post("/anki-profiles", response_model=AnkiProfile)
async def create_anki_profile(profile: AnkiProfile):
    profiles = load_json_file(ANKI_PROFILES_FILE, [])
    profiles.append(profile.dict())
    save_json_file(ANKI_PROFILES_FILE, profiles)
    return profile

# Prompt Template endpoints
@app.get("/templates", response_model=List[PromptTemplate])
async def get_templates():
    templates = load_json_file(PROMPT_TEMPLATES_FILE, [])
    return templates

@app.post("/templates", response_model=PromptTemplate)
async def create_template(template: PromptTemplate):
    templates = load_json_file(PROMPT_TEMPLATES_FILE, [])
    templates.append(template.dict())
    save_json_file(PROMPT_TEMPLATES_FILE, templates)
    return template

@app.put("/templates/{template_id}", response_model=PromptTemplate)
async def update_template(template_id: str, updated_template: PromptTemplate):
    templates = load_json_file(PROMPT_TEMPLATES_FILE, [])
    template_found = False
    
    for i, template in enumerate(templates):
        if template.get("id") == template_id:
            templates[i] = updated_template.dict()
            template_found = True
            break
    
    if not template_found:
        raise HTTPException(status_code=404, detail="Template not found")
    
    save_json_file(PROMPT_TEMPLATES_FILE, templates)
    return updated_template

@app.delete("/templates/{template_id}")
async def delete_template(template_id: str):
    templates = load_json_file(PROMPT_TEMPLATES_FILE, [])
    initial_count = len(templates)
    templates = [t for t in templates if t.get("id") != template_id]
    
    if len(templates) == initial_count:
        raise HTTPException(status_code=404, detail="Template not found")
    
    save_json_file(PROMPT_TEMPLATES_FILE, templates)
    return {"message": "Template deleted successfully"}

# AnkiConnect test endpoint
@app.get("/anki/test")
async def test_anki_connection():
    """Test connection to AnkiConnect."""
    try:
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        from anki_integration.anki_connect import AnkiConnect
        
        anki = AnkiConnect()
        
        if anki.ping():
            decks = anki.get_deck_names()
            return {
                "status": "connected",
                "message": "AnkiConnect is running",
                "decks": decks
            }
        else:
            return {
                "status": "disconnected",
                "message": "Cannot connect to AnkiConnect"
            }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

# Get available Anki decks
@app.get("/anki/decks")
async def get_anki_decks():
    """Get list of all available Anki decks."""
    try:
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        from anki_integration.anki_connect import AnkiConnect
        
        anki = AnkiConnect()
        
        if not anki.ping():
            raise HTTPException(
                status_code=503,
                detail="Cannot connect to Anki. Make sure Anki is running with AnkiConnect add-on."
            )
        
        decks = anki.get_deck_names()
        return {"decks": decks}
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Get available Anki note types
@app.get("/anki/note-types")
async def get_anki_note_types():
    """Get list of all available Anki note types."""
    try:
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        from anki_integration.anki_connect import AnkiConnect
        
        anki = AnkiConnect()
        
        if not anki.ping():
            raise HTTPException(
                status_code=503,
                detail="Cannot connect to Anki. Make sure Anki is running with AnkiConnect add-on."
            )
        
        # Get model names (note types)
        note_types = anki._invoke("modelNames")
        return {"note_types": note_types}
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Anki sync endpoint
@app.post("/anki/sync")
async def sync_to_anki(request: Dict[str, Any]):
    """
    Sync cards to Anki via AnkiConnect.
    
    Request body:
        {
            "deck_name": "My Deck",
            "card_ids": ["id1", "id2", ...]
        }
    """
    try:
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        from anki_integration.anki_connect import AnkiConnect
        
        deck_name = request.get("deck_name")
        card_ids = request.get("card_ids", [])
        
        if not deck_name:
            raise HTTPException(status_code=400, detail="deck_name is required")
        
        if not card_ids:
            raise HTTPException(status_code=400, detail="card_ids is required")
        
        # Get cards from database
        all_cards = load_json_file(CARDS_FILE, [])
        cards_to_sync = [card for card in all_cards if card["id"] in card_ids]
        
        if not cards_to_sync:
            raise HTTPException(status_code=404, detail="No cards found to sync")
        
        # Initialize AnkiConnect client
        anki = AnkiConnect()
        
        # Check connection
        if not anki.ping():
            raise HTTPException(
                status_code=503, 
                detail="Cannot connect to Anki. Make sure Anki is running with AnkiConnect add-on installed."
            )
        
        # Prepare cards (ensure remarks computed and tags normalized)
        for card in cards_to_sync:
            remarks = build_cuma_remarks(card, [])
            card["field__Remarks"] = remarks or ""
            card.pop("field__Remarks_annotations", None)
        
        # Sync cards
        print(f"üîÑ Syncing {len(cards_to_sync)} cards to deck '{deck_name}'...")
        for card in cards_to_sync:
            print(f"  - Card {card['id']}: {card.get('card_type')} (status: {card.get('status')})")
        
        results = anki.sync_cards(deck_name, cards_to_sync)
        
        print(f"‚úÖ Sync results: {results['total']} total, {len(results['success'])} success, {len(results['failed'])} failed")
        
        if results['failed']:
            for failure in results['failed']:
                print(f"  ‚ùå Failed: {failure['card_id']} - {failure['error']}")
        
        # Update card status to synced
        for card in all_cards:
            if card["id"] in [s["card_id"] for s in results["success"]]:
                card["status"] = "synced"
        
        save_json_file(CARDS_FILE, all_cards)
        
        return {
            "message": f"Synced {len(results['success'])} cards successfully",
            "results": results
        }
    
    except ImportError as e:
        raise HTTPException(status_code=500, detail=f"AnkiConnect module not found: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/kg/recommendations")
async def get_recommendations(request: RecommendationRequest):
    """
    Get vocabulary recommendations based on mastered words and knowledge graph.
    
    Returns top 50 words to learn next based on the Learning Frontier algorithm.
    """
    try:
        # Validate input
        if not request.mastered_words:
            return {
                "recommendations": [],
                "message": "No mastered words provided"
            }
        
        print(f"\nüìö Getting recommendations for {len(request.mastered_words)} mastered words")
        
        # Get recommendations
        # Dynamically determine target level based on mastery rates
        # Find the "learning frontier" - first level where mastery < 80%
        target_level = 3  # Default fallback
        
        print(f"üéØ Determining optimal target level...")
        
        if request.mastered_words:
            # Get mastery rate per level
            mastery_by_level = defaultdict(lambda: {'total': 0, 'mastered': 0})
            
            try:
                sparql = """
                PREFIX srs-kg: <http://srs4autism.com/schema/>
                PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                
                SELECT ?word ?word_text ?hsk WHERE {
                    ?word a srs-kg:Word ;
                          srs-kg:text ?word_text ;
                          srs-kg:hskLevel ?hsk .
                }
                """
                
                print(f"   üîç Querying knowledge graph...")
                csv_result = query_sparql(sparql, "text/csv")
                reader = csv.reader(io.StringIO(csv_result))
                next(reader)  # Skip header
                
                mastered_set = set(request.mastered_words)
                
                for row in reader:
                    if len(row) >= 3:
                        word_text = row[1]  # word_text is second column
                        try:
                            hsk = int(row[2]) if len(row) > 2 and row[2] else None
                        except ValueError:
                            hsk = None
                        
                        if hsk:
                            mastery_by_level[hsk]['total'] += 1
                            if word_text in mastered_set:
                                mastery_by_level[hsk]['mastered'] += 1
                
                print(f"   üìä Mastery rates by level:")
                for level in sorted(mastery_by_level.keys()):
                    if mastery_by_level[level]['total'] > 0:
                        rate = mastery_by_level[level]['mastered'] / mastery_by_level[level]['total']
                        print(f"      HSK {level}: {mastery_by_level[level]['mastered']}/{mastery_by_level[level]['total']} ({rate*100:.1f}%)")
                
                # Find learning frontier (first level < 80% mastery)
                for level in sorted(mastery_by_level.keys()):
                    if mastery_by_level[level]['total'] > 0:
                        rate = mastery_by_level[level]['mastered'] / mastery_by_level[level]['total']
                        if rate < 0.8:  # Less than 80% mastered
                            target_level = level
                            print(f"   üéØ Learning frontier: HSK {target_level} ({rate*100:.1f}% mastered)")
                            break
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Warning: Could not determine optimal target level: {e}")
                import traceback
                traceback.print_exc()
                target_level = 1  # Conservative default
        
        print(f"   üìå Using target_level = {target_level}")
        
        # Get concreteness weight from request (default 0.5 = balanced)
        concreteness_weight = request.concreteness_weight if hasattr(request, 'concreteness_weight') else 0.5
        concreteness_weight = max(0.0, min(1.0, concreteness_weight))  # Clamp to 0-1
        print(f"   ‚öñÔ∏è  Concreteness weight: {concreteness_weight:.2f} (HSK weight: {1.0 - concreteness_weight:.2f})")
        
        # Get mental_age from profile if available
        mental_age = None
        if request.profile_id:
            profiles = load_json_file(PROFILES_FILE, [])
            profile = next((p for p in profiles if p.get('id') == request.profile_id or p.get('name') == request.profile_id), None)
            if profile and profile.get('mental_age'):
                try:
                    mental_age = float(profile['mental_age'])
                    print(f"   üß† Using mental_age={mental_age:.1f} from profile for AoA filtering")
                except (ValueError, TypeError):
                    pass
        
        recommendations = find_learning_frontier(
            mastered_words=request.mastered_words,
            target_level=target_level,
            top_n=50,  # Changed from 20 to 50
            concreteness_weight=concreteness_weight,
            mental_age=mental_age
        )
        
        print(f"   ‚úÖ Found {len(recommendations)} recommendations")
        
        return {
            "recommendations": recommendations,
            "message": f"Found {len(recommendations)} recommendations"
        }
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting recommendations: {str(e)}")


@app.post("/kg/english-recommendations")
async def get_english_recommendations(request: EnglishRecommendationRequest):
    """
    Get English vocabulary recommendations based on mastered words and knowledge graph.
    
    Returns top 50 English words to learn next based on the Learning Frontier algorithm.
    Uses CEFR levels and concreteness scoring.
    """
    try:
        # Validate input
        if not request.mastered_words:
            return {
                "recommendations": [],
                "message": "No mastered words provided"
            }
        
        print(f"\nüìö Getting English recommendations for {len(request.mastered_words)} mastered words")
        
        # Import curious mario recommender
        import sys
        from pathlib import Path
        import requests
        sys.path.insert(0, str(PROJECT_ROOT))
        from scripts.knowledge_graph.curious_mario_recommender import (
            CuriousMarioRecommender, RecommenderConfig, KnowledgeGraphService, KnowledgeNode
        )
        
        # Build mastery vector from profile's mastered words
        # We need to map word texts to KG node IDs
        mastered_set = set(w.lower().strip() for w in request.mastered_words)
        
        # Create config for English vocabulary
        # Slider: 0.0 = Max Frequency (Utility), 1.0 = Max Concreteness (Ease)
        slider_value = max(0.0, min(1.0, request.concreteness_weight or 0.5))
        print(f"   ‚öñÔ∏è  Slider position: {slider_value:.2f} (0.0=Frequency/Utility, 1.0=Concreteness/Ease)")
        print(f"   üìã CEFR acts as hard filter: only showing current level and +1")
        
        config = RecommenderConfig(
            fuseki_endpoint="http://localhost:3030/srs4autism/query",
            node_types=("srs-kg:Word",),
            concreteness_weight=slider_value,  # This is now the slider position, not a weight
            top_n=50,
            auto_detect_language=True,
            mental_age=request.mental_age  # Pass mental age for AoA filtering
        )
        print(f"   ‚öñÔ∏è  Config slider: {config.concreteness_weight:.2f}")
        print(f"   üß† Semantic similarity weight: {config.semantic_similarity_weight:.2f}")
        english_similarity_map = get_english_semantic_similarity()
        
        # Fetch nodes to build mastery vector
        # Override fetch_nodes to filter for English words only (those with CEFR levels)
        kg_service = KnowledgeGraphService(config)
        
        # Custom query to get only English words (those with CEFR levels)
        node_types = " ".join(config.node_types)
        query = f"""
        PREFIX srs-kg: <http://srs4autism.com/schema/>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

        SELECT ?node ?label ?hsk ?cefr ?concreteness ?frequency ?freqRank ?aoa ?prereq WHERE {{
            VALUES ?type {{ {node_types} }}
            ?node a ?type ;
                  rdfs:label ?label .
            # Filter for English words: must have CEFR level (English words) OR English language tag
            OPTIONAL {{ ?node srs-kg:cefrLevel ?cefr }}
            OPTIONAL {{ ?node srs-kg:hskLevel ?hsk }}
            OPTIONAL {{ ?node srs-kg:concreteness ?concreteness }}
            OPTIONAL {{ ?node srs-kg:frequency ?frequency }}
            OPTIONAL {{ ?node srs-kg:frequencyRank ?freqRank }}
            OPTIONAL {{ ?node srs-kg:ageOfAcquisition ?aoa }}
            OPTIONAL {{ ?node srs-kg:requiresPrerequisite ?prereq }}
            # Only include words with CEFR level (English words)
            FILTER(BOUND(?cefr))
        }}
        """
        
        response = requests.post(
            config.fuseki_endpoint,
            data={"query": query},
            headers={"Accept": "application/sparql-results+json"},
            timeout=30,
        )
        response.raise_for_status()
        data = response.json()
        
        # Parse nodes from SPARQL results
        nodes = {}
        for row in data.get("results", {}).get("bindings", []):
            node_iri = row["node"]["value"]
            label = row["label"]["value"]
            # Extract node_id from IRI
            node_id = node_iri.split("/")[-1] if "/" in node_iri else node_iri
            
            from scripts.knowledge_graph.curious_mario_recommender import KnowledgeNode
            node = KnowledgeNode(
                node_id=node_id,
                iri=node_iri,
                label=label,
                hsk_level=None,
                cefr_level=None,
                concreteness=None,
                frequency=None,
                frequency_rank=None,
                age_of_acquisition=None,
                prerequisites=[],
            )
            
            if "hsk" in row and row["hsk"]["value"]:
                try:
                    node.hsk_level = int(float(row["hsk"]["value"]))
                except ValueError:
                    pass
            
            if "cefr" in row and row["cefr"]["value"]:
                node.cefr_level = str(row["cefr"]["value"]).upper()
            
            if "concreteness" in row and row["concreteness"]["value"]:
                try:
                    node.concreteness = float(row["concreteness"]["value"])
                except (ValueError, TypeError):
                    pass

            if "frequency" in row and row["frequency"]["value"]:
                try:
                    node.frequency = float(row["frequency"]["value"])
                except (ValueError, TypeError):
                    pass

            if "freqRank" in row and row["freqRank"]["value"]:
                try:
                    node.frequency_rank = int(float(row["freqRank"]["value"]))
                except (ValueError, TypeError):
                    pass
            
            if "aoa" in row and row["aoa"]["value"]:
                try:
                    node.age_of_acquisition = float(row["aoa"]["value"])
                except (ValueError, TypeError):
                    pass
            
            if "prereq" in row and row["prereq"]["value"]:
                prereq_id = row["prereq"]["value"].split("/")[-1] if "/" in row["prereq"]["value"] else row["prereq"]["value"]
                if prereq_id not in node.prerequisites:
                    node.prerequisites.append(prereq_id)
            
            nodes[node_id] = node
        
        print(f"   üìä Fetched {len(nodes)} English words from knowledge graph")
        
        if len(nodes) == 0:
            return {
                "recommendations": [],
                "message": "No English words found in knowledge graph. Please ensure the English vocabulary has been populated.",
                "learning_frontier": None
            }
        
        # Build mastery vector: mark words as mastered if they're in the profile
        mastery_vector = {}
        mastered_count = 0
        matched_words = []
        unmatched_words = []
        
        for node_id, node in nodes.items():
            # Check if word label matches any mastered word (case-insensitive)
            word_lower = node.label.lower().strip()
            if word_lower in mastered_set:
                mastery_vector[node_id] = 1.0  # Fully mastered
                mastered_count += 1
                matched_words.append(node.label)
            else:
                mastery_vector[node_id] = 0.0  # Not mastered
        
        # Show some examples of matched/unmatched words for debugging
        if mastered_count > 0:
            print(f"   ‚úÖ Matched {mastered_count} mastered words (examples: {matched_words[:5]})")
        else:
            # Check if any words from mastered_set exist in nodes (for debugging)
            sample_mastered = list(mastered_set)[:5]
            sample_node_labels = [node.label.lower() for node in list(nodes.values())[:10]]
            print(f"   ‚ö†Ô∏è  No matches found. Sample mastered words: {sample_mastered}")
            print(f"   ‚ö†Ô∏è  Sample node labels: {sample_node_labels}")
            print(f"   ‚ö†Ô∏è  Total mastered words provided: {len(mastered_set)}")
        
        print(f"   üìä Built mastery vector: {mastered_count} mastered out of {len(mastery_vector)} words")
        
        # Create recommender and find learning frontier
        recommender = CuriousMarioRecommender(config)
        # Reset debug counter for this request
        recommender._debug_logged_count = 0
        language = recommender._detect_language(nodes)
        print(f"   üåê Detected language: {language}")
        learning_frontier = recommender._find_learning_frontier(nodes, mastery_vector, language)
        
        if learning_frontier:
            print(f"   üéØ Learning frontier: CEFR {learning_frontier}")
        
        # Generate recommendations
        mastered_node_ids = {
            node_id for node_id, mastery in mastery_vector.items()
            if mastery >= config.mastery_threshold
        }
        semantic_weight = config.semantic_similarity_weight
        recommendations = []
        for node_id, node in nodes.items():
            mastery = mastery_vector.get(node_id, 0.0)
            
            # Skip if already mastered
            if mastery >= config.mastery_threshold:
                continue
            
            # Skip if no CEFR level (not in CEFR-J vocabulary)
            if not node.cefr_level:
                continue
            
            # Calculate score
            prereq_mastery = 1.0  # No prerequisites for now
            score = recommender._score_candidate(node, mastery, prereq_mastery, language, learning_frontier)
            semantic_boost = 0.0
            if english_similarity_map:
                similar_entries = english_similarity_map.get(node_id, [])
                for neighbour in similar_entries:
                    neighbour_id = neighbour.get("neighbor_id")
                    if not neighbour_id:
                        continue
                    if mastery_vector.get(neighbour_id, 0.0) >= config.mastery_threshold:
                        sim_value = float(neighbour.get("similarity", 0.0))
                        semantic_boost = max(semantic_boost, sim_value)
                if semantic_boost > 0:
                    score += semantic_boost * semantic_weight
            
            # Calculate component scores for debugging (0-1.0 scale, matching scoring formula)
            conc_score_val = 0.5
            aoa_score_val = 0.5
            freq_score_val = 0.5
            ease_score_val = 0.5
            
            if node.concreteness:
                conc_score_val = (node.concreteness - 1.0) / 4.0  # 1.0‚Üí0.0, 5.0‚Üí1.0
            
            # AoA score (0-1.0 scale, lower AoA = higher score = easier)
            if node.age_of_acquisition is not None:
                aoa_score_val = max(0.0, 1.0 - (node.age_of_acquisition / 15.0))
            
            # Ease score = 0.7 * concreteness + 0.3 * AoA
            ease_score_val = (0.7 * conc_score_val) + (0.3 * aoa_score_val)
            
            # Frequency score using Zipf scale (logarithmic normalization)
            max_rank = 20000.0
            if node.frequency_rank:
                rank = float(min(max_rank, max(1, node.frequency_rank)))
                freq_score_val = max(0.0, 1.0 - (math.log(rank) / math.log(max_rank)))
            elif node.frequency:
                freq_score_val = min(
                    1.0,
                    (math.log10(node.frequency + 1.0) / math.log10(50000.0)),
                ) if node.frequency > 0 else 0.5
            
            recommendations.append({
                "word": node.label,
                "cefr_level": node.cefr_level,
                "concreteness": node.concreteness,
                "age_of_acquisition": node.age_of_acquisition,
                "frequency": node.frequency,
                "frequency_rank": node.frequency_rank,
                "score": score,
                "mastery": mastery,
                "concreteness_score": conc_score_val,  # 0-1.0 scale for debugging
                "aoa_score": aoa_score_val,  # 0-1.0 scale for debugging
                "ease_score": ease_score_val,  # 0-1.0 scale for debugging (0.7*conc + 0.3*aoa)
                "frequency_score": freq_score_val,  # 0-1.0 scale for debugging
                "semantic_similarity_boost": round(semantic_boost, 4),
            })
        
        # Sort by score
        recommendations.sort(key=lambda x: x["score"], reverse=True)
        recommendations = recommendations[:config.top_n]
        
        print(f"   ‚úÖ Found {len(recommendations)} recommendations")
        # Log top 5 for debugging
        print(f"   üìä Top 5 recommendations:")
        for i, rec in enumerate(recommendations[:5], 1):
            print(
                f"      {i}. {rec['word']}: score={rec['score']:.2f}, "
                f"CEFR={rec['cefr_level']}, conc={rec.get('concreteness', 'N/A')}, "
                f"freq_rank={rec.get('frequency_rank')}"
            )
        
        return {
            "recommendations": recommendations,
            "message": f"Found {len(recommendations)} recommendations",
            "learning_frontier": learning_frontier,
            "weights_used": {
                "slider_position": config.concreteness_weight,  # 0.0=Frequency, 1.0=Concreteness
                "cefr_filter": "active"  # CEFR acts as hard filter (current level and +1 only)
            }
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error getting English recommendations: {str(e)}")


@app.post("/kg/ppr-recommendations")
async def get_ppr_recommendations(request: PPRRecommendationRequest):
    """
    Get English vocabulary recommendations using Personalized PageRank (PPR) algorithm.
    
    Uses semantic similarity graph, mastered words, and probability-based scoring.
    Returns top N words to learn next based on PPR scores combined with concreteness,
    frequency, and age of acquisition.
    """
    try:
        print(f"\nüìö Getting PPR recommendations for profile '{request.profile_id}'")
        
        # Import PPR service
        import sys
        sys.path.insert(0, str(PROJECT_ROOT / "backend"))
        from services.ppr_recommender_service import get_ppr_service
        
        # Load mastered words from database if not provided
        mastered_words = request.mastered_words
        if not mastered_words:
            db = next(get_db())
            try:
                mastered_words = ProfileService.get_mastered_words(db, request.profile_id, 'en')
                print(f"   üìò Loaded {len(mastered_words)} mastered words from database")
            finally:
                db.close()
        
        if not mastered_words:
            return {
                "recommendations": [],
                "message": "No mastered words found. Add some words to get recommendations."
            }
        
        # Build configuration from request
        config = {}
        if request.alpha is not None:
            config["alpha"] = request.alpha
        if request.beta_ppr is not None:
            config["beta_ppr"] = request.beta_ppr
        if request.beta_concreteness is not None:
            config["beta_concreteness"] = request.beta_concreteness
        if request.beta_frequency is not None:
            config["beta_frequency"] = request.beta_frequency
        if request.beta_aoa_penalty is not None:
            config["beta_aoa_penalty"] = request.beta_aoa_penalty
        if request.beta_intercept is not None:
            config["beta_intercept"] = request.beta_intercept
        if request.mental_age is not None:
            config["mental_age"] = request.mental_age
        if request.aoa_buffer is not None:
            config["aoa_buffer"] = request.aoa_buffer
        if request.exclude_multiword is not None:
            config["exclude_multiword"] = request.exclude_multiword
        if request.top_n is not None:
            config["top_n"] = request.top_n
        
        # Get PPR service (lazy-loaded singleton)
        similarity_file = PROJECT_ROOT / "data" / "content_db" / "english_word_similarity.json"
        kg_file = PROJECT_ROOT / "knowledge_graph" / "world_model_english.ttl"
        
        service = get_ppr_service(
            similarity_file=similarity_file,
            kg_file=kg_file,
            config=config
        )
        
        # Get recommendations
        recommendations = service.get_recommendations(
            mastered_words=mastered_words,
            profile_id=request.profile_id,
            exclude_words=request.exclude_words,
            **config
        )
        
        print(f"   ‚úÖ Found {len(recommendations)} recommendations")
        
        return {
            "recommendations": recommendations,
            "message": f"Found {len(recommendations)} recommendations",
            "config_used": config
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error getting PPR recommendations: {str(e)}")


@app.post("/kg/chinese-ppr-recommendations")
async def get_chinese_ppr_recommendations(request: ChinesePPRRecommendationRequest):
    """
    Get Chinese vocabulary recommendations using Personalized PageRank (PPR) algorithm.
    
    Uses semantic similarity graph, mastered words, and probability-based scoring.
    Returns top N words to learn next based on PPR scores combined with concreteness,
    frequency, and age of acquisition.
    """
    try:
        print(f"\nüìö Getting Chinese PPR recommendations for profile '{request.profile_id}'")
        
        # Import Chinese PPR service
        import sys
        sys.path.insert(0, str(PROJECT_ROOT / "backend"))
        from services.chinese_ppr_recommender_service import get_chinese_ppr_service
        
        # Load mastered words from database if not provided
        mastered_words = request.mastered_words
        if not mastered_words:
            db = next(get_db())
            try:
                mastered_words = ProfileService.get_mastered_words(db, request.profile_id, 'zh')
                print(f"   üìò Loaded {len(mastered_words)} mastered Chinese words from database")
            finally:
                db.close()
        
        if not mastered_words:
            return {
                "recommendations": [],
                "message": "No mastered words found. Add some words to get recommendations."
            }
        
        # Build configuration from request
        config = {}
        if request.alpha is not None:
            config["alpha"] = request.alpha
        if request.beta_ppr is not None:
            config["beta_ppr"] = request.beta_ppr
        if request.beta_concreteness is not None:
            config["beta_concreteness"] = request.beta_concreteness
        if request.beta_frequency is not None:
            config["beta_frequency"] = request.beta_frequency
        if request.beta_aoa_penalty is not None:
            config["beta_aoa_penalty"] = request.beta_aoa_penalty
        if request.beta_intercept is not None:
            config["beta_intercept"] = request.beta_intercept
        if request.mental_age is not None:
            config["mental_age"] = request.mental_age
        if request.aoa_buffer is not None:
            config["aoa_buffer"] = request.aoa_buffer
        if request.exclude_multiword is not None:
            config["exclude_multiword"] = request.exclude_multiword
        if request.top_n is not None:
            config["top_n"] = request.top_n
        
        # Get Chinese PPR service (lazy-loaded singleton)
        similarity_file = PROJECT_ROOT / "data" / "content_db" / "chinese_word_similarity.json"
        # Use merged KG which now includes SUBTLEX-CH frequency data
        kg_file = PROJECT_ROOT / "knowledge_graph" / "world_model_merged.ttl"
        
        service = get_chinese_ppr_service(
            similarity_file=similarity_file,
            kg_file=kg_file,
            config=config
        )
        
        # Get recommendations
        recommendations = service.get_recommendations(
            mastered_words=mastered_words,
            profile_id=request.profile_id,
            exclude_words=request.exclude_words,
            **config
        )
        
        print(f"   ‚úÖ Found {len(recommendations)} Chinese recommendations")
        
        return {
            "recommendations": recommendations,
            "message": f"Found {len(recommendations)} recommendations",
            "config_used": config
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error getting Chinese PPR recommendations: {str(e)}")


@app.post("/recommendations/integrated")
async def get_integrated_recommendations(
    request: IntegratedRecommendationRequest,
    db: Session = Depends(get_db)
):
    """
    Get integrated recommendations using three-stage funnel:
    1. Candidate Generation: PPR + ZPD Filter
    2. Campaign Manager: Inventory Logic (allocates slots based on profile ratios)
    3. Synergy Matcher: (skipped for now)
    
    Returns recommendations allocated according to profile's daily capacity and target ratios.
    """
    try:
        print(f"\nüéØ Getting integrated recommendations for profile '{request.profile_id}'")
        
        # Get profile
        profile = ProfileService.get_by_id(db, request.profile_id)
        if not profile:
            raise HTTPException(status_code=404, detail=f"Profile '{request.profile_id}' not found")
        
        # Import integrated recommender service
        import sys
        sys.path.insert(0, str(PROJECT_ROOT / "backend"))
        from services.integrated_recommender_service import IntegratedRecommenderService
        
        # Initialize integrated recommender
        recommender = IntegratedRecommenderService(profile, db)
        
        # Build PPR config overrides
        ppr_config = {}
        if request.alpha is not None:
            ppr_config["alpha"] = request.alpha
        if request.beta_ppr is not None:
            ppr_config["beta_ppr"] = request.beta_ppr
        if request.beta_concreteness is not None:
            ppr_config["beta_concreteness"] = request.beta_concreteness
        if request.beta_frequency is not None:
            ppr_config["beta_frequency"] = request.beta_frequency
        if request.beta_aoa_penalty is not None:
            ppr_config["beta_aoa_penalty"] = request.beta_aoa_penalty
        if request.beta_intercept is not None:
            ppr_config["beta_intercept"] = request.beta_intercept
        if request.mental_age is not None:
            ppr_config["mental_age"] = request.mental_age
        if request.aoa_buffer is not None:
            ppr_config["aoa_buffer"] = request.aoa_buffer
        if request.exclude_multiword is not None:
            ppr_config["exclude_multiword"] = request.exclude_multiword
        if request.top_n is not None:
            ppr_config["top_n"] = request.top_n
        if request.max_hsk_level is not None and request.language == "zh":
            ppr_config["max_hsk_level"] = request.max_hsk_level
        
        # Get recommendations
        recommendations = recommender.get_recommendations(
            language=request.language,
            mastered_words=request.mastered_words,
            **ppr_config
        )
        
        # Convert to dict format for JSON response
        recommendations_dict = [
            {
                "node_id": rec.node_id,
                "label": rec.label,
                "content_type": rec.content_type,
                "language": rec.language,
                "score": rec.score,
                "ppr_score": rec.ppr_score,
                "zpd_score": rec.zpd_score,
                "mastery": rec.mastery,
                "hsk_level": rec.hsk_level,
                "cefr_level": rec.cefr_level,
                "prerequisites": rec.prerequisites or [],
                "missing_prereqs": rec.missing_prereqs or []
            }
            for rec in recommendations
        ]
        
        print(f"   ‚úÖ Found {len(recommendations)} integrated recommendations")
        print(f"   üìä Allocation: {recommender.vocab_slots} vocab, {recommender.grammar_slots} grammar")
        print(f"   üìä Ratios: {recommender.vocab_ratio:.1%} vocab, {recommender.grammar_ratio:.1%} grammar")
        
        return {
            "recommendations": recommendations_dict,
            "allocation": {
                "daily_capacity": recommender.daily_capacity,
                "vocab_slots": recommender.vocab_slots,
                "grammar_slots": recommender.grammar_slots,
                "vocab_ratio": recommender.vocab_ratio,
                "grammar_ratio": recommender.grammar_ratio
            },
            "message": f"Found {len(recommendations)} recommendations"
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error getting integrated recommendations: {str(e)}")


@app.post("/agentic/plan")
async def agentic_plan(request: AgenticPlanRequest):
    """
    Entry point for the new Agentic Learning Agent.
    
    This is a "learning agent" that:
    1. Synthesizes cognitive state (mastery, KG, profile)
    2. Determines WHAT to learn (via recommender)
    3. Returns a learning plan (not just cards)
    
    The agent solves "what to learn" by integrating with the recommender system.
    Topic is optional - if not provided, the agent will determine the best learning
    content based on the child's cognitive state.
    """
    planner = get_agentic_planner()
    plan = planner.plan_learning_step(
        user_id=request.user_id,
        topic=request.topic,
        learner_level=request.learner_level,
        topic_complexity=request.topic_complexity,
    )
    response = {
        "learner_level": plan.learner_level,
        "topic": plan.topic,
        "topic_complexity": plan.topic_complexity,
        "scaffold_type": plan.scaffold_type,
        "rationale": plan.rationale,
        "cognitive_prior": {
            "mastery_summary": plan.cognitive_prior.get("mastery_summary", {}),
            "total_nodes": len(plan.cognitive_prior.get("mastery_vector", {})),
        },
        "recommendation_plan": plan.recommendation_plan,
        "cards": plan.cards_payload.get("cards") if plan.cards_payload else None,
    }
    return response

# Get HSK vocabulary for mastered words management
@app.get("/vocabulary/hsk")
async def get_hsk_vocabulary(hsk_level: Optional[int] = None):
    """
    Get HSK vocabulary words, optionally filtered by HSK level.
    Returns words with their pinyin, HSK level, and simplified/traditional forms.
    """
    try:
        import csv
        from pathlib import Path
        
        # Path to HSK vocabulary CSV
        vocab_file = PROJECT_ROOT / "data" / "content_db" / "hsk_vocabulary.csv"
        
        if not vocab_file.exists():
            raise HTTPException(status_code=404, detail="HSK vocabulary file not found")
        
        words = []
        with open(vocab_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    word_hsk = int(row.get('hsk_level', 0))
                    if hsk_level is None or word_hsk == hsk_level:
                        words.append({
                            'word': row.get('word', '').strip(),
                            'pinyin': row.get('pinyin', '').strip(),
                            'hsk_level': word_hsk,
                            'traditional': row.get('traditional', '').strip() if 'traditional' in row else None
                        })
                except (ValueError, KeyError):
                    continue
        
        return {
            "words": words,
            "total": len(words),
            "filtered_by": hsk_level
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading vocabulary: {str(e)}")

# Get CEFR English vocabulary for mastered words management
@app.get("/vocabulary/cefr")
async def get_cefr_vocabulary(cefr_level: Optional[str] = None):
    """
    Get English vocabulary words with CEFR levels, optionally filtered by CEFR level.
    Returns words with their definitions, CEFR level, and part of speech.
    """
    try:
        import csv
        from pathlib import Path
        
        # Path to CEFR-J vocabulary CSV files
        cefrj_dir = PROJECT_ROOT.parent / "olp-en-cefrj"
        cefrj_file = cefrj_dir / "cefrj-vocabulary-profile-1.5.csv"
        octanove_file = cefrj_dir / "octanove-vocabulary-profile-c1c2-1.0.csv"
        
        words = []
        word_set = set()  # Track unique words to avoid duplicates
        
        # Load CEFR-J main vocabulary
        if cefrj_file.exists():
            with open(cefrj_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    headword = row.get('headword', '').strip()
                    cefr = row.get('CEFR', '').strip()
                    pos = row.get('pos', '').strip()
                    
                    if headword and cefr:
                        # Handle variants like "catalog/catalogue"
                        variants = [v.strip() for v in headword.split('/')]
                        for variant in variants:
                            if variant.lower() not in word_set:
                                if cefr_level is None or cefr == cefr_level:
                                    words.append({
                                        'word': variant,
                                        'cefr_level': cefr,
                                        'pos': pos,
                                        'definition': ''  # CEFR-J doesn't include definitions
                                    })
                                    word_set.add(variant.lower())
        
        # Load Octanove C1/C2 supplement
        if octanove_file.exists():
            with open(octanove_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    headword = row.get('headword', '').strip()
                    cefr = row.get('CEFR', '').strip()
                    pos = row.get('pos', '').strip()
                    
                    if headword and cefr:
                        variants = [v.strip() for v in headword.split('/')]
                        for variant in variants:
                            if variant.lower() not in word_set:
                                if cefr_level is None or cefr == cefr_level:
                                    words.append({
                                        'word': variant,
                                        'cefr_level': cefr,
                                        'pos': pos,
                                        'definition': ''
                                    })
                                    word_set.add(variant.lower())
        
        # Sort by CEFR level, then by word
        cefr_order = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}
        words.sort(key=lambda w: (cefr_order.get(w['cefr_level'], 99), w['word'].lower()))
        
        return {
            "words": words,
            "total": len(words),
            "filtered_by": cefr_level
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading CEFR vocabulary: {str(e)}")

# Cache for word->image mapping (loaded from TTL file)
_word_image_cache = None
_word_image_cache_time = None

def get_word_image_map():
    """Load word->image mapping from TTL file (cached, much faster than full graph)."""
    global _word_image_cache, _word_image_cache_time
    from pathlib import Path
    import threading
    
    kg_file = PROJECT_ROOT / "knowledge_graph" / "world_model_cwn.ttl"
    
    # Reload if file is newer than cache (or cache doesn't exist)
    file_mtime = kg_file.stat().st_mtime if kg_file.exists() else 0
    if _word_image_cache is None or _word_image_cache_time is None or file_mtime > _word_image_cache_time:
        # If cache is being built, return empty dict (will be populated on next request)
        if _word_image_cache is None:
            # Start loading in background thread to avoid blocking
            def load_cache():
                global _word_image_cache, _word_image_cache_time
                from rdflib import Graph, Namespace
                from rdflib.namespace import RDF
                
                print("Loading word->image mapping from TTL file (this may take 20-30 seconds)...")
                graph = Graph()
                SRS_KG = Namespace("http://srs4autism.com/schema/")
                graph.bind("srs-kg", SRS_KG)
                
                if kg_file.exists():
                    try:
                        # Only parse the file (this is still slow but necessary)
                        graph.parse(str(kg_file), format="turtle")
                        
                        # Build a simple word->image mapping (much faster lookups)
                        word_image_map = {}
                        sparql = """
                        PREFIX srs-kg: <http://srs4autism.com/schema/>
                        SELECT DISTINCT ?word_text ?image_path
                        WHERE {
                            ?word_uri a srs-kg:Word .
                            ?word_uri srs-kg:text ?word_text .
                            ?word_uri srs-kg:means ?concept_uri .
                            ?concept_uri srs-kg:hasVisualization ?image_uri .
                            ?image_uri srs-kg:imageFilePath ?image_path .
                        }
                        """
                        
                        results = graph.query(sparql)
                        for row in results:
                            word = str(row.word_text).strip()
                            img = str(row.image_path).strip()
                            if word and img:
                                if not img.startswith('/'):
                                    img = '/' + img
                                word_image_map[word] = img
                        
                        _word_image_cache = word_image_map
                        _word_image_cache_time = file_mtime
                        print(f"‚úÖ Loaded {len(word_image_map)} word->image mappings from {len(graph)} triples")
                    except Exception as e:
                        print(f"Error loading KG file: {e}")
                        import traceback
                        traceback.print_exc()
                        if _word_image_cache is None:
                            _word_image_cache = {}  # Empty dict as fallback
            
            # Start loading in background
            thread = threading.Thread(target=load_cache, daemon=True)
            thread.start()
            # Return empty dict while loading
            return {}
    
    return _word_image_cache or {}

# Get images for words (batch query)
@app.post("/vocabulary/images")
async def get_word_images(request: Dict[str, Any]):
    """
    Get image paths for a list of Chinese words.
    Returns a mapping of word -> image path (if available).
    
    Query the TTL file directly since images aren't in Fuseki yet.
    
    Request body: { "words": ["ËãπÊûú", "ÊúãÂèã", ...] }
    Response: { "ËãπÊûú": "/media/visual_images/apple.png", "ÊúãÂèã": null, ... }
    """
    try:
        words = request.get("words", [])
        if not words or not isinstance(words, list):
            return {}
        
        # Get word->image mapping from cache (pre-built from TTL file)
        word_image_map = get_word_image_map()
        if not word_image_map:
            return {word: None for word in words}
        
        # Simple dictionary lookup (O(1) per word, very fast!)
        result = {}
        found_count = 0
        for word in words:
            image_path = word_image_map.get(word.strip())
            result[word] = image_path
            if image_path:
                found_count += 1
        
        print(f"Found {found_count} images for {len(words)} words (from cache)")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error getting word images: {e}")
        import traceback
        traceback.print_exc()
        # Return empty dict on error (graceful degradation)
        return {word: None for word in request.get("words", [])}

# Get grammar points for mastered grammar management
@app.get("/vocabulary/grammar")
async def get_grammar_points(cefr_level: Optional[str] = None, language: Optional[str] = "zh"):
    """
    Get grammar points from the knowledge graph, optionally filtered by CEFR level (A1, A2, etc.) and language.
    Returns grammar points with their structure, explanation, and CEFR level.
    
    Args:
        cefr_level: Optional CEFR level filter (A1, A2, B1, B2, etc.)
        language: Language filter - "zh" for Chinese, "en" for English (default: "zh")
    """
    try:
        # Filter by language: 
        # English grammar (CEFR-J): URI starts with "grammar-en-"
        # Chinese grammar: URI does NOT start with "grammar-en-"
        if language == "en":
            language_filter = 'FILTER(CONTAINS(STR(?gp_uri), "grammar-en-"))'
        else:
            language_filter = 'FILTER(!CONTAINS(STR(?gp_uri), "grammar-en-") && BOUND(?label_zh))'
        
        # Query the knowledge graph for grammar points
        # Use OPTIONAL for properties that might be missing, and handle language-tagged literals
        # Get both English and Chinese labels, and first example sentence (only one per grammar point)
        # First get all grammar points with their properties
        # Build CEFR level filter if specified
        cefr_filter = f'FILTER(?cefr = "{cefr_level}")' if cefr_level else ''
        
        sparql = f"""
        PREFIX srs-kg: <http://srs4autism.com/schema/>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        
        SELECT DISTINCT ?gp_uri ?label_en ?label_zh ?structure ?explanation ?cefr WHERE {{
            ?gp_uri a srs-kg:GrammarPoint .
            OPTIONAL {{ ?gp_uri rdfs:label ?label_en . FILTER(LANG(?label_en) = "en" || LANG(?label_en) = "") }}
            OPTIONAL {{ ?gp_uri rdfs:label ?label_zh . FILTER(LANG(?label_zh) = "zh") }}
            OPTIONAL {{ ?gp_uri srs-kg:structure ?structure }}
            OPTIONAL {{ ?gp_uri srs-kg:explanation ?explanation }}
            OPTIONAL {{ ?gp_uri srs-kg:cefrLevel ?cefr }}
            {language_filter}
            {cefr_filter}
        }}
        ORDER BY ?cefr ?label_en
        """
        
        # Clean up query string (remove extra whitespace)
        sparql = ' '.join(sparql.split())
        
        # Query Fuseki
        results = query_sparql(sparql, output_format="application/sparql-results+json")
        
        if not results or 'results' not in results:
            return {
                "grammar_points": [],
                "total": 0,
                "filtered_by": cefr_level
            }
        
        grammar_points = []
        seen_uris = set()  # Track seen grammar points to avoid duplicates
        
        for binding in results.get('results', {}).get('bindings', []):
            try:
                gp_uri = binding.get('gp_uri', {}).get('value', '')
                
                # Skip if we've already seen this grammar point (avoid duplicates)
                if gp_uri in seen_uris:
                    continue
                seen_uris.add(gp_uri)
                
                label_en = binding.get('label_en', {}).get('value', '')
                label_zh = binding.get('label_zh', {}).get('value', '')
                structure = binding.get('structure', {}).get('value', '')
                explanation = binding.get('explanation', {}).get('value', '')
                cefr = binding.get('cefr', {}).get('value', '')
                
                # For English grammar, use English label; for Chinese, prefer Chinese label
                if language == "en":
                    label = label_en or label_zh  # English grammar: prefer English label
                else:
                    label = label_zh or label_en  # Chinese grammar: prefer Chinese label
                
                if label:
                    # Get first example sentence for this grammar point
                    # For English grammar, look for English examples; for Chinese, look for Chinese examples
                    example_text = ''
                    example_lang = 'en' if language == 'en' else 'zh'
                    try:
                        # First try hasExample (grammar point -> sentence)
                        example_sparql = f"""
                        PREFIX srs-kg: <http://srs4autism.com/schema/>
                        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                        SELECT ?example_text WHERE {{
                            <{gp_uri}> srs-kg:hasExample ?example .
                            ?example rdfs:label ?example_text . FILTER(LANG(?example_text) = "{example_lang}")
                        }}
                        LIMIT 1
                        """
                        example_results = query_sparql(example_sparql, output_format="application/sparql-results+json")
                        if example_results and 'results' in example_results:
                            bindings = example_results.get('results', {}).get('bindings', [])
                            if bindings:
                                example_text = bindings[0].get('example_text', {}).get('value', '')
                        
                        # If no result, try reverse relationship (sentence -> grammar point)
                        if not example_text:
                            example_sparql_reverse = f"""
                            PREFIX srs-kg: <http://srs4autism.com/schema/>
                            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                            SELECT ?example_text WHERE {{
                                ?example srs-kg:demonstratesGrammar <{gp_uri}> .
                                ?example rdfs:label ?example_text . FILTER(LANG(?example_text) = "{example_lang}")
                            }}
                            LIMIT 1
                            """
                            example_results_reverse = query_sparql(example_sparql_reverse, output_format="application/sparql-results+json")
                            if example_results_reverse and 'results' in example_results_reverse:
                                bindings = example_results_reverse.get('results', {}).get('bindings', [])
                                if bindings:
                                    example_text = bindings[0].get('example_text', {}).get('value', '')
                    except:
                        pass  # If example query fails, just continue without example
                    
                    # For English grammar, don't include Chinese translation; for Chinese, include it
                    grammar_point_data = {
                        'gp_uri': gp_uri,  # Include URI for updating
                        'grammar_point': label,
                        'structure': structure,
                        'explanation': explanation,
                        'cefr_level': cefr,
                    }
                    
                    if language == "en":
                        # English grammar: use example_text for English examples
                        grammar_point_data['example'] = example_text
                        # Only include Chinese translation if it exists (for bilingual display)
                        if label_zh:
                            grammar_point_data['grammar_point_zh'] = label_zh
                    else:
                        # Chinese grammar: use example_chinese for Chinese examples
                        grammar_point_data['example_chinese'] = example_text
                        grammar_point_data['grammar_point_zh'] = label_zh  # Chinese translation
                    
                    grammar_points.append(grammar_point_data)
            except Exception as e:
                continue
        
        return {
            "grammar_points": grammar_points,
            "total": len(grammar_points),
            "filtered_by": cefr_level
        }
    except Exception as e:
        # If Fuseki is not available, return empty list
        print(f"Warning: Could not query grammar points from KG: {e}")
        return {
            "grammar_points": [],
            "total": 0,
            "filtered_by": cefr_level,
            "error": "Knowledge graph server may not be available"
        }

# Save grammar point corrections/edits
GRAMMAR_CORRECTIONS_FILE = str(PROJECT_ROOT / "data" / "content_db" / "grammar_corrections.json")

@app.put("/vocabulary/grammar/{gp_uri:path}")
async def update_grammar_point(gp_uri: str, grammar_data: dict):
    """
    Update a grammar point with user corrections.
    Stores corrections in a JSON file that can be applied when repopulating the knowledge graph.
    Uses :path to allow URLs with slashes in the URI.
    """
    try:
        # Load existing corrections
        corrections = load_json_file(GRAMMAR_CORRECTIONS_FILE, {})
        
        # URL decode the URI
        from urllib.parse import unquote
        decoded_uri = unquote(gp_uri)
        
        # Store the correction
        corrections[decoded_uri] = {
            'grammar_point': grammar_data.get('grammar_point', ''),
            'grammar_point_zh': grammar_data.get('grammar_point_zh', ''),
            'structure': grammar_data.get('structure', ''),
            'explanation': grammar_data.get('explanation', ''),
            'cefr_level': grammar_data.get('cefr_level', ''),
            'example_chinese': grammar_data.get('example_chinese', ''),
            'updated_at': datetime.now().isoformat()
        }
        
        # Save corrections
        save_json_file(GRAMMAR_CORRECTIONS_FILE, corrections)
        
        return {
            "message": "Grammar point updated successfully",
            "gp_uri": decoded_uri,
            "corrections": corrections[decoded_uri]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error saving grammar correction: {str(e)}")

@app.get("/vocabulary/grammar/corrections")
async def get_grammar_corrections():
    """Get all grammar point corrections."""
    try:
        corrections = load_json_file(GRAMMAR_CORRECTIONS_FILE, {})
        return {"corrections": corrections, "total": len(corrections)}
    except Exception as e:
        return {"corrections": {}, "total": 0, "error": str(e)}

@app.post("/kg/grammar-recommendations")
async def get_grammar_recommendations(request: GrammarRecommendationRequest):
    """
    Get grammar point recommendations based on mastered grammar and knowledge graph.
    
    Returns top 50 grammar points to learn next based on the Learning Frontier algorithm.
    Uses CEFR levels instead of HSK levels.
    """
    try:
        # Validate input - allow empty list for first-time users
        mastered_count = len(request.mastered_grammar) if request.mastered_grammar else 0
        
        language = request.language or "zh"
        print(f"\nüìñ Getting {language.upper()} grammar recommendations for {mastered_count} mastered grammar points")
        
        # Get all grammar points from knowledge graph
        # mastered_grammar should contain URIs, not names (to avoid comma issues)
        mastered_set = set(request.mastered_grammar) if request.mastered_grammar else set()
        
        # Get mastery rate per CEFR level
        mastery_by_level = defaultdict(lambda: {'total': 0, 'mastered': 0})
        
        try:
            # Filter by language: 
            # English grammar (CEFR-J): URI starts with "grammar-en-" (from populate_english_grammar.py)
            # Chinese grammar: URI does NOT start with "grammar-en-"
            if language == "en":
                # English grammar: URI must contain "grammar-en-"
                label_filter = 'FILTER(CONTAINS(STR(?gp_uri), "grammar-en-"))'
            else:
                # Chinese grammar: URI must NOT contain "grammar-en-" and must have Chinese label
                label_filter = 'FILTER(!CONTAINS(STR(?gp_uri), "grammar-en-") && BOUND(?label_zh))'
            
            sparql = f"""
            PREFIX srs-kg: <http://srs4autism.com/schema/>
            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
            
            SELECT ?gp_uri ?label_en ?label_zh ?cefr WHERE {{
                ?gp_uri a srs-kg:GrammarPoint .
                OPTIONAL {{ ?gp_uri rdfs:label ?label_en . FILTER(LANG(?label_en) = "en" || LANG(?label_en) = "") }}
                OPTIONAL {{ ?gp_uri rdfs:label ?label_zh . FILTER(LANG(?label_zh) = "zh") }}
                OPTIONAL {{ ?gp_uri srs-kg:cefrLevel ?cefr }}
                {label_filter}
            }}
            """
            
            print(f"   üîç Querying knowledge graph for grammar points...")
            results = query_sparql(sparql, output_format="application/sparql-results+json")
            
            if results and 'results' in results:
                for binding in results.get('results', {}).get('bindings', []):
                    gp_uri = binding.get('gp_uri', {}).get('value', '')
                    cefr = binding.get('cefr', {}).get('value', '') or 'not specified'
                    
                    if gp_uri:
                        mastery_by_level[cefr]['total'] += 1
                        if gp_uri in mastered_set:
                            mastery_by_level[cefr]['mastered'] += 1
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Warning: Could not query grammar points: {e}")
        
        # Find learning frontier (first CEFR level < 80% mastery)
        target_cefr = 'A1'  # Default
        cefr_order = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2', 'not specified']
        
        print(f"   üìä Mastery rates by CEFR level:")
        for level in cefr_order:
            if mastery_by_level[level]['total'] > 0:
                rate = mastery_by_level[level]['mastered'] / mastery_by_level[level]['total']
                print(f"      CEFR {level}: {mastery_by_level[level]['mastered']}/{mastery_by_level[level]['total']} ({rate*100:.1f}%)")
                
                if rate < 0.8:  # Less than 80% mastered
                    target_cefr = level
                    print(f"   üéØ Learning frontier: CEFR {target_cefr} ({rate*100:.1f}% mastered)")
                    break
        
        # Get all grammar points and score them
        print(f"   üìå Using target CEFR level = {target_cefr}")
        
        # Query all grammar points with their details (filtered by language)
        # English grammar: URI starts with "grammar-en-" (from CEFR-J)
        # Chinese grammar: URI does NOT start with "grammar-en-"
        if language == "en":
            label_filter_all = 'FILTER(CONTAINS(STR(?gp_uri), "grammar-en-"))'
        else:
            label_filter_all = 'FILTER(!CONTAINS(STR(?gp_uri), "grammar-en-") && BOUND(?label_zh))'
        
        sparql_all = f"""
        PREFIX srs-kg: <http://srs4autism.com/schema/>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        
        SELECT DISTINCT ?gp_uri ?label_en ?label_zh ?structure ?explanation ?cefr WHERE {{
            ?gp_uri a srs-kg:GrammarPoint .
            OPTIONAL {{ ?gp_uri rdfs:label ?label_en . FILTER(LANG(?label_en) = "en" || LANG(?label_en) = "") }}
            OPTIONAL {{ ?gp_uri rdfs:label ?label_zh . FILTER(LANG(?label_zh) = "zh") }}
            OPTIONAL {{ ?gp_uri srs-kg:structure ?structure }}
            OPTIONAL {{ ?gp_uri srs-kg:explanation ?explanation }}
            OPTIONAL {{ ?gp_uri srs-kg:cefrLevel ?cefr }}
            {label_filter_all}
        }}
        """
        
        results = query_sparql(sparql_all, output_format="application/sparql-results+json")
        
        if not results or 'results' not in results:
            return {
                "recommendations": [],
                "message": "Could not query grammar points from knowledge graph"
            }
        
        scored_grammar = []
        seen_uris = set()
        
        for binding in results.get('results', {}).get('bindings', []):
            try:
                gp_uri = binding.get('gp_uri', {}).get('value', '')
                if gp_uri in seen_uris:
                    continue
                seen_uris.add(gp_uri)
                
                label_en = binding.get('label_en', {}).get('value', '')
                label_zh = binding.get('label_zh', {}).get('value', '')
                structure = binding.get('structure', {}).get('value', '')
                explanation = binding.get('explanation', {}).get('value', '')
                cefr = binding.get('cefr', {}).get('value', '') or 'not specified'
                
                # Use language-appropriate label
                if language == "en":
                    grammar_point = label_en or label_zh  # Prefer English, fallback to Chinese
                else:
                    grammar_point = label_zh or label_en  # Prefer Chinese, fallback to English
                
                if not grammar_point or gp_uri in mastered_set:
                    continue  # Skip if no label or already mastered (check by URI)
                
                # Score grammar points
                score = 0
                
                # Prioritize grammar points in target CEFR level
                if cefr == target_cefr:
                    score += 100
                
                # Bonus for having structure and explanation
                if structure:
                    score += 10
                if explanation:
                    score += 10
                
                # Get example sentence
                # Try both hasExample (forward) and demonstratesGrammar (reverse) relationships
                example_chinese = ''
                try:
                    # First try hasExample (grammar point -> sentence)
                    example_sparql = f"""
                    PREFIX srs-kg: <http://srs4autism.com/schema/>
                    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                    SELECT ?example_chinese WHERE {{
                        <{gp_uri}> srs-kg:hasExample ?example .
                        ?example rdfs:label ?example_chinese . FILTER(LANG(?example_chinese) = "zh")
                    }}
                    LIMIT 1
                    """
                    example_results = query_sparql(example_sparql, output_format="application/sparql-results+json")
                    if example_results and 'results' in example_results:
                        bindings = example_results.get('results', {}).get('bindings', [])
                        if bindings:
                            example_chinese = bindings[0].get('example_chinese', {}).get('value', '')
                    
                    # If no result, try reverse relationship (sentence -> grammar point)
                    if not example_chinese:
                        example_sparql_reverse = f"""
                        PREFIX srs-kg: <http://srs4autism.com/schema/>
                        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                        SELECT ?example_chinese WHERE {{
                            ?example srs-kg:demonstratesGrammar <{gp_uri}> .
                            ?example rdfs:label ?example_chinese . FILTER(LANG(?example_chinese) = "zh")
                        }}
                        LIMIT 1
                        """
                        example_results_reverse = query_sparql(example_sparql_reverse, output_format="application/sparql-results+json")
                        if example_results_reverse and 'results' in example_results_reverse:
                            bindings = example_results_reverse.get('results', {}).get('bindings', [])
                            if bindings:
                                example_chinese = bindings[0].get('example_chinese', {}).get('value', '')
                except:
                    pass
                
                scored_grammar.append({
                    'gp_uri': gp_uri,  # Include URI as unique identifier
                    'grammar_point': grammar_point,
                    'grammar_point_zh': label_zh,
                    'structure': structure,
                    'explanation': explanation,
                    'cefr_level': cefr,
                    'example_chinese': example_chinese,
                    'score': score
                })
            except Exception as e:
                continue
        
        # Sort by score (descending) and take top 50
        scored_grammar.sort(key=lambda x: x['score'], reverse=True)
        recommendations = scored_grammar[:50]
        
        print(f"   ‚úÖ Found {len(recommendations)} grammar recommendations")
        
        return {
            "recommendations": recommendations,
            "message": f"Found {len(recommendations)} recommendations",
            "target_cefr": target_cefr
        }
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting grammar recommendations: {str(e)}")

# Character Recognition endpoints
@app.get("/character-recognition/notes")
async def get_character_recognition_notes(profile_id: str):
    """
    Get character recognition notes from database.
    Filters out mastered characters.
    Maintains the original order from the apkg file.
    """
    try:
        from database.models import CharacterRecognitionNote
        
        # Get mastered characters (separate list with language='character')
        db = next(get_db())
        try:
            # Get mastered characters from separate list
            mastered_chars = set(ProfileService.get_mastered_words(db, profile_id, 'character'))
            
            # Get all notes from database, ordered by display_order
            db_notes = db.query(CharacterRecognitionNote).order_by(CharacterRecognitionNote.display_order).all()
            
            # Convert to API format and filter mastered
            notes = []
            for db_note in db_notes:
                # Filter out mastered characters
                if db_note.character in mastered_chars:
                    continue
                
                # Parse fields JSON
                note_fields = json.loads(db_note.fields) if db_note.fields else {}
                
                note_data = {
                    'note_id': db_note.note_id,
                    'character': db_note.character,
                    'fields': note_fields
                }
                notes.append(note_data)
        finally:
            db.close()
        
        print(f"üìö Loaded {len(notes)} character recognition notes from database (filtered {len(mastered_chars)} mastered)")
        
        return {
            "notes": notes,
            "total": len(notes),
            "mastered_filtered": len(mastered_chars)
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error loading character recognition notes: {str(e)}")

@app.post("/character-recognition/sync")
async def sync_character_recognition_notes(request: Dict[str, Any]):
    """
    Sync character recognition notes to Anki.
    Creates 7 cards per note:
    1. Concept (picture) => Character
    2. Character => Concept (picture) (reverse)
    3. MCQ (Pick Char) - Concept => Character
    4. MCQ (Pick Pic) - Character => Concept
    5-7. Word1, Word2, Word3 - MCQ cloze completion
    
    Also extracts media files from apkg and copies them to media directory.
    """
    try:
        import sys
        import os
        import shutil
        import re
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        from anki_integration.anki_connect import AnkiConnect
        
        profile_id = request.get("profile_id")
        note_ids = request.get("note_ids", [])
        deck_name = request.get("deck_name", "ËØÜÂ≠ó")
        
        if not profile_id:
            raise HTTPException(status_code=400, detail="profile_id is required")
        
        if not note_ids:
            raise HTTPException(status_code=400, detail="note_ids is required")
        
        apkg_path = PROJECT_ROOT / "data" / "content_db" / "ËØ≠Ë®ÄËØ≠Êñá__ËØÜÂ≠ó__ÂÖ®ÈÉ®.apkg"
        
        # Extract media files from apkg
        media_dir = PROJECT_ROOT / "media" / "character_recognition"
        media_dir.mkdir(parents=True, exist_ok=True)
        
        media_map = {}  # Map original filename to new path
        
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir_path = Path(tmpdir)
            
            # Extract .apkg (it's a zip file)
            with zipfile.ZipFile(apkg_path, 'r') as zip_ref:
                zip_ref.extractall(tmpdir_path)
            
            # Copy media files to media directory
            # Note: In some apkg files, media might be stored differently
            media_source = tmpdir_path / "media"
            if media_source.exists() and media_source.is_dir():
                try:
                    print(f"üìÅ Extracting media files from apkg...")
                    for media_file in media_source.iterdir():
                        if media_file.is_file():
                            # Copy to media directory
                            dest_file = media_dir / media_file.name
                            if not dest_file.exists():  # Don't overwrite existing files
                                shutil.copy2(media_file, dest_file)
                                print(f"  ‚úÖ Copied {media_file.name}")
                            else:
                                print(f"  ‚ÑπÔ∏è  {media_file.name} already exists, skipping")
                            # Map original filename to URL path
                            media_map[media_file.name] = f"/media/character_recognition/{media_file.name}"
                except Exception as e:
                    print(f"‚ö†Ô∏è  Warning: Could not extract media files: {e}")
            else:
                # Try to find media files in the zip directly
                try:
                    with zipfile.ZipFile(apkg_path, 'r') as zip_ref:
                        media_files = [f for f in zip_ref.namelist() if f.startswith('media/') and not f.endswith('/')]
                        if media_files:
                            print(f"üìÅ Extracting {len(media_files)} media files from apkg...")
                            for media_path in media_files:
                                filename = Path(media_path).name
                                if filename:  # Skip directories
                                    dest_file = media_dir / filename
                                    if not dest_file.exists():
                                        with zip_ref.open(media_path) as source_file:
                                            with open(dest_file, 'wb') as dest:
                                                shutil.copyfileobj(source_file, dest)
                                        print(f"  ‚úÖ Extracted {filename}")
                                    # Map original filename to URL path
                                    media_map[filename] = f"/media/character_recognition/{filename}"
                except Exception as e:
                    print(f"‚ö†Ô∏è  Warning: Could not extract media files from zip: {e}")
            
        # Get the notes from the database
        from database.models import CharacterRecognitionNote
        
        db = next(get_db())
        try:
            # Get notes from database
            db_notes = db.query(CharacterRecognitionNote).filter(
                CharacterRecognitionNote.note_id.in_(note_ids)
            ).order_by(CharacterRecognitionNote.display_order).all()
            
            notes_to_sync = []
            for db_note in db_notes:
                note_fields = json.loads(db_note.fields) if db_note.fields else {}
                notes_to_sync.append({
                    'note_id': db_note.note_id,
                    'character': db_note.character,
                    'fields': note_fields
                })
        finally:
            db.close()
        
        if not notes_to_sync:
            raise HTTPException(status_code=404, detail="No notes found to sync")
        
        # Initialize AnkiConnect client
        anki = AnkiConnect()
        
        # Check connection
        if not anki.ping():
            raise HTTPException(
                status_code=503,
                detail="Cannot connect to Anki. Make sure Anki is running with AnkiConnect add-on installed."
            )
        
        # Create deck if it doesn't exist
        try:
            anki.create_deck(deck_name)
        except:
            pass  # Deck might already exist
        
        # Step 1: Collect all image files referenced in the notes and upload them to Anki
        # Following Media file management.md naming convention: cm_[Type]_[ContentID]_[Variant].[ext]
        # For character recognition: cm_char_[sanitized_original_name].[ext]
        anki_media_map = {}  # Maps original filename to Anki media filename
        media_dir = PROJECT_ROOT / "media" / "character_recognition"
        
        def extract_image_filenames_from_html(html_content: str) -> set:
            """Extract all image filenames from HTML content (extract filename from any path)"""
            if not html_content:
                return set()
            
            filenames = set()
            # Pattern to match <img src="..."> or <img src='...'>
            pattern = r'<img[^>]*src=["\']([^"\']+)["\'][^>]*>'
            
            for match in re.finditer(pattern, html_content):
                src_value = match.group(1)
                # Skip URLs
                if src_value.startswith('http'):
                    continue
                
                # Extract just the filename (remove any path, including /media/character_recognition/)
                # Handle both absolute paths (/media/...) and relative paths
                filename = src_value.strip()
                if '/' in filename:
                    # Get the last component (filename)
                    filename = Path(filename).name
                filename = filename.strip()
                
                if filename:  # Only add non-empty filenames
                    filenames.add(filename)
            
            return filenames
        
        # Collect all unique image filenames from all notes
        all_image_filenames = set()
        for note in notes_to_sync:
            fields = note['fields']
            for field_value in fields.values():
                if field_value and isinstance(field_value, str):
                    all_image_filenames.update(extract_image_filenames_from_html(field_value))
        
        # Upload each image file to Anki with namespaced naming
        import base64
        import hashlib
        
        PROJECT_PREFIX = "cm"  # Following the naming convention from Media file management.md
        
        # Track file hashes to avoid uploading duplicates
        uploaded_hashes = {}  # Maps content_hash -> anki_filename (for duplicate detection)
        
        for original_filename in all_image_filenames:
            source_file = media_dir / original_filename
            if not source_file.exists():
                print(f"‚ö†Ô∏è  Warning: Media file not found: {original_filename}")
                continue
            
            try:
                # Read file
                with open(source_file, 'rb') as f:
                    file_data = f.read()
                
                # Calculate content hash to detect duplicates
                content_hash = hashlib.md5(file_data).hexdigest()
                
                # Check if we've already uploaded this exact file
                if content_hash in uploaded_hashes:
                    # Reuse the existing Anki filename
                    anki_media_map[original_filename] = uploaded_hashes[content_hash]
                    print(f"  ‚ÑπÔ∏è  Reusing {original_filename} ‚Üí {uploaded_hashes[content_hash]} (duplicate)")
                    continue
                
                # Generate Anki filename: cm_char_[sanitized_original_name].[ext]
                # Sanitize: remove special chars, keep alphanumeric, underscore, dash, dot
                # Keep the extension
                file_ext = Path(original_filename).suffix
                file_stem = Path(original_filename).stem
                # Sanitize stem: replace non-alphanumeric with underscore (except dash and underscore)
                sanitized_stem = re.sub(r'[^a-zA-Z0-9_-]', '_', file_stem)
                # Limit length to avoid issues
                if len(sanitized_stem) > 50:
                    sanitized_stem = sanitized_stem[:50]
                
                # Always include short hash for uniqueness and to avoid collisions
                # Format: cm_char_[sanitized_name]_[8char_hash].[ext]
                short_hash = content_hash[:8]
                anki_filename = f"{PROJECT_PREFIX}_char_{sanitized_stem}_{short_hash}{file_ext}"
                
                # Encode to base64 for AnkiConnect
                base64_data = base64.b64encode(file_data).decode('utf-8')
                
                # Upload to Anki
                stored_filename = anki.store_media_file(anki_filename, base64_data)
                anki_media_map[original_filename] = stored_filename
                uploaded_hashes[content_hash] = stored_filename
                print(f"  ‚úÖ Uploaded {original_filename} ‚Üí {stored_filename}")
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Failed to upload {original_filename}: {e}")
                # Fallback: use original filename (might cause conflicts, but better than nothing)
                anki_media_map[original_filename] = original_filename
        
        # Step 2: Update HTML to reference Anki media filenames (just filename, no path)
        # Anki does NOT support subdirectories - all files must be in collection.media root
        def update_image_references_to_anki(html_content: str, character: str) -> str:
            """Update image references to use Anki media filenames (no paths, just filename)
            
            Replaces paths like /media/character_recognition/I2.png with just cm_char_I2_a1b2c3d4.png
            """
            if not html_content:
                return html_content
            
            # Pattern to match <img src="..."> or <img src='...'>
            pattern = r'<img[^>]*src=["\']([^"\']+)["\'][^>]*>'
            
            def replace_img(match):
                img_tag = match.group(0)
                src_value = match.group(1)
                
                # Skip URLs
                if src_value.startswith('http'):
                    return img_tag
                
                # Extract just the filename (remove any path, including /media/character_recognition/)
                # Handle both absolute paths (/media/...) and relative paths
                filename = src_value.strip()
                if '/' in filename:
                    # Get the last component (filename) - this handles /media/character_recognition/I2.png
                    filename = Path(filename).name
                filename = filename.strip()
                
                # Look up in anki_media_map (maps original filename like "I2.png" to Anki filename)
                if filename in anki_media_map:
                    anki_filename = anki_media_map[filename]
                    # Replace the entire src value with just the Anki filename (no path, no subdirectory)
                    # This ensures Anki can find the file in collection.media root
                    # Example: /media/character_recognition/I2.png -> cm_char_I2_a1b2c3d4.png
                    new_img_tag = re.sub(
                        r'src=["\']([^"\']+)["\']',
                        f'src="{anki_filename}"',
                        img_tag,
                        count=1
                    )
                    print(f"  üîÑ Replaced image: {src_value} -> {anki_filename}")
                    return new_img_tag
                else:
                    # If not in map, try to extract and warn
                    print(f"  ‚ö†Ô∏è  Warning: Image filename '{filename}' (from '{src_value}') not found in uploaded media map")
                    print(f"     Available keys: {list(anki_media_map.keys())[:5]}...")
                
                return img_tag
            
            return re.sub(pattern, replace_img, html_content)
        
        # Create cards for each note
        cards_created = 0
        errors = []
        
        for note in notes_to_sync:
            try:
                fields = note['fields']
                character = note['character']
                
                # Update all fields to use Anki media filenames
                processed_fields = {}
                for field_name, field_value in fields.items():
                    if field_value and isinstance(field_value, str):
                        processed_fields[field_name] = update_image_references_to_anki(field_value, character)
                    else:
                        processed_fields[field_name] = field_value or ""
                
                # Build _KG_Map following strict schema (Section 4 of Knowledge Tracking Spec)
                # Character recognition creates 7 cards (indices 0-6):
                # Card 0: Concept (picture) => Character (form_to_concept)
                # Card 1: Character => Concept (concept_to_form)
                # Card 2: MCQ Pick Char - Concept => Character (form_to_concept)
                # Card 3: MCQ Pick Pic - Character => Concept (concept_to_form)
                # Cards 4-6: Word examples with character cloze (form_to_concept)
                
                # For now, map all cards to the character knowledge point
                # TODO: Map individual cards to specific skills based on card type
                char_kp = f"char-zh-{character}"  # Knowledge point URI for the character
                
                card_mappings = {
                    "0": [{"kp": char_kp, "skill": "form_to_concept", "weight": 1.0}],  # Picture => Character
                    "1": [{"kp": char_kp, "skill": "concept_to_form", "weight": 1.0}],  # Character => Picture
                    "2": [{"kp": char_kp, "skill": "form_to_concept", "weight": 0.8}],  # MCQ Pick Char (hint available)
                    "3": [{"kp": char_kp, "skill": "concept_to_form", "weight": 0.8}],  # MCQ Pick Pic (hint available)
                    "4": [{"kp": char_kp, "skill": "form_to_concept", "weight": 0.6}],  # Word example 1 (context)
                    "5": [{"kp": char_kp, "skill": "form_to_concept", "weight": 0.6}],  # Word example 2 (context)
                    "6": [{"kp": char_kp, "skill": "form_to_concept", "weight": 0.6}],  # Word example 3 (context)
                }
                kg_map_json = build_kg_map_strict(card_mappings)
                
                # Add _KG_Map and _Remarks to the fields
                processed_fields["_KG_Map"] = kg_map_json
                if "_Remarks" not in processed_fields:
                    processed_fields["_Remarks"] = f"Character Recognition - {character}"
                
                # Create ONE note - the note type template will create 7 cards
                note_type = "CUMA - Chinese Recognition"
                note_id = anki.add_note(deck_name, note_type, processed_fields)
                cards_created += 7  # The note type creates 7 cards
                
            except Exception as e:
                errors.append({
                    "note_id": note.get('note_id', 'unknown'),
                    "character": note.get('character', 'unknown'),
                    "error": str(e)
                })
        
        notes_synced_successfully = len(notes_to_sync) - len(errors)
        print(f"‚úÖ Synced {notes_synced_successfully} notes (creating {cards_created} cards) from {len(notes_to_sync)} notes to deck '{deck_name}'")
        if errors:
            print(f"‚ö†Ô∏è  {len(errors)} errors occurred")
        
        return {
            "message": f"Synced {notes_synced_successfully} notes successfully",
            "cards_created": cards_created,
            "notes_synced": notes_synced_successfully,
            "errors": errors
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error syncing character recognition notes: {str(e)}")

@app.post("/character-recognition/master")
async def mark_character_mastered(request: Dict[str, Any]):
    """
    Mark a character as mastered (adds to separate mastered characters list).
    """
    try:
        profile_id = request.get("profile_id")
        characters = request.get("characters", [])
        
        if not profile_id:
            raise HTTPException(status_code=400, detail="profile_id is required")
        
        if not characters:
            raise HTTPException(status_code=400, detail="characters list is required")
        
        db = next(get_db())
        try:
            from database.models import MasteredWord
            
            added_count = 0
            for char in characters:
                if not char or not char.strip():
                    continue
                
                char_clean = char.strip()
                
                # Check if already mastered
                existing = db.query(MasteredWord).filter_by(
                    profile_id=profile_id,
                    word=char_clean,
                    language='character'
                ).first()
                
                if not existing:
                    mastered_char = MasteredWord(
                        profile_id=profile_id,
                        word=char_clean,
                        language='character'
                    )
                    db.add(mastered_char)
                    added_count += 1
            
            db.commit()
            
            return {
                "message": f"Marked {added_count} character(s) as mastered",
                "added": added_count,
                "total": len(characters)
            }
        finally:
            db.close()
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error marking characters as mastered: {str(e)}")


@app.get("/word-recognition/notes")
async def get_chinese_word_recognition_notes(profile_id: str):
    """
    Get Chinese word recognition notes from database.
    Filters out mastered words.
    Maintains the original order from the apkg file.
    """
    try:
        from database.models import ChineseWordRecognitionNote
        
        # Get mastered words (Chinese)
        db = next(get_db())
        try:
            # Get mastered Chinese words
            mastered_words = set(ProfileService.get_mastered_words(db, profile_id, 'zh'))
            
            # Get all notes from database, ordered by display_order
            db_notes = db.query(ChineseWordRecognitionNote).order_by(ChineseWordRecognitionNote.display_order).all()
            
            # Convert to API format and filter mastered
            notes = []
            for db_note in db_notes:
                # Filter out mastered words
                if db_note.word in mastered_words:
                    continue
                
                # Parse fields JSON
                note_fields = json.loads(db_note.fields) if db_note.fields else {}
                
                note_data = {
                    'note_id': db_note.note_id,
                    'word': db_note.word,
                    'concept': db_note.concept,
                    'fields': note_fields
                }
                notes.append(note_data)
        finally:
            db.close()
        
        print(f"üìö Loaded {len(notes)} Chinese word recognition notes from database (filtered {len(mastered_words)} mastered)")
        
        return {
            "notes": notes,
            "total": len(notes),
            "mastered_filtered": len(mastered_words)
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error loading Chinese word recognition notes: {str(e)}")


@app.post("/word-recognition/add-custom")
async def add_custom_word_recognition(
    word: str = Form(...),
    concept: str = Form(...),
    pinyin: str = Form(""),
    bopomofo: str = Form(""),
    image: UploadFile = File(None)
):
    """
    Add a custom word to the Chinese word recognition database.
    
    Args:
        word: Chinese word
        concept: English concept
        pinyin: Pinyin pronunciation (optional)
        bopomofo: Bopomofo/Zhuyin pronunciation (optional)
        image: Image file (optional)
    """
    try:
        import uuid
        import shutil
        from database.models import ChineseWordRecognitionNote
        from database.db import get_db
        
        # Generate unique note ID
        note_id = f"custom_{uuid.uuid4().hex[:12]}"
        
        # Prepare fields
        fields = {
            'Word': word,
            'Concept': concept,
            'Pinyin': pinyin,
            'Bopomofo': bopomofo,
            'Image': '',
            'Audio': ''
        }
        
        # Handle image upload if provided
        if image and image.filename:
            # Save image to media directory
            media_dir = PROJECT_ROOT / "media" / "chinese_word_recognition"
            media_dir.mkdir(parents=True, exist_ok=True)
            
            # Generate safe filename
            file_ext = Path(image.filename).suffix
            safe_filename = f"custom_{word}_{uuid.uuid4().hex[:8]}{file_ext}"
            image_path = media_dir / safe_filename
            
            # Save file
            with open(image_path, "wb") as buffer:
                shutil.copyfileobj(image.file, buffer)
            
            # Update fields with image reference
            fields['Image'] = f'<img src="/media/chinese_word_recognition/{safe_filename}" alt="{concept}">'
            print(f"   üì∑ Saved custom image: {safe_filename}")
        
        # Get the highest display_order and add 1
        db = next(get_db())
        try:
            max_order = db.query(ChineseWordRecognitionNote).count()
            display_order = max_order + 1
            
            # Create database entry
            new_note = ChineseWordRecognitionNote(
                note_id=note_id,
                word=word,
                concept=concept,
                display_order=display_order,
                fields=json.dumps(fields, ensure_ascii=False)
            )
            
            db.add(new_note)
            db.commit()
            
            print(f"‚úÖ Added custom word: {word} ({concept})")
            
            return {
                "message": "Custom word added successfully",
                "note_id": note_id,
                "word": word,
                "concept": concept
            }
        finally:
            db.close()
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error adding custom word: {str(e)}")


@app.post("/chinese-naming/sync")
async def sync_chinese_naming_notes(request: Dict[str, Any]):
    """
    Sync Chinese naming notes to Anki using "CUMA - Chinese Naming v2" note type.
    
    Based on Layer 0 design:
    - Uses Initial (Â£∞ÊØç), Medial (‰ªãÈü≥), Toned Final (ÈüµÊØç+Â£∞Ë∞É) components
    - Click-based interface (not drag-and-drop)
    - No Chinese characters displayed (reduces cognitive load)
    - Each component has a distractor
    - Components are color-coded
    
    Creates cards for verbal training focusing on:
    - Concept (picture) => Pinyin construction (Initial + Medial + Final)
    - Audio => Pinyin construction
    """
    try:
        import sys
        import os
        import re
        import base64
        import hashlib
        import random
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        from anki_integration.anki_connect import AnkiConnect
        from database.models import ChineseWordRecognitionNote, MasteredWord
        from database.db import get_db
        from scripts.knowledge_graph.pinyin_parser import parse_pinyin, PINYIN_INITIALS, PINYIN_MEDIALS, PINYIN_FINALS
        
        profile_id = request.get("profile_id")
        note_ids = request.get("note_ids", [])
        deck_name = request.get("deck_name", "‰∏≠ÊñáÂëΩÂêç")
        config = request.get("config", "simplified")  # "simplified" or "traditional"
        
        if not profile_id:
            raise HTTPException(status_code=400, detail="profile_id is required")
        
        if not note_ids:
            raise HTTPException(status_code=400, detail="note_ids is required")
        
        # Get the notes from the database
        db = next(get_db())
        try:
            db_notes = db.query(ChineseWordRecognitionNote).filter(
                ChineseWordRecognitionNote.note_id.in_(note_ids)
            ).order_by(ChineseWordRecognitionNote.display_order).all()
            
            notes_to_sync = []
            for db_note in db_notes:
                note_fields = json.loads(db_note.fields) if db_note.fields else {}
                notes_to_sync.append({
                    'note_id': db_note.note_id,
                    'word': db_note.word,
                    'concept': db_note.concept,
                    'fields': note_fields
                })
        finally:
            db.close()
        
        if not notes_to_sync:
            raise HTTPException(status_code=404, detail="No notes found to sync")
        
        # Initialize AnkiConnect client
        anki = AnkiConnect()
        
        # Check connection
        if not anki.ping():
            raise HTTPException(
                status_code=503,
                detail="Cannot connect to Anki. Make sure Anki is running with AnkiConnect add-on installed."
            )
        
        # Create deck if it doesn't exist
        try:
            anki.create_deck(deck_name)
        except:
            pass  # Deck might already exist
        
        # Handle media files (images and audio)
        MEDIA_DIR = PROJECT_ROOT / "media" / "chinese_word_recognition"
        PROJECT_PREFIX = "cm"
        anki_media_map = {}
        uploaded_hashes = {}
        
        # Collect all media filenames (images and audio)
        all_image_filenames = set()
        all_audio_filenames = set()
        
        for note in notes_to_sync:
            fields = note['fields']
            for field_value in fields.values():
                if field_value and isinstance(field_value, str):
                    # Extract image filenames
                    img_pattern = r'<img[^>]*src=["\']([^"\']+)["\'][^>]*>'
                    for match in re.finditer(img_pattern, field_value):
                        src_value = match.group(1)
                        if not src_value.startswith('http'):
                            all_image_filenames.add(Path(src_value).name)
                    
                    # Extract audio filenames from [sound:...] tags
                    audio_pattern = r'\[sound:([^\]]+)\]'
                    for match in re.finditer(audio_pattern, field_value):
                        audio_filename = match.group(1)
                        all_audio_filenames.add(audio_filename)
        
        # Upload images
        for original_filename in all_image_filenames:
            source_file = MEDIA_DIR / original_filename
            if not source_file.exists():
                print(f"‚ö†Ô∏è  Warning: Image file not found: {original_filename}")
                continue
            
            try:
                with open(source_file, 'rb') as f:
                    file_data = f.read()
                
                content_hash = hashlib.md5(file_data).hexdigest()
                
                if content_hash in uploaded_hashes:
                    anki_media_map[original_filename] = uploaded_hashes[content_hash]
                    continue
                
                file_ext = Path(original_filename).suffix
                file_stem = Path(original_filename).stem
                sanitized_stem = re.sub(r'[^a-zA-Z0-9_-]', '_', file_stem)
                if len(sanitized_stem) > 50:
                    sanitized_stem = sanitized_stem[:50]
                
                short_hash = content_hash[:8]
                anki_filename = f"{PROJECT_PREFIX}_word_{sanitized_stem}_{short_hash}{file_ext}"
                
                base64_data = base64.b64encode(file_data).decode('utf-8')
                stored_filename = anki.store_media_file(anki_filename, base64_data)
                anki_media_map[original_filename] = stored_filename
                uploaded_hashes[content_hash] = stored_filename
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Failed to upload {original_filename}: {e}")
                anki_media_map[original_filename] = original_filename
        
        # Upload audio files
        for original_filename in all_audio_filenames:
            source_file = MEDIA_DIR / original_filename
            if not source_file.exists():
                print(f"‚ö†Ô∏è  Warning: Audio file not found: {original_filename}")
                continue
            
            try:
                with open(source_file, 'rb') as f:
                    file_data = f.read()
                
                content_hash = hashlib.md5(file_data).hexdigest()
                
                if content_hash in uploaded_hashes:
                    anki_media_map[original_filename] = uploaded_hashes[content_hash]
                    print(f"  ‚ÑπÔ∏è  Reusing {original_filename} ‚Üí {uploaded_hashes[content_hash]} (duplicate)")
                    continue
                
                file_ext = Path(original_filename).suffix
                file_stem = Path(original_filename).stem
                sanitized_stem = re.sub(r'[^a-zA-Z0-9_-]', '_', file_stem)
                if len(sanitized_stem) > 50:
                    sanitized_stem = sanitized_stem[:50]
                
                short_hash = content_hash[:8]
                anki_filename = f"{PROJECT_PREFIX}_audio_{sanitized_stem}_{short_hash}{file_ext}"
                
                base64_data = base64.b64encode(file_data).decode('utf-8')
                stored_filename = anki.store_media_file(anki_filename, base64_data)
                anki_media_map[original_filename] = stored_filename
                uploaded_hashes[content_hash] = stored_filename
                print(f"  ‚úÖ Uploaded audio: {original_filename} ‚Üí {stored_filename}")
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Failed to upload {original_filename}: {e}")
                anki_media_map[original_filename] = original_filename
        
        # Update media references (images and audio)
        def update_media_references(content: str) -> str:
            if not content:
                return content
            
            # Update image references
            img_pattern = r'<img[^>]*src=["\']([^"\']+)["\'][^>]*>'
            def replace_img(match):
                img_tag = match.group(0)
                src_value = match.group(1)
                if src_value.startswith('http'):
                    return img_tag
                filename = Path(src_value).name
                if filename in anki_media_map:
                    anki_filename = anki_media_map[filename]
                    return re.sub(r'src=["\']([^"\']+)["\']', f'src="{anki_filename}"', img_tag, count=1)
                return img_tag
            
            # Update audio references [sound:...]
            audio_pattern = r'\[sound:([^\]]+)\]'
            def replace_audio(match):
                original_filename = match.group(1)
                if original_filename in anki_media_map:
                    anki_filename = anki_media_map[original_filename]
                    return f"[sound:{anki_filename}]"
                return match.group(0)
            
            content = re.sub(img_pattern, replace_img, content)
            content = re.sub(audio_pattern, replace_audio, content)
            return content
        
        # Generate notes with pinyin decomposition
        cards_created = 0
        errors = []
        
        for note in notes_to_sync:
            try:
                fields = note['fields']
                word = note['word']
                concept = note['concept']
                
                # Get pinyin from fields
                pinyin_raw = fields.get('Pinyin', '').strip()
                if not pinyin_raw:
                    # Try to extract from other fields or skip
                    print(f"‚ö†Ô∏è  Warning: No pinyin found for word '{word}', skipping")
                    continue
                
                # Parse pinyin (handle multiple syllables - keep word as whole unit)
                syllable_pinyins = pinyin_raw.split()
                if not syllable_pinyins:
                    syllable_pinyins = [pinyin_raw]
                
                # Limit to 3 syllables (as per template design)
                if len(syllable_pinyins) > 3:
                    print(f"‚ö†Ô∏è  Warning: Word '{word}' has {len(syllable_pinyins)} syllables, truncating to 3")
                    syllable_pinyins = syllable_pinyins[:3]
                
                # Get image and audio (shared across all cards)
                image_html = fields.get('Image', '') or fields.get('image', '')
                image_html = update_media_references(image_html)
                audio_html = fields.get('Audio', '') or fields.get('audio', '')
                audio_html = update_media_references(audio_html)
                
                # For CUMA - Chinese Naming v2: Create ONE note per word with ALL syllables
                # The note will generate 2 cards (Easy and Harder) that progressively go through all syllables
                
                # Prepare fields for all syllables (up to 3)
                anki_note_fields = {
                    'Concept': concept,
                    'Image': image_html,
                    'Audio': audio_html,
                    'FullPinyin': pinyin_raw,
                    'TotalSyllables': str(len(syllable_pinyins)),
                    'Config': config,
                }
                
                # Generate syllable options and component options for each syllable
                for syllable_idx, syllable_pinyin in enumerate(syllable_pinyins):
                    # Parse pinyin into components
                    pinyin_components = parse_pinyin(syllable_pinyin)
                    
                    # Generate syllable distractors for Easy mode
                    syllable_options = [syllable_pinyin]
                    available_syllables = [
                        'mƒÅ', 'm√°', 'm«é', 'm√†',
                        'shƒ´', 'sh√≠', 'sh«ê', 'sh√¨',
                        'zƒ´', 'z√≠', 'z«ê', 'z√¨', 'zi',
                        'l√≥ng', 'l«íng', 'l√≤ng',
                        'xi√≥ng', 'xi«íng', 'xi√≤ng'
                    ]
                    available_syllables = [s for s in available_syllables if s != syllable_pinyin]
                    if available_syllables:
                        syllable_options.extend(random.sample(available_syllables, min(3, len(available_syllables))))
                    random.shuffle(syllable_options)
                    
                    # Generate component distractors for Harder mode
                    initial_options = [pinyin_components['initial']] if pinyin_components['initial'] else []
                    medial_options = [pinyin_components['medial']] if pinyin_components['medial'] else []
                    final_options = [pinyin_components['toned_final']] if pinyin_components['toned_final'] else []
                    
                    if pinyin_components['initial']:
                        available_initials = [i for i in PINYIN_INITIALS if i != pinyin_components['initial']]
                        if available_initials:
                            initial_options.append(random.choice(available_initials))
                    
                    if pinyin_components['medial']:
                        available_medials = [m for m in PINYIN_MEDIALS if m != pinyin_components['medial']]
                        if available_medials:
                            medial_options.append(random.choice(available_medials))
                    
                    if pinyin_components['toned_final']:
                        available_finals = [f for f in PINYIN_FINALS if f != pinyin_components['final']]
                        if available_finals:
                            distractor_final = random.choice(available_finals)
                            from scripts.knowledge_graph.pinyin_parser import add_tone_to_final
                            distractor_toned = add_tone_to_final(distractor_final, pinyin_components['tone'])
                            final_options.append(distractor_toned)
                    
                    random.shuffle(initial_options)
                    random.shuffle(medial_options)
                    random.shuffle(final_options)
                    
                    # Add fields for this syllable (Syllable1, Syllable2, or Syllable3)
                    syl_num = syllable_idx + 1
                    anki_note_fields[f'Syllable{syl_num}'] = syllable_pinyin
                    anki_note_fields[f'Syllable{syl_num}Option1'] = syllable_options[0] if len(syllable_options) > 0 else ''
                    anki_note_fields[f'Syllable{syl_num}Option2'] = syllable_options[1] if len(syllable_options) > 1 else ''
                    anki_note_fields[f'Syllable{syl_num}Option3'] = syllable_options[2] if len(syllable_options) > 2 else ''
                    anki_note_fields[f'Syllable{syl_num}Option4'] = syllable_options[3] if len(syllable_options) > 3 else ''
                    
                    anki_note_fields[f'Initial{syl_num}'] = pinyin_components['initial'] or ''
                    anki_note_fields[f'Initial{syl_num}Option1'] = initial_options[0] if len(initial_options) > 0 else ''
                    anki_note_fields[f'Initial{syl_num}Option2'] = initial_options[1] if len(initial_options) > 1 else ''
                    
                    anki_note_fields[f'Medial{syl_num}'] = pinyin_components['medial'] or ''
                    anki_note_fields[f'Medial{syl_num}Option1'] = medial_options[0] if len(medial_options) > 0 else ''
                    anki_note_fields[f'Medial{syl_num}Option2'] = medial_options[1] if len(medial_options) > 1 else ''
                    
                    anki_note_fields[f'TonedFinal{syl_num}'] = pinyin_components['toned_final'] or ''
                    anki_note_fields[f'TonedFinal{syl_num}Option1'] = final_options[0] if len(final_options) > 0 else ''
                    anki_note_fields[f'TonedFinal{syl_num}Option2'] = final_options[1] if len(final_options) > 1 else ''
                
                # Fill empty fields for unused syllables (if word has < 3 syllables)
                for syl_num in range(len(syllable_pinyins) + 1, 4):
                    anki_note_fields[f'Syllable{syl_num}'] = ''
                    anki_note_fields[f'Syllable{syl_num}Option1'] = ''
                    anki_note_fields[f'Syllable{syl_num}Option2'] = ''
                    anki_note_fields[f'Syllable{syl_num}Option3'] = ''
                    anki_note_fields[f'Syllable{syl_num}Option4'] = ''
                    anki_note_fields[f'Initial{syl_num}'] = ''
                    anki_note_fields[f'Initial{syl_num}Option1'] = ''
                    anki_note_fields[f'Initial{syl_num}Option2'] = ''
                    anki_note_fields[f'Medial{syl_num}'] = ''
                    anki_note_fields[f'Medial{syl_num}Option1'] = ''
                    anki_note_fields[f'Medial{syl_num}Option2'] = ''
                    anki_note_fields[f'TonedFinal{syl_num}'] = ''
                    anki_note_fields[f'TonedFinal{syl_num}Option1'] = ''
                    anki_note_fields[f'TonedFinal{syl_num}Option2'] = ''
                
                # Build _KG_Map following strict schema (Section 4 of Knowledge Tracking Spec)
                # Chinese Naming v2 creates 2 cards:
                # Card 0 (Easy): Concept (picture) => Pinyin syllable selection (concept_to_sound)
                # Card 1 (Harder): Audio => Pinyin component construction (pinyin_assembly)
                
                word_kp = f"word-zh-{word}"  # Knowledge point URI for the word
                
                card_mappings = {
                    "0": [{"kp": word_kp, "skill": "concept_to_sound", "weight": 1.0}],  # Easy: Picture => Pinyin selection
                    "1": [{"kp": word_kp, "skill": "pinyin_assembly", "weight": 1.0}],   # Harder: Audio => Component construction
                }
                anki_note_fields["_KG_Map"] = build_kg_map_strict(card_mappings)
                anki_note_fields["_Remarks"] = f"Chinese Naming v2 - {word} ({concept})"
                
                # Create ONE note (which will generate 2 cards: Easy and Harder)
                note_type = "CUMA - Chinese Naming v2"
                anki.add_note(deck_name, note_type, anki_note_fields)
                cards_created += 2  # The note type creates 2 cards
                
                # Note: Do NOT auto-mark as mastered
                # Caregiver will manually manage the mastered list using "Ê†áËÆ∞‰∏∫Â∑≤ÊéåÊè°" button
                
            except Exception as e:
                errors.append({
                    "note_id": note.get('note_id', 'unknown'),
                    "word": note.get('word', 'unknown'),
                    "error": str(e)
                })
                import traceback
                traceback.print_exc()
        
        notes_synced_successfully = len(notes_to_sync) - len(errors)
        print(f"‚úÖ Synced {notes_synced_successfully} notes (creating {cards_created} cards) to deck '{deck_name}'")
        if errors:
            print(f"‚ö†Ô∏è  {len(errors)} errors occurred")
        
        return {
            "message": f"Synced {notes_synced_successfully} notes successfully",
            "cards_created": cards_created,
            "notes_synced": notes_synced_successfully,
            "errors": errors
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error syncing Chinese naming notes: {str(e)}")


# ============================================================================
# Pinyin Learning endpoints
# ============================================================================

@app.get("/pinyin/elements")
async def get_pinyin_elements(profile_id: str, db: Session = Depends(get_db)):
    """
    Get Pinyin element notes (initial/final teaching cards) from database.
    """
    try:
        from database.models import PinyinElementNote
        
        # Get all element notes, ordered by display_order
        db_notes = db.query(PinyinElementNote).order_by(PinyinElementNote.display_order).all()
        
        # Convert to API format
        notes = []
        for db_note in db_notes:
            # Parse fields JSON
            note_fields = json.loads(db_note.fields) if db_note.fields else {}
            
            note_data = {
                'note_id': db_note.note_id,
                'element': db_note.element,
                'element_type': db_note.element_type,
                'fields': note_fields
            }
            notes.append(note_data)
        
        return {
            "notes": notes,
            "total": len(notes)
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error loading pinyin element notes: {str(e)}")


@app.get("/pinyin/syllables")
async def get_pinyin_syllables(profile_id: str, db: Session = Depends(get_db)):
    """
    Get Pinyin syllable notes from database.
    """
    try:
        from database.models import PinyinSyllableNote
        
        # Get all syllable notes, ordered by display_order
        db_notes = db.query(PinyinSyllableNote).order_by(PinyinSyllableNote.display_order).all()
        
        # Convert to API format
        notes = []
        for db_note in db_notes:
            # Parse fields JSON
            note_fields = json.loads(db_note.fields) if db_note.fields else {}
            
            note_data = {
                'note_id': db_note.note_id,
                'syllable': db_note.syllable,
                'word': db_note.word,
                'concept': db_note.concept,
                'fields': note_fields
            }
            notes.append(note_data)
        
        print(f"üìö Loaded {len(notes)} pinyin syllable notes from database")
        
        return {
            "notes": notes,
            "total": len(notes)
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error loading pinyin syllable notes: {str(e)}")


@app.post("/pinyin/sync")
async def sync_pinyin_notes(request: Dict[str, Any]):
    """
    Sync pinyin notes (elements and/or syllables) to Anki.
    Both types are synced to the same deck, preserving the order from display_order.
    """
    try:
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        from anki_integration.anki_connect import AnkiConnect
        from database.models import PinyinElementNote, PinyinSyllableNote
        from database.db import get_db
        
        profile_id = request.get("profile_id")
        element_note_ids = request.get("element_note_ids", [])
        syllable_note_ids = request.get("syllable_note_ids", [])
        deck_name = request.get("deck_name", "ÊãºÈü≥")
        
        # Support legacy API format
        if not element_note_ids and not syllable_note_ids:
            note_ids = request.get("note_ids", [])
            note_type = request.get("note_type")
            if note_type == "element":
                element_note_ids = note_ids
            elif note_type == "syllable":
                syllable_note_ids = note_ids
        
        if not profile_id:
            raise HTTPException(status_code=400, detail="profile_id is required")
        
        if not element_note_ids and not syllable_note_ids:
            raise HTTPException(status_code=400, detail="At least one note_id is required")
        
        # Get notes from database, preserving order
        db = next(get_db())
        try:
            all_notes = []
            
            # Get element notes ordered by display_order
            if element_note_ids:
                element_notes = db.query(PinyinElementNote).filter(
                    PinyinElementNote.note_id.in_(element_note_ids)
                ).order_by(PinyinElementNote.display_order).all()
                for db_note in element_notes:
                    note_fields = json.loads(db_note.fields) if db_note.fields else {}
                    all_notes.append({
                        'note_id': db_note.note_id,
                        'type': 'element',
                        'display_order': db_note.display_order,
                        'fields': note_fields,
                        'element': db_note.element
                    })
            
            # Get syllable notes ordered by display_order
            if syllable_note_ids:
                syllable_notes = db.query(PinyinSyllableNote).filter(
                    PinyinSyllableNote.note_id.in_(syllable_note_ids)
                ).order_by(PinyinSyllableNote.display_order).all()
                for db_note in syllable_notes:
                    note_fields = json.loads(db_note.fields) if db_note.fields else {}
                    all_notes.append({
                        'note_id': db_note.note_id,
                        'type': 'syllable',
                        'display_order': db_note.display_order,
                        'fields': note_fields,
                        'syllable': db_note.syllable
                    })
            
            # Sort all notes by display_order to preserve .apkg order
            all_notes.sort(key=lambda x: x['display_order'])
        finally:
            db.close()
        
        if not all_notes:
            return {
                "message": "No notes found to sync",
                "cards_created": 0,
                "notes_synced": 0
            }
        
        # Connect to Anki
        anki = AnkiConnect()
        if not anki.ping():
            raise HTTPException(status_code=500, detail="AnkiConnect not available")
        
        # Sync to Anki in order
        cards_created = 0
        notes_synced = 0
        errors = []
        
        for note in all_notes:
            try:
                fields = note['fields']
                note_type = note['type']
                
                # Build Anki note fields
                anki_note_fields = {}
                for field_name, field_value in fields.items():
                    if field_value:
                        anki_note_fields[field_name] = field_value
                    else:
                        anki_note_fields[field_name] = ""
                
                # Build _KG_Map following strict schema
                if note_type == "element":
                    element = fields.get('Element', '') or note.get('element', '')
                    element_kp = f"pinyin-element-{element}"
                    # Element cards are teaching cards - map to appropriate skill
                    card_mappings = {
                        "0": [{"kp": element_kp, "skill": "form_to_sound", "weight": 1.0}],  # Element => Sound
                    }
                    anki_note_type = "CUMA - Pinyin Element"
                    cards_per_note = 1
                else:  # syllable
                    syllable = fields.get('Syllable', '') or note.get('syllable', '')
                    syllable_kp = f"pinyin-syllable-{syllable}"
                    # Syllable has 6 cards (indices 0-5):
                    # Card 0: Element Card
                    # Card 1: Word to Pinyin (sound_to_form)
                    # Card 2: MCQ Recent (sound_to_form with hint)
                    # Card 3: MCQ Tone (sound_to_form with hint)
                    # Card 4: MCQ Confusor (sound_to_form with hint)
                    # Card 5: Pinyin to Word (form_to_sound)
                    card_mappings = {
                        "0": [{"kp": syllable_kp, "skill": "form_to_sound", "weight": 1.0}],  # Element Card
                        "1": [{"kp": syllable_kp, "skill": "sound_to_form", "weight": 1.0}],  # Word to Pinyin
                        "2": [{"kp": syllable_kp, "skill": "sound_to_form", "weight": 0.8}],  # MCQ Recent
                        "3": [{"kp": syllable_kp, "skill": "sound_to_form", "weight": 0.8}],  # MCQ Tone
                        "4": [{"kp": syllable_kp, "skill": "sound_to_form", "weight": 0.8}],  # MCQ Confusor
                        "5": [{"kp": syllable_kp, "skill": "form_to_sound", "weight": 1.0}],  # Pinyin to Word
                    }
                    anki_note_type = "CUMA - Pinyin Syllable"
                    cards_per_note = 6
                
                anki_note_fields["_KG_Map"] = build_kg_map_strict(card_mappings)
                
                # Create note in Anki
                anki.add_note(deck_name, anki_note_type, anki_note_fields)
                
                cards_created += cards_per_note
                notes_synced += 1
                
            except Exception as e:
                errors.append({
                    "note_id": note.get('note_id', 'unknown'),
                    "error": str(e)
                })
                import traceback
                traceback.print_exc()
        
        notes_synced_successfully = len(all_notes) - len(errors)
        print(f"‚úÖ Synced {notes_synced_successfully} pinyin notes (creating {cards_created} cards) to deck '{deck_name}'")
        if errors:
            print(f"‚ö†Ô∏è  {len(errors)} errors occurred")
        
        return {
            "message": f"Synced {notes_synced_successfully} notes successfully",
            "cards_created": cards_created,
            "notes_synced": notes_synced_successfully,
            "errors": errors
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error syncing pinyin notes: {str(e)}")


@app.get("/pinyin/word-info")
async def get_pinyin_for_word(word: str):
    """
    Get pinyin and other info for a Chinese word from the knowledge graph.
    Ensures pinyin follows 'i u Âπ∂ÂàóÊ†áÂú®Âêé' rule.
    
    Uses get_word_knowledge which includes KG lookup, cache, and LLM fallback if needed.
    Runs in executor with timeout to prevent hanging.
    """
    import asyncio
    try:
        if not word or not word.strip():
            raise HTTPException(status_code=400, detail="word parameter is required")
        
        word_stripped = word.strip()
        
        # Use get_word_knowledge but run it in executor with timeout to prevent hanging
        # This includes KG lookup, cache, and LLM fallback if needed
        loop = asyncio.get_event_loop()
        try:
            word_info = await asyncio.wait_for(
                loop.run_in_executor(None, lambda: get_word_knowledge(word_stripped)),
                timeout=8.0  # 8 second timeout for LLM call if needed
            )
        except asyncio.TimeoutError:
            # If timeout, fall back to KG + cache only (no LLM)
            print(f"‚ö†Ô∏è [word-info] Timeout for '{word_stripped}', using KG+cache only")
            info = fetch_word_knowledge_points(word_stripped) or {}
            pronunciations: List[str] = list(dict.fromkeys(info.get("pronunciations") or []))
            meanings: List[str] = list(dict.fromkeys(info.get("meanings") or []))
            hsk_level = info.get("hsk_level")
            
            # Check cache
            cached = _word_kp_cache.get(word_stripped) or {}
            cache_pinyin = cached.get("pinyin")
            cache_meaning = cached.get("meaning") or cached.get("meaning_en")
            cache_pron_list = cached.get("pronunciations") or []
            cache_meaning_list = cached.get("meanings") or []
            cache_hsk = cached.get("hsk_level")
            
            # Merge cache data
            for val in [cache_pinyin] + cache_pron_list:
                if val and val not in pronunciations:
                    pronunciations.append(val)
            for val in [cache_meaning] + cache_meaning_list:
                if val and val not in meanings:
                    meanings.append(val)
            if not hsk_level and cache_hsk:
                hsk_level = cache_hsk
            
            word_info = {
                "pronunciations": pronunciations,
                "meanings": meanings,
                "hsk_level": hsk_level
            }
        
        # Extract pinyin (use first pronunciation if available)
        pinyin = None
        if word_info.get("pronunciations"):
            pinyin = word_info["pronunciations"][0]
            # Fix tone placement for iu/ui patterns
            pinyin = fix_iu_ui_tone_placement(pinyin)
        
        # Also fix pronunciations list
        fixed_pronunciations = [fix_iu_ui_tone_placement(p) for p in word_info.get("pronunciations", [])]
        
        return {
            "word": word.strip(),
            "pinyin": pinyin,
            "pronunciations": fixed_pronunciations,
            "meanings": word_info.get("meanings", []),
            "hsk_level": word_info.get("hsk_level")
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error getting word info: {str(e)}")


@app.get("/pinyin/gap-fill-suggestions")
async def get_pinyin_gap_fill_suggestions():
    """
    Get pinyin gap fill suggestions from the CSV file.
    """
    import asyncio
    try:
        suggestions_file = PROJECT_ROOT / "data" / "pinyin_gap_fill_suggestions.csv"
        
        if not suggestions_file.exists():
            raise HTTPException(status_code=404, detail="Suggestions file not found. Please run find_best_pinyin_words.py first.")
        
        # Run file I/O in thread executor to avoid blocking event loop
        def read_csv_file():
            suggestions = []
            with open(suggestions_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    suggestions.append(dict(row))
            return suggestions
        
        loop = asyncio.get_event_loop()
        suggestions = await loop.run_in_executor(None, read_csv_file)
        
        return {
            "suggestions": suggestions,
            "total": len(suggestions)
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error loading suggestions: {str(e)}")


@app.put("/pinyin/gap-fill-suggestions")
async def save_pinyin_gap_fill_suggestions(request: Dict[str, Any]):
    """
    Save approved/edited pinyin gap fill suggestions.
    """
    import asyncio
    try:
        suggestions_file = PROJECT_ROOT / "data" / "pinyin_gap_fill_suggestions.csv"
        approved_suggestions = request.get("suggestions", [])
        
        if not approved_suggestions:
            return {"message": "No suggestions to save", "saved": 0}
        
        print(f"üíæ [SAVE] Starting save of {len(approved_suggestions)} suggestions to CSV...")
        print(f"üíæ [SAVE] File path: {suggestions_file}")
        
        # Load existing suggestions
        print(f"üíæ [SAVE] Loading existing suggestions from CSV...")
        existing_suggestions = {}
        if suggestions_file.exists():
            try:
                with open(suggestions_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    row_count = 0
                    for row in reader:
                        syllable = row.get('Syllable')
                        if syllable:
                            existing_suggestions[syllable] = dict(row)
                            row_count += 1
                    print(f"üíæ [SAVE] Loaded {row_count} existing suggestions")
            except Exception as e:
                print(f"‚ö†Ô∏è [SAVE] Error reading existing suggestions: {e}")
                import traceback
                traceback.print_exc()
                # Continue with empty dict if read fails
        
        # Update with approved/edited suggestions
        print(f"üíæ [SAVE] Updating {len(approved_suggestions)} suggestions...")
        for suggestion in approved_suggestions:
            syllable = suggestion.get('Syllable')
            if syllable:
                # Ensure all fields are properly preserved, especially Image File
                image_file = suggestion.get('Image File', '') or ''
                if image_file and str(image_file).strip():
                    suggestion['Has Image'] = 'Yes'
                    suggestion['Image File'] = str(image_file).strip()
                else:
                    suggestion['Has Image'] = 'No'
                    suggestion['Image File'] = ''
                
                if syllable in existing_suggestions:
                    existing_suggestions[syllable].update(suggestion)
                else:
                    existing_suggestions[syllable] = suggestion
                
                print(f"üíæ [SAVE] Updated syllable '{syllable}': Image File = '{suggestion.get('Image File', '')}', Has Image = '{suggestion.get('Has Image', '')}'")
        
        # Save back to CSV
        if existing_suggestions:
            fieldnames = ['Syllable', 'Suggested Word', 'Word Pinyin', 'HSK Level', 
                         'Frequency Rank', 'Has Image', 'Image File', 'Concreteness', 
                         'AoA', 'Num Syllables', 'Score', 'approved']
            
            # Write to a temporary file first, then rename (atomic operation)
            import tempfile
            import shutil
            temp_file = suggestions_file.with_suffix('.tmp')
            
            try:
                print(f"üìù [SAVE] Writing to temp file: {temp_file}")
                # Run file I/O in thread executor to avoid blocking event loop
                import asyncio
                
                def write_csv_file():
                    with open(temp_file, 'w', encoding='utf-8', newline='') as f:
                        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
                        writer.writeheader()
                        print(f"üìù [SAVE] Writing {len(existing_suggestions)} suggestions...")
                        for suggestion in existing_suggestions.values():
                            writer.writerow(suggestion)
                    print(f"üìù [SAVE] Temp file written successfully")
                
                # Run in executor with timeout
                loop = asyncio.get_event_loop()
                await asyncio.wait_for(
                    loop.run_in_executor(None, write_csv_file),
                    timeout=30.0  # 30 second timeout for file write
                )
                
                print(f"üìù [SAVE] Renaming temp file to: {suggestions_file}")
                # Atomic rename - also run in executor
                await loop.run_in_executor(
                    None,
                    lambda: shutil.move(str(temp_file), str(suggestions_file))
                )
                print(f"‚úÖ [SAVE] Successfully saved {len(existing_suggestions)} suggestions")
            except Exception as e:
                # Clean up temp file on error
                if temp_file.exists():
                    temp_file.unlink()
                raise
        
        return {
            "message": f"Saved {len(approved_suggestions)} suggestions",
            "saved": len(approved_suggestions)
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error saving suggestions: {str(e)}")


def generate_tone_variations(syllable: str) -> list:
    """
    Generate all 4 tone variations of a syllable.
    Example: 'zhen' -> ['zhƒìn', 'zh√©n', 'zhƒõn', 'zh√®n']
    """
    from scripts.knowledge_graph.pinyin_parser import extract_tone, add_tone_to_final
    
    syllable_no_tone, _ = extract_tone(syllable)
    if not syllable_no_tone:
        return ['', '', '', '']
    
    variations = []
    for tone in [1, 2, 3, 4]:
        toned = add_tone_to_final(syllable_no_tone, tone)
        variations.append(toned)
    return variations


def generate_confusors(syllable: str) -> list:
    """
    Generate confusor syllables (similar syllables that might be confused).
    Based on pattern: change initial to b/p and vary tones.
    """
    from scripts.knowledge_graph.pinyin_parser import extract_tone, add_tone_to_final
    
    syllable_no_tone, _ = extract_tone(syllable)
    if not syllable_no_tone or len(syllable_no_tone) < 2:
        return ['', '', '']
    
    # Extract initial and final
    initial = syllable_no_tone[0]
    final = syllable_no_tone[1:] if len(syllable_no_tone) > 1 else ''
    
    # Generate confusors: b+final (tone 1), p+final (tone 2), original with different tone
    confusors = []
    if final:
        confusors.append(add_tone_to_final('b' + final, 1))
        confusors.append(add_tone_to_final('p' + final, 2))
    else:
        confusors.append(add_tone_to_final('ba', 1))
        confusors.append(add_tone_to_final('pa', 2))
    
    # Third confusor: original syllable with tone 3 or 4 (if original wasn't tone 3)
    original_toned = add_tone_to_final(syllable_no_tone, 3)
    if original_toned == syllable:
        original_toned = add_tone_to_final(syllable_no_tone, 4)
    confusors.append(original_toned)
    
    return confusors


def get_element_to_learn(syllable: str) -> str:
    """
    Determine ElementToLearn from syllable.
    For syllable cards, ElementToLearn is typically the final (ÈüµÊØç).
    Example: 'zhen' -> 'en' (final)
    """
    from scripts.knowledge_graph.pinyin_parser import parse_pinyin, extract_tone
    
    syllable_no_tone, _ = extract_tone(syllable)
    if not syllable_no_tone:
        return ''
    
    parsed = parse_pinyin(syllable_no_tone)
    # ElementToLearn is the final (ÈüµÊØç)
    return parsed.get('final', '') or ''


def generate_word_audio(word: str) -> str:
    """
    Generate WordAudio field in format: [sound:cm_tts_zh_<word>.mp3]
    """
    if not word:
        return ''
    return f"[sound:cm_tts_zh_{word}.mp3]"


@app.post("/pinyin/apply-suggestions")
async def apply_pinyin_suggestions(request: Dict[str, Any]):
    """
    Apply approved pinyin suggestions to the pinyin deck database.
    Creates new PinyinSyllableNote entries for approved suggestions.
    """
    try:
        from database.models import PinyinSyllableNote
        from database.db import get_db
        
        suggestions = request.get("suggestions", [])
        profile_id = request.get("profile_id")
        
        if not suggestions:
            raise HTTPException(status_code=400, detail="No suggestions provided")
        
        if not profile_id:
            raise HTTPException(status_code=400, detail="profile_id is required")
        
        from database.db import get_db_session
        
        created_count = 0
        updated_count = 0
        errors = []
        
        with get_db_session() as db:
            # Get the highest display_order to append new notes
            max_order = db.query(PinyinSyllableNote.display_order).order_by(
                PinyinSyllableNote.display_order.desc()
            ).first()
            next_order = (max_order[0] if max_order else 0) + 1
            
            for suggestion in suggestions:
                try:
                    # Handle both CSV field names (from frontend) and direct field names
                    syllable = suggestion.get('Syllable') or suggestion.get('syllable', '')
                    word = suggestion.get('Suggested Word') or suggestion.get('word', '')
                    pinyin = suggestion.get('Word Pinyin') or suggestion.get('pinyin', '')
                    image_file = suggestion.get('Image File') or suggestion.get('image_file', '')
                    
                    if not syllable or not word or word == 'NONE':
                        continue
                    
                    # Check if note already exists for this syllable AND word combination
                    # This prevents duplicates when same syllable is used for different words
                    existing = db.query(PinyinSyllableNote).filter(
                        PinyinSyllableNote.syllable == syllable,
                        PinyinSyllableNote.word == word
                    ).first()
                    
                    # If not found by syllable+word, check by syllable only (for backward compatibility)
                    if not existing:
                        existing = db.query(PinyinSyllableNote).filter(
                            PinyinSyllableNote.syllable == syllable
                        ).first()
                    
                    # Fetch concept from knowledge graph
                    concept = word  # Default to word if concept not found
                    try:
                        word_info = get_word_knowledge(word)
                        if word_info.get("meanings"):
                            # Use first English meaning as concept
                            concept = word_info["meanings"][0]
                        elif word_info.get("pronunciations"):
                            # Fallback: use word itself
                            concept = word
                    except Exception:
                        # If KG lookup fails, use word as concept
                        concept = word
                    
                    if existing:
                        # Update existing note (idempotent - safe to apply multiple times)
                        existing.word = word
                        existing.concept = concept
                        fields = json.loads(existing.fields) if existing.fields else {}
                        
                        # Update basic fields
                        if pinyin:
                            fields['WordPinyin'] = fix_iu_ui_tone_placement(pinyin)
                        fields['WordHanzi'] = word
                        
                        # Update image if provided
                        if image_file:
                            fields['WordPicture'] = f'<img src="{image_file}">'
                        
                        # Generate missing fields if they don't exist
                        if not fields.get('Tone1') or not fields.get('Tone2') or not fields.get('Tone3') or not fields.get('Tone4'):
                            tone_variations = generate_tone_variations(syllable)
                            if not fields.get('Tone1'):
                                fields['Tone1'] = tone_variations[0] if len(tone_variations) > 0 else ''
                            if not fields.get('Tone2'):
                                fields['Tone2'] = tone_variations[1] if len(tone_variations) > 1 else ''
                            if not fields.get('Tone3'):
                                fields['Tone3'] = tone_variations[2] if len(tone_variations) > 2 else ''
                            if not fields.get('Tone4'):
                                fields['Tone4'] = tone_variations[3] if len(tone_variations) > 3 else ''
                        
                        if not fields.get('Confusor1') or not fields.get('Confusor2') or not fields.get('Confusor3'):
                            confusors = generate_confusors(syllable)
                            if not fields.get('Confusor1'):
                                fields['Confusor1'] = confusors[0] if len(confusors) > 0 else ''
                                fields['ConfusorPicture1'] = ''
                            if not fields.get('Confusor2'):
                                fields['Confusor2'] = confusors[1] if len(confusors) > 1 else ''
                                fields['ConfusorPicture2'] = ''
                            if not fields.get('Confusor3'):
                                fields['Confusor3'] = confusors[2] if len(confusors) > 2 else ''
                                fields['ConfusorPicture3'] = ''
                        
                        if not fields.get('ElementToLearn'):
                            fields['ElementToLearn'] = get_element_to_learn(syllable)
                        
                        if not fields.get('WordAudio'):
                            fields['WordAudio'] = generate_word_audio(word)
                        
                        existing.fields = json.dumps(fields, ensure_ascii=False)
                        updated_count += 1
                    else:
                        # Create new note
                        # Fix tone placement for iu/ui patterns in pinyin
                        fixed_pinyin = fix_iu_ui_tone_placement(pinyin or '') if pinyin else ''
                        
                        # Generate all required fields
                        tone_variations = generate_tone_variations(syllable)
                        confusors = generate_confusors(syllable)
                        element_to_learn = get_element_to_learn(syllable)
                        word_audio = generate_word_audio(word)
                        
                        # Format WordPicture as HTML img tag if image_file exists
                        word_picture = ''
                        if image_file:
                            word_picture = f'<img src="{image_file}">'
                        
                        fields = {
                            'ElementToLearn': element_to_learn,
                            'Syllable': syllable,
                            'WordPinyin': fixed_pinyin,
                            'WordHanzi': word,
                            'WordPicture': word_picture,
                            'WordAudio': word_audio,
                            'Tone1': tone_variations[0] if len(tone_variations) > 0 else '',
                            'Tone2': tone_variations[1] if len(tone_variations) > 1 else '',
                            'Tone3': tone_variations[2] if len(tone_variations) > 2 else '',
                            'Tone4': tone_variations[3] if len(tone_variations) > 3 else '',
                            'Confusor1': confusors[0] if len(confusors) > 0 else '',
                            'ConfusorPicture1': '',
                            'Confusor2': confusors[1] if len(confusors) > 1 else '',
                            'ConfusorPicture2': '',
                            'Confusor3': confusors[2] if len(confusors) > 2 else '',
                            'ConfusorPicture3': '',
                            '_Remarks': '',
                            '_KG_Map': '{}'
                        }
                        
                        new_note = PinyinSyllableNote(
                            note_id=f"syllable_{syllable}_{next_order}",
                            syllable=syllable,
                            word=word,
                            concept=concept,
                            fields=json.dumps(fields, ensure_ascii=False),
                            display_order=next_order
                        )
                        
                        db.add(new_note)
                        next_order += 1
                        created_count += 1
                
                except Exception as e:
                    errors.append(f"Error processing {suggestion.get('syllable', 'unknown')}: {str(e)}")
                    continue
        
        return {
            "message": f"Applied {len(suggestions)} suggestions ({created_count} created, {updated_count} updated)",
            "created": created_count,
            "updated": updated_count,
            "total": len(suggestions),
            "errors": errors
        }
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error applying suggestions: {str(e)}")


@app.get("/pinyin/english-vocab-suggestions")
async def get_english_vocab_suggestions_for_pending():
    """
    Get English vocabulary-based suggestions for pending pinyin syllables.
    Loads pre-computed matches from JSON file (generated by match_pending_syllables_english_vocab.py).
    Returns up to 3 suggestions per syllable with radio button options.
    """
    import json
    from pathlib import Path
    
    try:
        # Load pre-computed matches from JSON file
        matches_file = PROJECT_ROOT / "data" / "pending_syllable_english_matches.json"
        
        if not matches_file.exists():
            return {"matches": {}, "message": "No matches file found. Run match_pending_syllables_english_vocab.py first."}
        
        with open(matches_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return {
            "matches": data.get("matches", {}),
            "total_pending": data.get("total_pending", 0),
            "matched": data.get("matched", 0),
            "generated_at": data.get("generated_at", "")
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"matches": {}, "error": str(e)}


# Mount static files for serving images
# Images are stored in media/visual_images/ relative to project root
            
            if concreteness_file.exists():
                with open(concreteness_file, 'r', encoding='utf-8') as f:
                    next(f)  # Skip header
                    for line in f:
                        parts = line.strip().split('\t')
                        if len(parts) >= 3:
                            word = parts[0].lower().strip()
                            try:
                                rating = float(parts[2])  # Conc.M column
                                ratings[word] = rating
                            except (ValueError, IndexError):
                                continue
            return ratings
        
        def query_kg_for_english_word(english_word: str):
            """Query knowledge graph for Chinese translations of an English word"""
            sparql_word = _sanitize_for_sparql_literal(english_word.lower())
            # Try multiple SPARQL patterns to find matches
            sparql_queries = [
                # Pattern 1: Exact match in definition
                f"""
                PREFIX srs-kg: <http://srs4autism.com/schema/>
                PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                
                SELECT ?chinese ?pinyin WHERE {{
                    ?word a srs-kg:Word .
                    ?word srs-kg:definition ?definition .
                    FILTER(LANG(?definition) = "en" || LANG(?definition) = "")
                    FILTER(CONTAINS(LCASE(?definition), "{sparql_word}"))
                    ?word srs-kg:text ?chinese .
                    OPTIONAL {{ ?word srs-kg:pinyin ?pinyin . }}
                }}
                LIMIT 10
                """,
                # Pattern 2: Match in concept label
                f"""
                PREFIX srs-kg: <http://srs4autism.com/schema/>
                PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                
                SELECT ?chinese ?pinyin WHERE {{
                    ?word a srs-kg:Word .
                    ?word srs-kg:means ?concept .
                    ?concept rdfs:label ?label .
                    FILTER(LANG(?label) = "en" || LANG(?label) = "")
                    FILTER(CONTAINS(LCASE(?label), "{sparql_word}"))
                    ?word srs-kg:text ?chinese .
                    OPTIONAL {{ ?word srs-kg:pinyin ?pinyin . }}
                }}
                LIMIT 10
                """
            ]
            
            translations = []
            seen = set()
            
            for sparql in sparql_queries:
                try:
                    results = query_sparql(sparql, output_format="application/sparql-results+json")
                    bindings = []
                    if isinstance(results, dict):
                        bindings = results.get("results", {}).get("bindings", [])
                    
                    for row in bindings:
                        chinese = row.get("chinese", {}).get("value")
                        pinyin = row.get("pinyin", {}).get("value", "")
                        if chinese and chinese not in seen:
                            seen.add(chinese)
                            translations.append((chinese, pinyin))
                            if len(translations) >= 10:
                                return translations
                except Exception:
                    continue
            
            return translations
        
        def extract_english_words():
            """Extract English words from .apkg files"""
            words = set()
            apkg_files = [
                PROJECT_ROOT / "data" / "content_db" / "English__Vocabulary__1. Basic.apkg",
                PROJECT_ROOT / "data" / "content_db" / "English__Vocabulary__2. Level 2.apkg"
            ]
            
            for apkg_file in apkg_files:
                if not apkg_file.exists():
                    continue
                
                with tempfile.TemporaryDirectory() as tmpdir:
                    with zipfile.ZipFile(apkg_file, 'r') as zip_ref:
                        zip_ref.extractall(tmpdir)
                    
                    db_path = Path(tmpdir) / "collection.anki21"
                    if not db_path.exists():
                        db_path = Path(tmpdir) / "collection.anki2"
                    
                    if not db_path.exists():
                        continue
                    
                    conn = sqlite3.connect(db_path)
                    cursor = conn.cursor()
                    
                    try:
                        cursor.execute("SELECT flds FROM notes")
                        for row in cursor.fetchall():
                            fields = row[0].split('\x1f')
                            for field in fields:
                                field = re.sub(r'<[^>]+>', '', field).strip()
                                if field and re.match(r'^[a-zA-Z\s]+$', field):
                                    for word in field.split():
                                        word = word.lower().strip('.,!?;:"()[]{}')
                                        if len(word) > 1 and word.isalpha():
                                            words.add(word)
                    except Exception:
                        pass
                    finally:
                        conn.close()
            
            return words
        
        def normalize_syllable(syllable: str):
            return syllable.lower().strip()
        
        def split_pinyin_to_syllables(pinyin: str):
            """Split pinyin string into individual syllables"""
            pinyin = re.sub(r'[1-5]', '', pinyin)
            syllables = []
            for s in pinyin.split():
                s = s.strip()
                s = re.sub(r'[ƒÅ√°«é√†ƒì√©ƒõ√®ƒ´√≠«ê√¨≈ç√≥«í√≤≈´√∫«î√π«ñ«ò«ö«ú]', lambda m: {
                    'ƒÅ': 'a', '√°': 'a', '«é': 'a', '√†': 'a',
                    'ƒì': 'e', '√©': 'e', 'ƒõ': 'e', '√®': 'e',
                    'ƒ´': 'i', '√≠': 'i', '«ê': 'i', '√¨': 'i',
                    '≈ç': 'o', '√≥': 'o', '«í': 'o', '√≤': 'o',
                    '≈´': 'u', '√∫': 'u', '«î': 'u', '√π': 'u',
                    '«ñ': 'u', '«ò': 'u', '«ö': 'u', '«ú': 'u'
                }.get(m.group(0), m.group(0)), s)
                if s:
                    syllables.append(s.lower())
            return syllables
        
        def build_syllable_index_from_kg(english_words: set, concreteness_ratings: dict):
            """Build an index mapping syllables to Chinese words from knowledge graph"""
            syllable_index = defaultdict(list)
            
            # For each English word in vocabulary, query KG for Chinese translations
            for eng_word in english_words:
                translations = query_kg_for_english_word(eng_word)
                concreteness = concreteness_ratings.get(eng_word, 0.0)
                
                # Index by syllable
                for chinese, pinyin in translations:
                    pinyin_syllables = split_pinyin_to_syllables(pinyin)
                    for syllable in pinyin_syllables:
                        syllable_normalized = normalize_syllable(syllable)
                        syllable_index[syllable_normalized].append({
                            'english': eng_word,
                            'chinese': chinese,
                            'pinyin': pinyin,
                            'matched_syllable': syllable_normalized,
                            'concreteness': concreteness
                        })
            
            return syllable_index
        
        def find_chinese_words_for_syllable(syllable: str, syllable_index: dict, limit=3):
            """Find Chinese words for a syllable using pre-built index, prioritized by concreteness"""
            syllable_normalized = normalize_syllable(syllable)
            candidates = syllable_index.get(syllable_normalized, [])
            
            # Remove duplicates based on (chinese, english) combination
            seen_combinations = set()
            unique_candidates = []
            for candidate in candidates:
                key = (candidate['chinese'], candidate['english'])
                if key not in seen_combinations:
                    seen_combinations.add(key)
                    unique_candidates.append(candidate)
            
            # Sort by concreteness (higher is better, more concrete)
            unique_candidates.sort(key=lambda x: x.get('concreteness', 0.0), reverse=True)
            
            # Return top N
            return unique_candidates[:limit]
        
        # Run in executor to avoid blocking
        loop = asyncio.get_event_loop()
        
        # Load data (all fast operations)
        print("üìö Loading CEDICT and vocabulary data...")
        cedict_dict, english_words, concreteness_ratings = await asyncio.gather(
            loop.run_in_executor(None, load_cedict),
            loop.run_in_executor(None, extract_english_words),
            loop.run_in_executor(None, load_concreteness_ratings)
        )
        
        print(f"   Loaded {len(cedict_dict)} CEDICT entries")
        print(f"   Loaded {len(english_words)} English vocabulary words")
        print(f"   Loaded {len(concreteness_ratings)} concreteness ratings")
        
        # Build syllable index from CEDICT (fast - no SPARQL queries)
        print(f"üìä Building syllable index from CEDICT for {len(english_words)} English words...")
        syllable_index = await loop.run_in_executor(
            None, build_syllable_index_from_cedict, english_words, cedict_dict, concreteness_ratings
        )
        print(f"‚úÖ Built index with {len(syllable_index)} unique syllables")
        
        # Get pending syllables
        suggestions_file = PROJECT_ROOT / "data" / "pinyin_gap_fill_suggestions.csv"
        pending = []
        
        def get_pending():
            if not suggestions_file.exists():
                return []
            
            pending_list = []
            with open(suggestions_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    approved_val = row.get('approved', '').strip().lower()
                    if not approved_val or approved_val == 'false':
                        syllable = row.get('Syllable', '').strip()
                        word = row.get('Suggested Word', '').strip()
                        if syllable and (word == 'NONE' or not word):
                            pending_list.append({
                                'syllable': syllable,
                                'row': row
                            })
            return pending_list
        
        pending = await loop.run_in_executor(None, get_pending)
        
        # Match each pending syllable (now O(1) lookup instead of O(n))
        matches = {}
        for item in pending:
            syllable = item['syllable']
            suggestions = find_chinese_words_for_syllable(
                syllable, syllable_index, limit=3
            )
            if suggestions:
                matches[syllable] = suggestions
        
        return {
            "matches": matches,
            "total_pending": len(pending),
            "matched": len(matches)
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error matching syllables: {str(e)}")


# Mount static files for serving images
# Images are stored in media/visual_images/ relative to project root
media_dir = PROJECT_ROOT / "media"
if media_dir.exists():
    app.mount("/media", StaticFiles(directory=str(media_dir)), name="media")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
