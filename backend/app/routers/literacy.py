"""
Literacy Router V2 (Logic City Level 2)
Features:
- Interleaved Sorting (Concrete/Abstract)
- Advanced Anki Integration (Cloze, Pinyin Typing)
- Automatic Pinyin Generation
- Performance Optimizations (Caching, Startup Loading)
"""
from fastapi import APIRouter, HTTPException, Depends, Query
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from pathlib import Path
import logging
import json
import re
from html import unescape
import sys
import base64
import hashlib
import itertools
import time
from datetime import datetime
import threading
import csv

# Pinyin Library Check
try:
    from pypinyin import pinyin as get_pinyin, Style
    HAS_PYPINYIN = True
except ImportError:
    HAS_PYPINYIN = False
    print("⚠️ pypinyin not found. Pinyin generation may be inaccurate. Run: pip install pypinyin")

# Add project root to path
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from database.db import get_db
from database.services import ProfileService
from database.kg_client import KnowledgeGraphClient
from anki_integration.anki_connect import AnkiConnect

router = APIRouter(prefix="/literacy", tags=["literacy"])
logger = logging.getLogger(__name__)

# Path to the English Vocabulary Level 2 deck (Reference only)
ENGLISH_VOCAB_DECK = PROJECT_ROOT / "data" / "content_db" / "English__Vocabulary__2. Level 2.apkg"
# JSON file containing the extracted order
ANKI_ORDER_JSON = PROJECT_ROOT / "data" / "content_db" / "vocab_order.json"

# Cache configuration
CACHE_TTL = 3600  # 1 hour in seconds

# In-memory cache for original order (word -> order_index)
_anki_order_cache: Optional[Dict[str, int]] = None

# Global cache for sorted vocabulary list
_sorted_vocab_cache: Optional[List[Dict[str, Any]]] = None
_cache_timestamp: Optional[float] = None
_cache_lock = threading.Lock()

# --- MODELS ---

class LogicCityVocabItem(BaseModel):
    """Vocabulary item for Logic City"""
    word_id: str
    english: str
    chinese: Optional[str] = None
    pinyin: Optional[str] = None
    image_path: Optional[str] = None
    word_type: str = "Concrete"  # 'Concrete' or 'Abstract'
    sentence: Optional[str] = None
    synonym_hint: Optional[str] = None
    anki_order: Optional[int] = None
    is_mastered: bool = False
    is_synced: bool = False

class PaginatedVocabResponse(BaseModel):
    items: List[LogicCityVocabItem]
    total: int
    page: int
    page_size: int
    total_pages: int

# --- HELPERS ---

def clean_anki_field(text: str) -> str:
    if not text: return ""
    text = unescape(text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\[.*?\]', '', text)
    text = text.replace('\xa0', ' ')
    return text.strip().lower()

def _load_anki_order_from_json() -> Dict[str, int]:
    """
    Load the original order of words from the JSON file generated by scripts/extract_vocab_order.py.
    """
    if not ANKI_ORDER_JSON.exists():
        logger.warning(f"Anki order JSON not found: {ANKI_ORDER_JSON}")
        return {}
    
    try:
        with open(ANKI_ORDER_JSON, 'r', encoding='utf-8') as f:
            order_map = json.load(f)
            logger.info(f"Loaded Anki order from JSON: {len(order_map)} words")
            return order_map
    except Exception as e:
        logger.error(f"Error reading Anki order JSON: {e}")
        return {}

def get_anki_original_order() -> Dict[str, int]:
    """
    Get Anki order from cache. Should be pre-loaded at startup.
    """
    global _anki_order_cache
    
    if _anki_order_cache is not None:
        return _anki_order_cache
    
    # Lazy load if not initialized
    _anki_order_cache = _load_anki_order_from_json()
    return _anki_order_cache

def query_sparql(query: str, output_format: str = "application/sparql-results+json", timeout: int = 30):
    """Execute a SPARQL query against Oxigraph knowledge graph store."""
    try:
        kg_client = KnowledgeGraphClient()
        result = kg_client.query(query)
        # Return JSON format (Oxigraph always returns JSON format compatible with SPARQL JSON results)
        return result
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Graph Error: {str(e)}")

def find_image_file(image_path: str) -> Optional[Path]:
    if not image_path: return None
    
    # Clean the input path to get just the filename
    filename = Path(image_path).name
    
    # Define explicit search paths based on the user's project structure
    media_dirs = [
        PROJECT_ROOT / "content" / "media" / "objects",  # <--- ADD THIS

        #Legacy
        PROJECT_ROOT / "content" / "media" / "images",        # Primary match for your structure
        PROJECT_ROOT / "content" / "media" / "visual_images",
        PROJECT_ROOT / "media" / "images",
        PROJECT_ROOT / "media" / "visual_images",
        PROJECT_ROOT / "media"
    ]
    
    # Search for the file
    for d in media_dirs:
        candidate = d / filename
        if candidate.exists(): 
            return candidate
            
    logger.warning(f"Image not found: {filename} (Checked {len(media_dirs)} dirs)")
    return None

# --- GATEKEEPER CONFIG ---
CURATION_REPORT_PATH = PROJECT_ROOT / "logs" / "vision_cleanup_report.csv"

def _load_curation_blocklist() -> Dict[str, bool]:
    """
    Loads the curation report to determine which words to BLOCK.
    """
    if not CURATION_REPORT_PATH.exists():
        logger.info(f"Gatekeeper: Report not found at {CURATION_REPORT_PATH}. Showing ALL.")
        return set()

    blocklist = set()

    try:
        with open(CURATION_REPORT_PATH, 'r', encoding='utf-8') as f:
            content = f.read()
            # Handle Excel BOM if present
            if content.startswith('\ufeff'): content = content[1:]

            reader = csv.DictReader(content.splitlines())

            for row in reader:
                english_word = row.get('English_Word', '').strip().lower()
                if not english_word: continue

                # Check columns case-insensitively
                reviewed = row.get('Reviewed', '').strip().lower() == 'true'
                match_status = row.get('Match?', '').strip().lower() == 'true'
                new_filename = row.get('New_Filename', '').strip()

                # BLOCK CONDITION:
                # You reviewed it AND (You said it's not a match OR You explicitly wrote DELETE)
                is_rejected = (not match_status) or (new_filename.upper() == 'DELETE')

                if reviewed and is_rejected:
                    # If you corrected it with a valid filename, don't block.
                    if new_filename and new_filename.upper() != 'DELETE':
                        continue

                    blocklist.add(english_word)

        logger.info(f"Gatekeeper Active: Blocking {len(blocklist)} explicitly deleted words.")
        return blocklist

    except Exception as e:
        logger.error(f"Gatekeeper Error: {e}")
        return set()

def _build_sorted_vocab_cache(sort_order: str = "interleaved") -> List[Dict[str, Any]]:
    """
    Build the sorted vocabulary cache by querying the knowledge graph.
    This is expensive and should be cached.
    """
    anki_order = get_anki_original_order()

    # 1. LOAD THE BLOCKLIST
    blocklist = _load_curation_blocklist()

    # Optimized query (updated for ontology v2: uses srs-kg:text|rdfs:label)
    light_query = """
    PREFIX srs-kg: <http://srs4autism.com/schema/>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT DISTINCT ?wordUri ?englishWord (SAMPLE(?imageNode) AS ?exampleImage) WHERE {
        ?zhNode srs-kg:learningTheme "Logic City" ; srs-kg:means ?concept .
        ?wordUri a srs-kg:Word ; srs-kg:means ?concept ; srs-kg:text|rdfs:label ?englishWord .
        FILTER (lang(?englishWord) = "en")
        OPTIONAL { ?concept srs-kg:hasVisualization ?imageNode }
    } GROUP BY ?wordUri ?englishWord
    """

    light_result = query_sparql(light_query)
    bindings = light_result.get("results", {}).get("bindings", [])

    # Bucketing
    concrete_list = []
    abstract_list = []
    seen = set()

    for b in bindings:
        english = b['englishWord']['value'].strip()
        english_lower = english.lower()

        # 2. CHECK BLOCKLIST (Explicit Deletions)
        if english_lower in blocklist:
            continue

        # 3. CHECK HALLUCINATIONS (The "Smoke" Check)
        # If it's not in the original Anki deck list, it's a generated ghost. Block it.
        if english_lower not in anki_order:
            continue

        if english_lower in seen: continue
        seen.add(english_lower)

        word_uri = b['wordUri']['value']
        has_image = 'exampleImage' in b and b['exampleImage'].get('value')

        item = {
            'word_uri': word_uri,
            'word_id': word_uri.split('/')[-1],
            'english': english,
            'word_type': 'Concrete' if has_image else 'Abstract',
            'anki_order': anki_order.get(english_lower, 999999)
        }

        if item['word_type'] == 'Concrete':
            concrete_list.append(item)
        else:
            abstract_list.append(item)

    # Sorting Strategy
    final_list = []
    if sort_order == "interleaved":
        concrete_list.sort(key=lambda x: x['anki_order'])
        abstract_list.sort(key=lambda x: x['anki_order'])
        for c, a in itertools.zip_longest(concrete_list, abstract_list):
            if c: final_list.append(c)
            if a: final_list.append(a)
    else:
        final_list = concrete_list + abstract_list
        final_list.sort(key=lambda x: x['anki_order'])

    return final_list

def get_sorted_vocab_cache(force_refresh: bool = False, sort_order: str = "interleaved") -> List[Dict[str, Any]]:
    """
    Get the sorted vocabulary cache, rebuilding if necessary.
    Thread-safe with locking to prevent concurrent rebuilds.
    """
    global _sorted_vocab_cache, _cache_timestamp
    
    with _cache_lock:
        current_time = time.time()
        
        # Check if cache is valid
        if (_sorted_vocab_cache is not None and 
            _cache_timestamp is not None and 
            not force_refresh and
            (current_time - _cache_timestamp) < CACHE_TTL):
            return _sorted_vocab_cache
        
        # Rebuild cache
        logger.info(f"Building sorted vocabulary cache (force_refresh={force_refresh})...")
        _sorted_vocab_cache = _build_sorted_vocab_cache(sort_order)
        _cache_timestamp = current_time
        logger.info(f"Cache built: {len(_sorted_vocab_cache)} words")
        return _sorted_vocab_cache

def initialize_literacy_cache():
    """
    Initialize the cache at startup. Call this from the main app's startup event.
    """
    logger.info("Initializing literacy cache at startup...")
    global _anki_order_cache
    
    # Load Anki order
    _anki_order_cache = _load_anki_order_from_json()
    
    # Pre-populate vocab cache
    get_sorted_vocab_cache(force_refresh=False)
    
    logger.info("Literacy cache initialized")

# --- ANKI ARCHITECTURE ---

def ensure_cuma_level2_model(anki: AnkiConnect) -> None:
    """Creates the V2 Note Type with simplified fields."""
    model_name = "CUMA-Word-Entity-v2"
    fields = ["Word", "WordAudio", "WordPicture", "Category", "UUID", "_Remarks", "_KG_Map"]
    
    css = """
    .card { font-family: Arial; font-size: 24px; text-align: center; color: #333; background-color: #f9f9f9; padding: 20px; }
    .word { font-size: 2em; font-weight: bold; margin-bottom: 20px; }
    .image img { max-height: 300px; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
    .category { font-size: 0.8em; color: #888; margin-top: 20px; }
    """
    
    # Basic Template
    front = """
    <div class="word">{{Word}}</div>
    <div class="image">{{WordPicture}}</div>
    """
    
    back = """
    <div class="word">{{Word}}</div>
    <div class="image">{{WordPicture}}</div>
    <div class="category">{{Category}}</div>
    {{WordAudio}}
    """
    
    existing_models = anki._invoke("modelNames", {})
    if model_name not in existing_models:
        logger.info(f"Creating Advanced Note Model: {model_name}")
        anki._invoke("createModel", {
            "modelName": model_name,
            "inOrderFields": fields,
            "css": css,
            "cardTemplates": [{"Name": "Card 1", "Front": front, "Back": back}]
        })
    else:
        # Ensure fields exist if updating
        curr_fields = anki._invoke("modelFieldNames", {"modelName": model_name})
        for f in fields:
            if f not in curr_fields:
                anki._invoke("modelFieldAdd", {"modelName": model_name, "fieldName": f})

# --- ENDPOINTS ---

@router.get("/logic-city/vocab", response_model=PaginatedVocabResponse)
async def get_logic_city_vocab(
    page: int = Query(1, ge=1),
    page_size: int = Query(50, ge=1),
    profile_id: Optional[str] = None,
    sort_order: str = "interleaved",
    force_refresh: bool = Query(False, description="Force cache refresh")
):
    """
    Get Logic City vocabulary items with optimized caching.
    """
    try:
        # Get cached sorted list (or rebuild if needed)
        final_list = get_sorted_vocab_cache(force_refresh=force_refresh, sort_order=sort_order)
        
        # OPTIMIZATION: Paginate FIRST, then only process details for the current page
        total = len(final_list)
        total_pages = (total + page_size - 1) // page_size
        start = (page - 1) * page_size
        paged_items = final_list[start : start + page_size]
        
        # Early return if no items
        if not paged_items:
            return PaginatedVocabResponse(
                items=[],
                total=total,
                page=page,
                page_size=page_size,
                total_pages=total_pages
            )
        
        # STEP 2: Detail Fetch - ONLY for the current page (50 items max)
        uris = " ".join([f"<{x['word_uri']}>" for x in paged_items])
        
        detail_query = f"""
        PREFIX srs-kg: <http://srs4autism.com/schema/>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        SELECT ?wordUri ?chineseWord ?imagePath WHERE {{
            VALUES ?wordUri {{ {uris} }}
            ?wordUri srs-kg:means ?concept .

            # Fetch ONLY the Logic City tagged Chinese word (updated to check srs-kg:text)
            OPTIONAL {{
                ?zhNode srs-kg:text|rdfs:label ?chineseWord ;
                        srs-kg:means ?concept ;
                        srs-kg:learningTheme "Logic City" .
                FILTER (lang(?chineseWord) = "zh")
            }}

            OPTIONAL {{
                ?concept srs-kg:hasVisualization ?v . ?v srs-kg:imageFilePath ?imagePath .
            }}
        }}
        """
        
        detail_res = query_sparql(detail_query)
        details_map = {}
        for b in detail_res.get("results", {}).get("bindings", []):
            uri = b['wordUri']['value']
            if uri not in details_map: details_map[uri] = {}
            if 'chineseWord' in b: details_map[uri]['chinese'] = b['chineseWord']['value']
            if 'imagePath' in b: details_map[uri]['image_path'] = b['imagePath']['value']
        
        # Build Final Response - only for the current page
        vocab = []
        for item in paged_items:
            det = details_map.get(item['word_uri'], {})
            chinese = det.get('chinese', '')
            
            # --- START FIX: Image Path Resolution ---
            raw_img_path = det.get('image_path')
            final_image_path = None
            
            if raw_img_path:
                # 1. Use existing helper to find the physical file on disk
                found_path = find_image_file(raw_img_path)
                
                if found_path:
                    # 2. If found, construct a valid URL path
                    media_objects_dir = PROJECT_ROOT / "content" / "media" / "objects"
                    
                    if media_objects_dir in found_path.parents or found_path.parent == media_objects_dir:
                        final_image_path = f"/static/media/{found_path.name}"
                    else:
                        try:
                            rel_path = found_path.relative_to(PROJECT_ROOT)
                            final_image_path = f"/{rel_path}"
                        except ValueError:
                            final_image_path = f"/media/{found_path.name}"
                else:
                    # 3. Fallback: Clean the raw path if file not found on disk
                    clean_path = raw_img_path.replace("content/media/", "").replace("/media/", "")
                    if clean_path.startswith("/"): clean_path = clean_path[1:]
                    if re.match(r'^[0-9a-fA-F]+\.(jpg|png|jpeg|webp|gif)$', clean_path, re.IGNORECASE):
                         final_image_path = f"/static/media/{clean_path}"
                    else:
                         final_image_path = f"/media/{clean_path}"
            # --- END FIX ---

            # Generate Pinyin
            pinyin_str = ""
            if chinese and HAS_PYPINYIN:
                pinyin_str = " ".join([x[0] for x in get_pinyin(chinese, style=Style.TONE)])
            
            vocab.append(LogicCityVocabItem(
                word_id=item['word_id'],
                english=item['english'],
                chinese=chinese,
                pinyin=pinyin_str,
                image_path=final_image_path,
                word_type=item['word_type'],
                anki_order=item['anki_order']
            ))
        
        return PaginatedVocabResponse(
            items=vocab,
            total=total,
            page=page,
            page_size=page_size,
            total_pages=total_pages
        )
    
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/logic-city/sync")
async def sync_logic_city_to_anki(request: Dict[str, Any]):
    """Syncs cards using the new CUMA-Word-Entity-v2 Note Type"""
    try:
        word_ids = request.get("word_ids", [])
        deck_name = request.get("deck_name", "English Vocabulary Level 2")
        anki = AnkiConnect()
        if not anki.ping(): raise HTTPException(503, "Anki unreachable")
        
        anki.create_deck(deck_name)
        ensure_cuma_level2_model(anki)
        
        # Fetch Data
        uris = [f"http://srs4autism.com/schema/{wid}" for wid in word_ids]
        uri_str = " ".join([f"<{u}>" for u in uris])
        
        q = f"""
        PREFIX srs-kg: <http://srs4autism.com/schema/>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        SELECT ?wordUri ?english ?chinese ?imagePath WHERE {{
            VALUES ?wordUri {{ {uri_str} }}
            ?wordUri srs-kg:text|rdfs:label ?english .
            FILTER(lang(?english)="en")
            ?wordUri srs-kg:means ?c .
            OPTIONAL {{
                ?zh srs-kg:means ?c; srs-kg:text|rdfs:label ?chinese; srs-kg:learningTheme "Logic City".
                FILTER(lang(?chinese)="zh")
            }}
            OPTIONAL {{ ?c srs-kg:hasVisualization ?v. ?v srs-kg:imageFilePath ?imagePath. }}
        }}
        """
        res = query_sparql(q)
        
        processed = 0
        added = 0
        updated = 0
        
        for b in res.get("results", {}).get("bindings", []):
            english = b['english']['value']
            img_raw = b.get('imagePath', {}).get('value', '')
            word_uri = b['wordUri']['value']
            uuid = word_uri.split('/')[-1]
            
            # Determine Type (Category)
            word_type = "Concrete" if img_raw else "Abstract"
            
            # Image Upload
            image_html = ""
            if img_raw:
                img_path = find_image_file(img_raw)
                if img_path:
                    with open(img_path, "rb") as f:
                        data = f.read()
                        b64 = base64.b64encode(data).decode('utf-8')
                        fname = f"{hashlib.md5(data).hexdigest()[:12]}{img_path.suffix}"
                        anki_fname = anki.store_media_file(fname, b64)
                        image_html = f'<img src="{anki_fname}">'
            
            fields = {
                "Word": english,
                "WordAudio": "",  # No audio source yet
                "WordPicture": image_html,
                "Category": word_type,
                "UUID": uuid,
                "_Remarks": f"Auto-synced: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                "_KG_Map": word_uri
            }
            
            # Escape double quotes safely outside the f-string
            safe_english = english.replace('"', '\\"')
            query = f'deck:"{deck_name}" Word:"{safe_english}"'
            notes = anki._invoke("findNotes", {"query": query})
            
            if notes:
                anki._invoke("updateNoteFields", {"note": {"id": notes[0], "fields": fields}})
                updated += 1
            else:
                anki.add_note(deck_name, "CUMA-Word-Entity-v2", fields, tags=["CUMA", "Level2"])
                added += 1
            processed += 1
            
        return {"message": f"Synced {processed} cards", "added": added, "updated": updated}

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, str(e))
